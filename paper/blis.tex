\documentclass[11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{times}
\usepackage[title, titletoc]{appendix}
\usepackage{graphicx}
\usepackage{amsmath, amssymb}
\usepackage{mathtools} % for \xmapsto
\usepackage{bm}
\usepackage[most]{tcolorbox}
\usepackage{tabularx}
\usepackage{pifont} % optional for symbols
% --- Flowcharts (TikZ) ---
\usepackage{tikz}
\usetikzlibrary{arrows.meta,positioning,shapes.geometric,fit,calc}
\usepackage{xcolor}
% Algorithms (compact)
\usepackage{algorithm}
\usepackage{algpseudocode}

% Optional: slightly tighter algorithm typography
\algrenewcommand\algorithmicrequire{\textbf{Input:}}
\algrenewcommand\algorithmicensure{\textbf{Output:}}
\algrenewcommand\algorithmiccomment[1]{\hfill{\footnotesize$\triangleright$~#1}}

\usepackage{hyperref}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{booktabs,longtable,array,makecell}
\setlength{\LTleft}{0pt}   % no left indent for longtable
\setlength{\LTright}{0pt}  % no right indent for longtable
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}p{#1}}


% --- Theorems, lemmas, claims, proofs ---
\usepackage{amsthm}

\theoremstyle{plain} % bold headings, italic body
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{claim}[theorem]{Claim}

\theoremstyle{definition} % bold heading, upright body
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark} % italic heading, upright body
\newtheorem{remark}[theorem]{Remark}
\newtheorem{note}[theorem]{Note}

% Optional: unnumbered versions
\newtheorem*{theorem*}{Theorem}
\newtheorem*{lemma*}{Lemma}
\newtheorem*{claim*}{Claim}

% Proof environment is provided by amsthm:
% \begin{proof} ... \end{proof}
% Customize QED symbol if you like:
\renewcommand{\qedsymbol}{$\blacksquare$}

\newcommand{\code}[1]{\texttt{#1}}

\definecolor{prefillcolor}{RGB}{173,216,230}  % Light blue
\definecolor{decodecolor}{RGB}{144,238,144}   % Light green
\definecolor{gridcolor}{RGB}{200,200,200}     % Light gray
\definecolor{highlightcolor}{RGB}{255,182,193} % Light pink for integration region


% (Optional) If you want equations numbered by section (since you already use amsmath):
\numberwithin{equation}{section}



\title{BLIS: Blackbox Inference Performance Estimator}
\author{AI Platform Optimization Team}
\date{\today}

\begin{document}

\maketitle

\section{Front matter -- placeholder}
\label{sec:front}

% ============================================================
\subsection{vLLM Execution Model (Serving Semantics)}
\label{subsec:vllm-overview}

We abstract vLLM as a representative example of a modern GPU-based LLM inference
engine.
Our objective is not to document vLLMâ€™s implementation, but to extract the
minimal execution semantics required to model request latency and throughput
from production traces.
The resulting abstraction applies to a broad class of vLLM-style inference
engines that employ dynamic batching, chunked prefill, and iterative decoding.

At a high level, the system consists of a front-end API layer and a back-end
execution engine, connected by an asynchronous queue.
Only the execution engine participates directly in GPU scheduling and is
modeled at step-level granularity.

% ------------------------------------------------------------
\subsubsection{API layer (abstracted)}

The API layer handles client-facing responsibilities, including request ingress,
tokenization, response streaming, and final detokenization.
It operates asynchronously with respect to GPU execution and does not influence
the formation or scheduling of GPU batches.

In our model, the API layer contributes latency that depends linearly on the
number of input and output tokens.
These costs are treated as request-local and are modeled separately from the
engine execution loop.

% ------------------------------------------------------------
\subsubsection{Engine core and busy-loop semantics}

The engine core executes a centralized inference loop driven by a scheduler.
We model this loop as a sequence of discrete \emph{busy-loop iterations}, or
\emph{steps}, each corresponding to a single GPU forward pass over a dynamically
constructed batch of active requests.

We assume the following execution invariants, consistent with vLLM-style
engines:

\begin{enumerate}
  \item Each busy-loop iteration executes exactly one forward pass on the GPU.
  \item A request participates in at most one phase (prefill or decode) per
        iteration.
  \item During decode, a request advances by at most one output token per
        iteration.
  \item During prefill, a request advances by a fixed chunk size of tokens per
        iteration, except possibly for the final prefill iteration.
  \item The duration of each iteration depends on the total number of prefill
        and decode tokens processed in that iteration.
\end{enumerate}

These invariants induce a natural \emph{step-level execution model}, which
serves as the foundation for the latency model and coefficient estimation
procedures developed in the remainder of the paper.

We now formalize the latency model used by BLIS.
The model decomposes request latency into a sequence of stages corresponding to
request ingress, engine execution, and response egress.
Only stages that interact with the engine busy loop are modeled at step-level
granularity; all other stages are treated as linear token-dependent delays.

% ------------------------------------------------------------
\subsection{Request Types}
\label{subsubsec:request-types}

We distinguish between two classes of inference requests based on their maximum
output length:

\begin{itemize}
  \item \emph{Prefill-only requests}, which generate exactly one output token.
  \item \emph{Decode requests}, which generate one or more output tokens.
\end{itemize}

This distinction affects how requests participate in the engine busy loop.
Both request types incur prefill execution; only decode requests participate in
subsequent decode iterations.

% ------------------------------------------------------------
\subsection{Request Lifecycle}
\label{subsubsec:request-life-cycle}

\begin{figure}[!ht]
  \centering
  \includegraphics[height=0.8\textwidth]{figs/reqlifecycle.png}
  \caption{Request lifecycle in a vLLM-style inference engine.}
  \label{fig:req-lifecycle}
\end{figure}

The lifecycle of an inference request proceeds through the stages illustrated
in Figure~\ref{fig:req-lifecycle} and described below.

\paragraph*{1. Ingress.}
A client emits an inference request $i$ to the server.
This stage contributes a network latency that depends linearly on the number of
input tokens $\nu_i^{\mathrm{in}}$.

\paragraph*{2. Pre-processing.}
The API layer tokenizes the input, performs lightweight validation, and enqueues
the request for execution.
This stage contributes a latency that depends linearly on
$\nu_i^{\mathrm{in}}$ and does not interact with GPU scheduling.

\paragraph*{3. Scheduling.}
The request waits in the engine queue until it is admitted into a busy-loop
iteration.
During this period, other requests may be serviced by the engine.

\paragraph*{4. Chunked prefill.}
The uncached portion of the request prompt is processed during prefill.
The request participates in one or more busy-loop iterations, each processing a
fixed-size chunk of tokens, except possibly for the final iteration.
The prefill latency equals the sum of the durations of the iterations in which
the request participates in prefill.

\paragraph*{5. Decode.}
For decode requests, the engine generates output tokens iteratively.
Each busy-loop iteration advances at most one output token for the request.
The decode latency equals the sum of the durations of the iterations in which
the request participates in decode.
The first output token is generated during the prefill phase; subsequent tokens
are generated during decode.

\paragraph*{6. Post-processing.}
Once all output tokens are produced, the API layer detokenizes the generated
sequence and prepares the final response.
This stage contributes a latency that depends linearly on the number of output
tokens $\nu_i^{\mathrm{out}}$.

\paragraph*{7. Egress.}
The server transmits the response back to the client.
This contributes a network latency that depends linearly on
$\nu_i^{\mathrm{out}}$.

\paragraph{Modeling focus.}
Among these stages, only prefill and decode interact with the engine busy loop
and contribute to step-level execution time.
All other stages are treated as additive, request-local delays and are not
modeled at step granularity.


% ============================================================
\section{Problem formulation}
\label{sec:problem_formulation}

% ------------------------------------------------------------
\subsection{Observability model: what we measure (and what we do not)}
\label{subsec:observability_model}

\paragraph{Environments.}
Let $e\in\mathcal{E}$ index a deployment environment, defined by a fixed
combination of: LLM (architecture/size), GPU type/interconnect, and engine
configuration (e.g., chunk size, parallelism, scheduler settings).
Each environment has nonnegative feature vectors
\[
x_e \in \mathbb{R}^{d_x}_{\ge 0},\qquad
z_e \in \mathbb{R}^{d_z}_{\ge 0},
\]
where $x_e$ captures execution-relevant features (e.g., parameter count, hidden
dimension, GPU FLOPs/bandwidth, interconnect characteristics, configuration
flags), and $z_e$ captures client-visible overhead features (often similar, but
allowed to differ; e.g., network topology).

\paragraph{Trace-side observations.}
In each environment $e$, we observe a collection of \emph{phase instances}
$\mathcal{I}_e$ extracted from production traces.
Each phase instance $i\in\mathcal{I}_e$ corresponds to either the \textsf{prefill}
phase or the \textsf{decode} phase of a particular request, and provides:
\begin{itemize}
  \item request identifier $r_i$;
  \item phase type $\tau_i\in\{\textsf{prefill},\textsf{decode}\}$;
  \item phase start/end timestamps $(t_{i,s},t_{i,e})$ and duration
        $\ell_i := t_{i,e}-t_{i,s}$;
  \item request-level aggregate token totals associated with $r_i$:
        prefill tokens $P_{r_i}$ and decode tokens $D_{r_i}$.
\end{itemize}
We assume $P_r$ counts only the \emph{uncached} prompt tokens that are actually
executed during prefill (prefix-cached tokens are excluded).

\paragraph{Client-side observations.}
For a subset of requests (or for separate client experiments) in environment $e$,
we observe per-request end-to-end latency $y_r$ and token lengths
$\nu_r^{\mathrm{in}}$ (input tokens) and $\nu_r^{\mathrm{out}}$ (output tokens).
Client-side $y_r$ includes networking, serialization, streaming, and all server
effects (queueing + execution); it is not assumed to expose internal phase
timestamps.

\paragraph{Unobserved quantities.}
Crucially, neither traces nor client data reveal:
\begin{itemize}
  \item busy-loop step boundaries;
  \item per-step runtimes;
  \item per-step token totals $(T_k^{\mathrm{pf}},T_k^{\mathrm{dec}})$.
\end{itemize}
Our estimators therefore operate by converting trace-visible
phase overlaps into \emph{time-integrated workload exposures} suitable for
regressing a step-level execution model.

% ------------------------------------------------------------
\subsection{Problem statement: featurized execution and client models}
\label{subsec:problem_statement_featurized}

\paragraph{Step-level execution model.}
We model a vLLM-style engine as a single busy-loop that executes a sequence of
discrete \emph{steps} (GPU forward passes), indexed by $k$.
Step $k$ processes some number of prefill tokens $T_k^{\mathrm{pf}}$ and decode
tokens $T_k^{\mathrm{dec}}$ across a dynamically constructed batch of active
requests, and has duration
\begin{equation}
\Delta t_k
=
\beta_{e,0}
+
\beta_{e,1}\,T_k^{\mathrm{pf}}
+
\beta_{e,2}\,T_k^{\mathrm{dec}},
\label{eq:featurized_step_model}
\end{equation}
where environment-specific coefficients
\[
\beta_e=(\beta_{e,0},\beta_{e,1},\beta_{e,2})
\]
have units
(sec/step, sec/token for prefill, sec/token for decode), respectively.

\paragraph{Client overhead model.}
Client-visible latency includes additional request-local overheads not visible
in server traces (e.g., client--server networking, request/response
serialization, streaming effects).
We model these overheads as
\begin{equation}
L^{\mathrm{client}}_{r,e}
=
\alpha_{e,0}
+
\alpha_{e,1}\,\nu_r^{\mathrm{in}}
+
\alpha_{e,2}\,\nu_r^{\mathrm{out}},
\label{eq:client_overhead_model}
\end{equation}
with environment-specific coefficients
$\alpha_e=(\alpha_{e,0},\alpha_{e,1},\alpha_{e,2})$ (units: seconds, sec/token,
sec/token).

\paragraph{Featurized coefficient maps.}
Rather than re-estimating $\beta_e$ and $\alpha_e$ independently for each
environment, we learn nonnegative linear feature maps:
\begin{equation}
\beta_e = x_e^\top B,
\qquad
\alpha_e = z_e^\top A,
\label{eq:featurized_maps}
\end{equation}
where
\[
B \in \mathbb{R}^{d_x \times 3}_{\ge 0},
\qquad
A \in \mathbb{R}^{d_z \times 3}_{\ge 0}.
\]
Nonnegativity of $(x_e,z_e,A,B)$ ensures physical interpretability and guarantees
$\beta_e,\alpha_e\ge 0$ for all environments (including unseen ones).

\paragraph{Learning objective}
Our estimators constructs, from trace-side observables, phase-level
regression features that approximate the \emph{step-aggregated} work experienced
during each trace phase instance. These features allow learning $B$ from traces
alone. Separately, we learn $A$ from client latencies after subtracting the
trace-visible server-side component.

Formally, the problem is:
\begin{itemize}
  \item \textbf{Trace-only:} estimate $B$ such that, for each phase instance
  $i\in\mathcal{I}_e$, the observed duration $\ell_i$ is well-predicted by a
  featurized version of the step model aggregated over the inferred step count.
  \item \textbf{Client residual:} estimate $A$ such that client residual latency
  (end-to-end minus trace-visible server latency) is well-predicted by the
  linear overhead model in Eq.~\eqref{eq:client_overhead_model}.
\end{itemize}

% -------------------------
\subsubsection{Featurized predictors for $B$ (trace-only)}
\label{subsubsec:nnls_B_trace_only}

\paragraph{Phase-level predictor in environment $e$.}
For phase instance $i\in\mathcal{I}_e$, BLIS predicts its duration as
\begin{equation}
\widehat \ell_i(B)
=
N_i\,\beta_{e,0}
+
A_i^{\mathrm{pf}}\,\beta_{e,1}
+
A_i^{\mathrm{dec}}\,\beta_{e,2},
\qquad
\beta_e = x_e^\top B.
\label{eq:featurized_phase_predictor}
\end{equation}
This preserves the semantics of the step model:
$N_i$ counts steps, while $(A_i^{\mathrm{pf}},A_i^{\mathrm{dec}})$ approximate
step-aggregated token totals experienced during the phase.

\paragraph{Linearization in $\mathrm{vec}(B)$.}
Let $b_0,b_1,b_2\in\mathbb{R}^{d_x}_{\ge 0}$ denote the columns of $B$.
Then Eq.~\eqref{eq:featurized_phase_predictor} becomes
\[
\widehat \ell_i(B)
=
N_i(x_e^\top b_0)
+
A_i^{\mathrm{pf}}(x_e^\top b_1)
+
A_i^{\mathrm{dec}}(x_e^\top b_2),
\]
which is linear in the entries of $B$.
Define the per-instance row (dimension $1\times 3d_x$)
\begin{equation}
\phi_i
:=
\big[
N_i x_e^\top,\;
A_i^{\mathrm{pf}} x_e^\top,\;
A_i^{\mathrm{dec}} x_e^\top
\big],
\label{eq:phi_row_def}
\end{equation}
and let $\theta := \mathrm{vec}(B)$ stack the columns of $B$.
Then $\widehat \ell_i(B)=\phi_i \theta$.

% ============================================================
\section{Continuous EM for Execution Coefficients}
\label{sec:featurized_continuous_em}

\subsection{Trace pressures}
\label{subsec:trace_pressures}

This subsection defines (i) how we infer per-phase step counts from token totals,
(ii) how we compute trace-derived token ``pressures'' and integrated exposures,
and (iii) how we stack a global nonnegative least-squares (NNLS) problem to learn
$B$ and $A$.

% -------------------------
\subsubsection{Trace-inferred step counts and partial prefill chunking}
\label{subsubsec:featurized_step_counts}

\paragraph{Prefill and decode step counts.}
For each request $r$ in environment $e$, let $P_r$ be its uncached prefill token
count and $D_r$ be its decode token count.
Let $C$ denote the engine's prefill chunk size (tokens/step) for environment $e$.
We infer:
\begin{itemize}
  \item \textbf{Decode:} decode advances at most one token per step per active
  request; hence a decode phase instance has inferred step count
  \(
    N_i = D_{r_i}
  \)
  when $\tau_i=\textsf{decode}$.
  \item \textbf{Prefill:} prefill proceeds in chunks of up to $C$ tokens/step,
  with the final step possibly partial; hence a prefill phase instance of request
  $r$ has inferred step count
  \begin{equation}
  N_r^{\mathrm{pf}} = \left\lceil \frac{P_r}{C} \right\rceil,
  \qquad
  N_i = N_{r_i}^{\mathrm{pf}}
  \ \ \text{when }\tau_i=\textsf{prefill}.
  \label{eq:prefill_step_count}
  \end{equation}
\end{itemize}

\paragraph{Final partial prefill chunk.}
For a request $r$ with $N_r^{\mathrm{pf}}$ prefill steps, the last prefill step
processes
\begin{equation}
\rho_r
=
P_r - C\,(N_r^{\mathrm{pf}}-1),
\qquad
\rho_r\in(0,C],
\label{eq:rho_def_featurized}
\end{equation}
so the \emph{missing mass} relative to a full chunk is
\begin{equation}
\mu_r = C - \rho_r \in [0,C).
\label{eq:mu_def_featurized}
\end{equation}
Ignoring $\mu_r$ systematically overstates prefill token work.

% -------------------------
\subsubsection{Trace-derived pressures and integrated token exposures}
\label{subsubsec:pressures_exposures}

\paragraph{Trace-induced time grid.}
Let $\mathcal{G}=\{g_1<g_2<\cdots<g_{J+1}\}$ be the sorted set of all phase
boundaries $\{t_{i,s}\}\cup\{t_{i,e}\}$ within environment $e$.
This partitions time into half-open grid cells $[g_j,g_{j+1})$, on which phase
membership is constant.

\paragraph{Implementation (grid-exact integrals).}
All integrals are computed exactly on the trace-induced grid: 
for any piecewise-constant signal $f(t)$,
\[
\int_{t_{i,s}}^{t_{i,e}} f(t)\,dt
=
\sum_{j=1}^J f_j \cdot \bigl|\,[t_{i,s},t_{i,e})\cap[g_j,g_{j+1})\,\bigr|,
\]
where $f_j$ is the constant value of $f(t)$ on cell $[g_j,g_{j+1})$.

\paragraph{Instantaneous phase indicators.}
Define binary indicators at wall-clock time $t$:
\[
a_r(t)=\mathbf{1}\{r \text{ is in prefill at }t\},
\qquad
d_r(t)=\mathbf{1}\{r \text{ is in decode at }t\}.
\]
These are trace-visible because prefill/decode phase windows are observed.

\paragraph{Token pressures (units: tokens/step).}
We define piecewise-constant \emph{pressures} that quantify how many tokens would
be attributed to a representative step executed at time $t$:
\begin{align}
p^{\mathrm{dec}}(t)
&:= \sum_r d_r(t),
&&\text{(decode: 1 token per active decode request per step)}
\label{eq:pressure_decode_featurized}
\\
p^{\mathrm{pf}}_{\mathrm{full}}(t)
&:= C\sum_r a_r(t).
&&\text{(prefill: full-chunk assumption)}
\label{eq:pressure_prefill_full_featurized}
\end{align}
The quantity $p^{\mathrm{pf}}_{\mathrm{full}}(t)$ corresponds to the prefill
token pressure without any correction for the final partial chunk.

\paragraph{Base pressures used by the step-rate model.}
Throughout the continuous EM procedure, the step-rate
$\lambda_e(t;\beta_e)$ in Eq.~\eqref{eq:em_step_rate} is defined using the
\emph{uncorrected} (full-chunk) prefill pressure:
\begin{equation}
p^{\mathrm{pf}}(t) := p^{\mathrm{pf}}_{\mathrm{full}}(t),
\qquad
p^{\mathrm{dec}}(t) \text{ as in Eq.~\eqref{eq:pressure_decode_featurized}}.
\label{eq:base_pressures_for_em}
\end{equation}
The \emph{corrected} prefill pressure $\tilde p^{\mathrm{pf}}_e(t)$ is introduced
later (Eq.~\eqref{eq:em_effective_prefill_pressure}) and is used only when
forming step-averaged prefill work in the E-step.

% ------------------------------------------------------------
\subsection{Latent step model and step-rate function}

We consider a fixed environment $e$ with feature vector $x_e$ and trace data
$\mathcal{I}_e$.
Execution proceeds as a sequence of latent busy-loop steps indexed by $k$.
Each step processes some number of prefill and decode tokens and has duration
\begin{equation}
\Delta t_k
=
\beta_{e,0}
+
\beta_{e,1}\,T^{\mathrm{pf}}_k
+
\beta_{e,2}\,T^{\mathrm{dec}}_k,
\label{eq:em_step_model}
\end{equation}
where $\beta_e=(\beta_{e,0},\beta_{e,1},\beta_{e,2})\in\mathbb{R}^3_{\ge 0}$.

Given trace-derived instantaneous token pressures
$p^{\mathrm{pf}}(t)$ and $p^{\mathrm{dec}}(t)$
(defined in Section~\ref{subsubsec:pressures_exposures}),
we define the \emph{local step-time proxy}
Given trace-derived instantaneous token pressures
$p^{\mathrm{pf}}_{\mathrm{full}}(t)$ and $p^{\mathrm{dec}}(t)$
(defined in Section~\ref{subsubsec:pressures_exposures}),
we define the \emph{local step-time proxy}
\begin{equation}
\Delta_e(t;\beta_e)
=
\beta_{e,0}
+
\beta_{e,1}\,p^{\mathrm{pf}}_{\mathrm{full}}(t)
+
\beta_{e,2}\,p^{\mathrm{dec}}(t),
\label{eq:em_step_time_proxy}
\end{equation}
and the corresponding \emph{step-rate} (intensity)
\begin{equation}
\lambda_e(t;\beta_e)
=
\frac{1}{\Delta_e(t;\beta_e)},
\qquad
\text{units: steps/sec}.
\label{eq:em_step_rate}
\end{equation}

We assume $\Delta_e(t;\beta_e)>0$ for all $t$ in the trace support, which can be
ensured by enforcing $\beta_{e,0}\ge \beta_{\min}>0$ or by clamping
$\Delta_e(t)\leftarrow\max\{\Delta_e(t),\Delta_{\min}\}$ for a small
$\Delta_{\min}>0$.

% ------------------------------------------------------------
\subsection{Phase membership and latent step densities}

For each phase instance $i\in\mathcal{I}_e$ with observed start and end times
$(t_{i,s},t_{i,e})$ and inferred step count $N_i$, we introduce a continuous
phase-membership function $w_i(t)\in[0,1]$ that captures uncertainty near
instrumented boundaries.
A simple boundary-aware choice is
\begin{equation}
w_i(t)
=
F_h(t-t_{i,s})\,
\bigl(1-F_h(t-t_{i,e})\bigr),
\label{eq:em_phase_membership}
\end{equation}
where $F_h$ is a one-sided cumulative kernel with bandwidth $h>0$
(e.g., $F_h(x)=1-e^{-x/h}$ for $x\ge 0$).
By construction, $w_i(t)=1$ throughout the interior of the observed phase window
and decays smoothly to zero near its boundaries.

Conditioned on execution coefficients $\beta_e$, we model the latent placement
of the $N_i$ steps of phase $i$ as independent draws from a continuous density
proportional to the step-rate.
The posterior step-density for phase $i$ is
\begin{equation}
q_i(t \mid \beta_e)
=
\frac{w_i(t)\,\lambda_e(t;\beta_e)}
{\int w_i(u)\,\lambda_e(u;\beta_e)\,du},
\label{eq:em_phase_step_density}
\end{equation}
which integrates to one.
The expected number of steps of phase $i$ occurring in an infinitesimal interval
$dt$ is $N_i\,q_i(t\mid\beta_e)\,dt$.

% ------------------------------------------------------------
\subsection{Partial prefill chunk correction}

For a prefill request $r$, exactly one of its $N_r^{\mathrm{pf}}$ prefill steps
processes fewer than $C$ tokens.
Let $\mu_r$ denote the missing token mass of that final step
(defined in Eq.~\eqref{eq:mu_def_featurized}).

We localize the final prefill step near the observed end of the prefill phase
using a boundary-local kernel $\varphi_r(t)$ supported on $(-\infty,t_{r,e}]$,
normalized to integrate to one. 
Concretely, we define $\varphi_r(t)$ as a one-sided exponential kernel:
\begin{equation}
\varphi_r(t)
=
\frac{1}{h}\,
\exp\!\left(-\frac{t_{r,e}-t}{h}\right)
\mathbf{1}\{t \le t_{r,e}\},
\label{eq:final_prefill_kernel}
\end{equation}
where $h>0$ is a bandwidth parameter controlling the temporal localization.
By construction, $\varphi_r(t)$ is nonnegative, supported on
$(-\infty,t_{r,e}]$, and satisfies
$\int \varphi_r(t)\,dt = 1$.

The corresponding posterior density of the final prefill step is
\begin{equation}
q^{\mathrm{final}}_r(t \mid \beta_e)
=
\frac{\varphi_r(t)\,\lambda_e(t;\beta_e)}
{\int \varphi_r(u)\,\lambda_e(u;\beta_e)\,du}.
\label{eq:em_final_step_density}
\end{equation}

The expected missing-token correction per step is then
\begin{equation}
\delta_e(t;\beta_e)
=
\frac{\sum_r \mu_r\,q^{\mathrm{final}}_r(t\mid\beta_e)}
{\lambda_e(t;\beta_e)},
\qquad
\text{units: tokens/step}.
\label{eq:em_partial_chunk_correction}
\end{equation}

The effective prefill pressure used in EM is
\begin{equation}
\tilde p^{\mathrm{pf}}_e(t)
=
p^{\mathrm{pf}}_{\mathrm{full}}(t)
-
\delta_e(t;\beta_e),
\label{eq:em_effective_prefill_pressure}
\end{equation}
which ensures that each request contributes exactly one partial prefill chunk
in expectation.

\paragraph{Dependence on $\beta_e$.}
Because $\delta_e(t;\beta_e)$ in Eq.~\eqref{eq:em_partial_chunk_correction}
depends on the step-rate $\lambda_e(t;\beta_e)$, the effective prefill pressure
$\tilde p^{\mathrm{pf}}_e(t)$ in Eq.~\eqref{eq:em_effective_prefill_pressure}
is $\beta_e$-dependent.
To emphasize this, we will sometimes write
$\tilde p^{\mathrm{pf}}_e(t;\beta_e)$, though we drop the explicit argument when
unambiguous.

% ------------------------------------------------------------
\subsection{EM objective and updates}

\paragraph{E-step.}
Given current coefficients $\beta_e^{(m-1)}$, compute:
\begin{itemize}
  \item the step-rate $\lambda_e(t;\beta_e^{(m-1)})$;
  \item phase step densities $q_i(t\mid\beta_e^{(m-1)})$;
  \item effective prefill pressure $\tilde p^{\mathrm{pf}}_e(t)$.
\end{itemize}

Using these quantities, form step-averaged pressures
\begin{align}
\bar p^{\mathrm{pf}}_{i,(m)}
&=
\int q_i(t\mid\beta_e^{(m-1)})\,
\tilde p^{\mathrm{pf}}_e(t)\,dt,
\\
\bar p^{\mathrm{dec}}_{i,(m)}
&=
\int q_i(t\mid\beta_e^{(m-1)})\,
p^{\mathrm{dec}}(t)\,dt.
\end{align}

\paragraph{M-step.}
Update execution coefficients by solving
\begin{equation}
\beta_e^{(m)}
=
\arg\min_{\beta\in\mathbb{R}^3_{\ge 0}}
\sum_{i\in\mathcal{I}_e}
\left(
N_i\beta_0
+
N_i\bar p^{\mathrm{pf}}_{i,(m)}\beta_1
+
N_i\bar p^{\mathrm{dec}}_{i,(m)}\beta_2
-
\ell_i
\right)^2 .
\label{eq:em_mstep_beta}
\end{equation}
This is a convex nonnegative least-squares problem in $\beta$.

Each EM iteration monotonically decreases the observed-data squared-error
objective, which corresponds to a restricted E-step with phase-local 
uniform step densities.

\subsection{Initialization via a phase-local constant step-rate surrogate}
\label{subsec:initialization}

The continuous EM procedure requires an initial allocation of latent busy-loop
steps within each observed phase window.
As initialization, we use a deterministic surrogate that assumes a
\emph{phase-local constant step rate}.

For each phase instance $i$ with inferred step count $N_i$ and observed duration
$\ell_i = t_{i,e} - t_{i,s}$, we define
\begin{equation}
\widehat\lambda_i := \frac{N_i}{\ell_i},
\label{eq:initial_lambda_hat}
\end{equation}
and assume hard phase membership over the observed window,
\[
w_i^{\mathrm{base}}(t)
=
\mathbf{1}\{t \in [t_{i,s}, t_{i,e})\}.
\]

Under this surrogate, the induced latent step density is
\begin{equation}
q_i^{\mathrm{base}}(t)
=
\frac{w_i^{\mathrm{base}}(t)\,\widehat\lambda_i}
{\int w_i^{\mathrm{base}}(u)\,\widehat\lambda_i\,du}
=
\frac{1}{\ell_i}\,
\mathbf{1}\{t \in [t_{i,s}, t_{i,e})\},
\label{eq:initial_step_density}
\end{equation}
i.e., latent steps are assumed to be uniformly distributed in time within each
phase window.

This phase-local constant-rate assumption yields closed-form, trace-only
expressions for time-averaged token pressures and leads to a single nonnegative
least-squares update for the execution coefficients.
No $\beta$-dependent step-rate or latent step refinement is introduced at this
stage.

\paragraph{Induced initialization objective.}
Under the surrogate density $q_i^{\mathrm{base}}(t)$ in
Eq.~\eqref{eq:initial_step_density}, step-averaged pressures reduce to
time-averages over the observed phase window.
Specifically, define
\begin{align}
\bar p^{\mathrm{pf,base}}_i
&:=
\frac{1}{\ell_i}
\int_{t_{i,s}}^{t_{i,e}}
p^{\mathrm{pf}}_{\mathrm{full}}(t)\,dt,
\\
\bar p^{\mathrm{dec,base}}_i
&:=
\frac{1}{\ell_i}
\int_{t_{i,s}}^{t_{i,e}}
p^{\mathrm{dec}}(t)\,dt .
\end{align}
(At initialization, no partial-chunk correction is applied.)

The corresponding execution coefficients are obtained by a single
nonnegative least-squares solve:
\begin{equation}
\beta_e^{(0)}
=
\arg\min_{\beta\in\mathbb{R}^3_{\ge 0}}
\sum_{i\in\mathcal{I}_e}
\left(
N_i\beta_0
+
N_i\bar p^{\mathrm{pf,base}}_i\,\beta_1
+
N_i\bar p^{\mathrm{dec,base}}_i\,\beta_2
-
\ell_i
\right)^2 .
\label{eq:initial_nnls_beta}
\end{equation}

% ------------------------------------------------------------
\subsection{Transfer and EM recalibration for a new environment}
\label{subsec:transfer}

Let $e^\star$ be a new environment with feature vector $x_{e^\star}$.

\paragraph{Zero-shot initialization.}
Assume that a global execution map $\widehat B$ has been learned from trace data
collected across a set of training environments
$\mathcal{E}_{\mathrm{train}}$.
For a new environment $e^\star$ with feature vector $x_{e^\star}$, but without
requiring any trace data from $e^\star$, we initialize execution coefficients as
\begin{equation}
\beta_{e^\star}^{(0)}
:=
x_{e^\star}^\top \widehat B .
\label{eq:em_transfer_init}
\end{equation}
This produces a physically valid, feature-consistent starting point that
supports zero-shot transfer across environments.


\paragraph{EM-based recalibration.}
When trace data $\mathcal{I}_{e^\star}$ is available, refine execution modeling
by running the continuous EM algorithm described above, updating only
$\beta_{e^\star}$ while keeping $\widehat B$ fixed.
This yields environment-adapted coefficients
$\widehat\beta_{e^\star}^{\mathrm{EM}}$ that capture step-density and
interference effects.

\paragraph{Regularization toward the feature prior (optional).}
When trace coverage is limited, we optionally regularize the M-step toward the
feature-based prior:
\begin{equation}
\beta_{e^\star}^{(m)}
=
\arg\min_{\beta\ge 0}
\Bigg[
\sum_{i\in\mathcal{I}_{e^\star}}
\left(
N_i\beta_0
+
N_i\bar p^{\mathrm{pf}}_{i,(m)}\beta_1
+
N_i\bar p^{\mathrm{dec}}_{i,(m)}\beta_2
-
\ell_i
\right)^2
+
\lambda_{\mathrm{EM}}
\left\|
\beta - x_{e^\star}^\top \widehat B
\right\|_2^2
\Bigg].
\label{eq:em_recalibration_regularized}
\end{equation}
This implements partial pooling: deviations from the feature-predicted
coefficients are permitted when supported by trace evidence, while preventing
overfitting to short trace windows.

\paragraph{Summary.}
Featurized continuous EM refines environment-specific execution coefficients
using trace-aware latent step allocation, while preserving the portability and
interpretability of the global execution map. The procedure cleanly 
separates global generalization (via $B$) from (optional) local 
adaptation (via $\beta_e$) and remains fully trace-only and 
efficiently-solvable (convex) in its updates.

% -------------------------
\section{Learning $A$ from client residuals}
\label{sec:nnls_A_client}

\paragraph{Trace-visible server latency per request.}
For a request $r$ in environment $e$, traces provide its prefill phase duration
$\ell^{\mathrm{pf}}_r$ and (if applicable) decode phase duration
$\ell^{\mathrm{dec}}_r$. Define the trace-visible server-side latency
\begin{equation}
S^{\mathrm{trace}}_{r,e}
:=
\ell^{\mathrm{pf}}_r + \ell^{\mathrm{dec}}_r,
\label{eq:server_trace_latency_def}
\end{equation}
where $\ell^{\mathrm{dec}}_r:=0$ for prefill-only requests.

\paragraph{Client residual.}
For requests where we observe end-to-end client latency $y_r$, define
\begin{equation}
R_{r,e} := y_r - S^{\mathrm{trace}}_{r,e}.
\label{eq:client_residual_def}
\end{equation}
By construction, $R_{r,e}$ isolates client-visible effects not captured by
server traces (networking, serialization, streaming).

\paragraph{Featurized client model and NNLS for $A$.}
Substituting $\alpha_e=z_e^\top A$ into Eq.~\eqref{eq:client_overhead_model}
yields
\[
R_{r,e}
\approx
(z_e^\top a_0)
+
\nu_r^{\mathrm{in}}(z_e^\top a_1)
+
\nu_r^{\mathrm{out}}(z_e^\top a_2),
\]
where $a_0,a_1,a_2$ are columns of $A$.
Define the per-request row
\[
\psi_r
:=
\big[
1\cdot z_e^\top,\;
\nu_r^{\mathrm{in}} z_e^\top,\;
\nu_r^{\mathrm{out}} z_e^\top
\big],
\]
and stack across all observed client requests to solve the global NNLS:
\begin{equation}
\widehat A
=
\arg\min_{A\in\mathbb{R}^{d_z\times 3}_{\ge 0}}
\sum_{(r,e)\in\mathcal{D}_{\mathrm{client}}}
\left(
\psi_r\,\mathrm{vec}(A) - R_{r,e}
\right)^2.
\label{eq:global_nnls_A}
\end{equation}

\paragraph{Separation of concerns.}
The estimation of $B$ uses \emph{only} trace instances and is unaffected by
client networking effects. The estimation of $A$ uses \emph{only} client
residuals after removing trace-visible server latency. Thus, execution and
client overhead learning are cleanly decoupled.

\subsection{One-shot recalibration of client-side overheads}
\label{subsec:client-side-recalibration}

For a new environment $e^\star$ with paired client and trace data, we form
trace-decomposed residuals
\[
R_r := y_r - S_r^{\mathrm{trace}},
\qquad r\in\mathcal{J}_{e^\star},
\]
where $y_r$ is end-to-end latency observed at the client and
$S_r^{\mathrm{trace}}$ is the server-side latency observable from the trace
(admission, batching, and execution).

Given the featurized prediction
\[
\alpha^{\mathrm{feat}}_{e^\star} := z_{e^\star}^\top \widehat A,
\]
BLIS recalibrates $\alpha_{e^\star}$ by solving a regularized nonnegative least
squares problem:
\begin{equation}
\widehat \alpha_{e^\star}
=
\arg\min_{\alpha\in\mathbb{R}^3_{\ge 0}}
\sum_{r\in\mathcal{J}_{e^\star}}
\left(
\alpha_0
+
\alpha_1 \nu_r^{\mathrm{in}}
+
\alpha_2 \nu_r^{\mathrm{out}}
-
R_r
\right)^2
\;+\;
\lambda_{\mathrm{recal}}^{\alpha}
\left\|
\alpha-\alpha^{\mathrm{feat}}_{e^\star}
\right\|_2^2.
\label{eq:alpha_recalibration_to_featurized_prior}
\end{equation}
This objective is convex and decoupled from server scheduling dynamics by
construction, since all server-side queueing and interference are absorbed into
$S_r^{\mathrm{trace}}$ and removed before fitting $\alpha$.

\bibliographystyle{plain}
\bibliography{blis}

\end{document}
