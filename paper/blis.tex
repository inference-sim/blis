\documentclass[11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{times}
\usepackage[title, titletoc]{appendix}
\usepackage{graphicx}
\usepackage{amsmath, amssymb}
\usepackage{mathtools} % for \xmapsto
\usepackage{bm}
\usepackage[most]{tcolorbox}
\usepackage{tabularx}
\usepackage{pifont} % optional for symbols
% --- Flowcharts (TikZ) ---
\usepackage{tikz}
\usetikzlibrary{arrows.meta,positioning,shapes.geometric,fit,calc}
\usepackage{xcolor}
% Algorithms (compact)
\usepackage{algorithm}
\usepackage{algpseudocode}

% Optional: slightly tighter algorithm typography
\algrenewcommand\algorithmicrequire{\textbf{Input:}}
\algrenewcommand\algorithmicensure{\textbf{Output:}}
\algrenewcommand\algorithmiccomment[1]{\hfill{\footnotesize$\triangleright$~#1}}

\usepackage{hyperref}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{booktabs,longtable,array,makecell}
\setlength{\LTleft}{0pt}   % no left indent for longtable
\setlength{\LTright}{0pt}  % no right indent for longtable
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}p{#1}}


% --- Theorems, lemmas, claims, proofs ---
\usepackage{amsthm}

\theoremstyle{plain} % bold headings, italic body
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{claim}[theorem]{Claim}

\theoremstyle{definition} % bold heading, upright body
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark} % italic heading, upright body
\newtheorem{remark}[theorem]{Remark}
\newtheorem{note}[theorem]{Note}

% Optional: unnumbered versions
\newtheorem*{theorem*}{Theorem}
\newtheorem*{lemma*}{Lemma}
\newtheorem*{claim*}{Claim}

% Proof environment is provided by amsthm:
% \begin{proof} ... \end{proof}
% Customize QED symbol if you like:
\renewcommand{\qedsymbol}{$\blacksquare$}

\newcommand{\code}[1]{\texttt{#1}}

\definecolor{prefillcolor}{RGB}{173,216,230}  % Light blue
\definecolor{decodecolor}{RGB}{144,238,144}   % Light green
\definecolor{gridcolor}{RGB}{200,200,200}     % Light gray
\definecolor{highlightcolor}{RGB}{255,182,193} % Light pink for integration region


% (Optional) If you want equations numbered by section (since you already use amsmath):
\numberwithin{equation}{section}



\title{BLIS: Blackbox Inference Performance Estimator}
\author{AI Platform Optimization Team}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This paper introduces BLIS, a system for blackbox inference performance estimation 
designed to model inference request flows and latency.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

\section{Related Work}
\label{sec:related-work}

% ============================================================
\subsection{vLLM Execution Model (Serving Semantics)}
\label{subsec:vllm-overview}

We abstract vLLM as a representative example of a modern GPU-based LLM inference
engine.
Our objective is not to document vLLM’s implementation, but to extract the
minimal execution semantics required to model request latency and throughput
from production traces.
The resulting abstraction applies to a broad class of vLLM-style inference
engines that employ dynamic batching, chunked prefill, and iterative decoding.

At a high level, the system consists of a front-end API layer and a back-end
execution engine, connected by an asynchronous queue.
Only the execution engine participates directly in GPU scheduling and is
modeled at step-level granularity.

% ------------------------------------------------------------
\subsubsection{API layer (abstracted)}

The API layer handles client-facing responsibilities, including request ingress,
tokenization, response streaming, and final detokenization.
It operates asynchronously with respect to GPU execution and does not influence
the formation or scheduling of GPU batches.

In our model, the API layer contributes latency that depends linearly on the
number of input and output tokens.
These costs are treated as request-local and are modeled separately from the
engine execution loop.

% ------------------------------------------------------------
\subsubsection{Engine core and busy-loop semantics}

The engine core executes a centralized inference loop driven by a scheduler.
We model this loop as a sequence of discrete \emph{busy-loop iterations}, or
\emph{steps}, each corresponding to a single GPU forward pass over a dynamically
constructed batch of active requests.

We assume the following execution invariants, consistent with vLLM-style
engines:

\begin{enumerate}
  \item Each busy-loop iteration executes exactly one forward pass on the GPU.
  \item A request participates in at most one phase (prefill or decode) per
        iteration.
  \item During decode, a request advances by at most one output token per
        iteration.
  \item During prefill, a request advances by a fixed chunk size of tokens per
        iteration, except possibly for the final prefill iteration.
  \item The duration of each iteration depends on the total number of prefill
        and decode tokens processed in that iteration.
\end{enumerate}

These invariants induce a natural \emph{step-level execution model}, which
serves as the foundation for the latency model and coefficient estimation
procedures developed in the remainder of the paper.

% ============================================================
\subsection{Design of BLIS}
\label{subsec:blis-design}

BLIS is a trace-driven inference simulator designed to reproduce the latency and
throughput behavior of vLLM-style engines without requiring intrusive
instrumentation or access to internal execution traces.
It operates in two phases:
(i) learning step-level execution coefficients from production traces, and
(ii) replaying request arrivals and engine dynamics using these learned
coefficients.

% ============================================================
\section{Latency Model}
\label{sec:latency-model}

We now formalize the latency model used by BLIS.
The model decomposes request latency into a sequence of stages corresponding to
request ingress, engine execution, and response egress.
Only stages that interact with the engine busy loop are modeled at step-level
granularity; all other stages are treated as linear token-dependent delays.

% ------------------------------------------------------------
\subsection{Request Types}
\label{subsubsec:request-types}

We distinguish between two classes of inference requests based on their maximum
output length:

\begin{itemize}
  \item \emph{Prefill-only requests}, which generate exactly one output token.
  \item \emph{Decode requests}, which generate one or more output tokens.
\end{itemize}

This distinction affects how requests participate in the engine busy loop.
Both request types incur prefill execution; only decode requests participate in
subsequent decode iterations.

% ------------------------------------------------------------
\subsection{Request Lifecycle}
\label{subsubsec:request-life-cycle}

\begin{figure}[!ht]
  \centering
  \includegraphics[height=0.8\textwidth]{figs/reqlifecycle.png}
  \caption{Request lifecycle in a vLLM-style inference engine.}
  \label{fig:req-lifecycle}
\end{figure}

The lifecycle of an inference request proceeds through the stages illustrated
in Figure~\ref{fig:req-lifecycle} and described below.

\paragraph*{1. Ingress.}
A client emits an inference request $i$ to the server.
This stage contributes a network latency that depends linearly on the number of
input tokens $\ell_i^{\mathrm{in}}$.

\paragraph*{2. Pre-processing.}
The API layer tokenizes the input, performs lightweight validation, and enqueues
the request for execution.
This stage contributes a latency that depends linearly on
$\ell_i^{\mathrm{in}}$ and does not interact with GPU scheduling.

\paragraph*{3. Scheduling.}
The request waits in the engine queue until it is admitted into a busy-loop
iteration.
During this period, other requests may be serviced by the engine.

\paragraph*{4. Chunked prefill.}
The uncached portion of the request prompt is processed during prefill.
The request participates in one or more busy-loop iterations, each processing a
fixed-size chunk of tokens, except possibly for the final iteration.
The prefill latency equals the sum of the durations of the iterations in which
the request participates in prefill.

\paragraph*{5. Decode.}
For decode requests, the engine generates output tokens iteratively.
Each busy-loop iteration advances at most one output token for the request.
The decode latency equals the sum of the durations of the iterations in which
the request participates in decode.
The first output token is generated during the prefill phase; subsequent tokens
are generated during decode.

\paragraph*{6. Post-processing.}
Once all output tokens are produced, the API layer detokenizes the generated
sequence and prepares the final response.
This stage contributes a latency that depends linearly on the number of output
tokens $\ell_i^{\mathrm{out}}$.

\paragraph*{7. Egress.}
The server transmits the response back to the client.
This contributes a network latency that depends linearly on
$\ell_i^{\mathrm{out}}$.

\paragraph{Modeling focus.}
Among these stages, only prefill and decode interact with the engine busy loop
and contribute to step-level execution time.
All other stages are treated as additive, request-local delays and are not
modeled at step granularity.


\section{Trace-Only Estimation of Step-Level Execution Coefficients}
\label{sec:step_beta_estimation}

This section develops a trace-only methodology for estimating the
\emph{step-level} execution coefficients of a vLLM-style inference engine.
Our goal is to recover the coefficient vector
$\beta=(\beta_0,\beta_1,\beta_2)$ in a step-time model that is used by the
simulator, using only production traces that report phase start/end times and
aggregate token counts.
No step boundaries, per-step timings, or engine instrumentation are assumed.

We present two estimators.
Section~\ref{sec:baseline} introduces a simple, trace-only baseline that fits
phase durations using time-integrated workload signals.
Section~\ref{sec:em_estimation} then refines this baseline by making the
\emph{latent placement of steps in wall-clock time} explicit and estimating it
via an EM procedure with exact handling of the final partial prefill chunk.
The baseline can be interpreted as a one-shot instance of the same latent-step
allocation view, with a phase-local uniform allocation surrogate; EM replaces
this surrogate by an iteration-dependent allocation driven by a learned
step-density model.

\paragraph{Notation and units (summary).}
Table~\ref{tab:notation_summary} lists the principal symbols used in the baseline
and EM estimators and their units.

\begin{table}[ht!]
\centering
\small
\begin{tabular}{lll}
\toprule
\textbf{Symbol} & \textbf{Units} & \textbf{Meaning} \\
\midrule
$k$ & index & busy-loop iteration (latent execution step) \\
$r$ & index & inference request; each has one prefill phase and one decode phase \\
$i$ & index & a phase instance (prefill or decode) of a specific request \\
$j$ & index & index of a trace-induced time-grid cell $[g_j,g_{j+1})$ \\
$m$ & index & EM iteration index \\[0.5ex]

$t_{i,s},\,t_{i,e}$ & seconds & start and end times of phase instance $i$ \\
$\ell_i$ & seconds & observed duration of phase instance $i$ ($\ell_i=t_{i,e}-t_{i,s}$) \\
$\tau_i$ & categorical & phase type of instance $i$ (\textsf{prefill} or \textsf{decode}) \\
$N_i$ & steps & trace-inferred step count for phase instance $i$ (fixed) \\[0.5ex]

$C$ & tokens/step & prefill chunk size \\
$P_r$ & tokens & total (uncached) prefill tokens for request $r$ \\
$D_r$ & tokens & total decode tokens for request $r$ \\
$\rho_r$ & tokens & tokens in the final prefill step of request $r$, Eq.~\eqref{eq:rho_def} \\
$\mu_r$ & tokens & missing token mass in final prefill step ($\mu_r=C-\rho_r$) \\[0.5ex]

$a_r(t)$ & unitless & indicator that request $r$ is in prefill at time $t$ (trace-visible) \\
$p^{\mathrm{pf}}_{\mathrm{full}}(t)$ & tokens/step & naive full-chunk prefill pressure $C\sum_r a_r(t)$ \\
$p^{\mathrm{dec}}(t)$ & tokens/step & decode pressure from overlapping decode phases \\[0.5ex]

$\widehat\lambda_i$ & steps/sec & baseline phase-local step-density proxy $N_i/\ell_i$, Eq.~\eqref{eq:baseline_lambda_hat} \\
$c(t)$ & tokens/sec & baseline partial-chunk correction signal (none/uniform/end) \\[0.5ex]

$A_i^{\mathrm{pf}}$ & tokens & baseline prefill exposure (feature), Eq.~\eqref{eq:baseline_integrated_exposures_pf} \\
$A_i^{\mathrm{dec}}$ & tokens & baseline decode exposure (feature), Eq.~\eqref{eq:baseline_integrated_exposures_dec} \\[0.5ex]

$g_1<\cdots<g_{J+1}$ & seconds & trace-induced grid boundaries; cells are $[g_j,g_{j+1})$ \\
$\mathcal{J}(i)$ & set & grid cells fully contained in phase window of $i$ (EM) \\[0.5ex]

$Z_{ij}$ & steps & latent number of steps of phase $i$ executed in grid cell $j$ (EM) \\
$\pi^{(m)}_{ij}$ & unitless & EM allocation weight of phase $i$ to cell $j$ at iter.~$m$, Eq.~\eqref{eq:idealized_pi} \\
$\Delta^{(m)}_j$ & sec/step & EM cell-level step-time proxy at iter.~$m$, Eq.~\eqref{eq:idealized_step_rate} \\
$\lambda^{(m)}_j$ & steps/sec & EM cell-level step density $1/\Delta^{(m)}_j$, Eq.~\eqref{eq:idealized_step_rate} \\
$\tilde p^{\mathrm{pf},(m)}_j$ & tokens/step & EM effective prefill pressure on cell $j$ at iter.~$m$ \\[0.5ex]

$\beta_0$ & sec/step & fixed per-step overhead coefficient \\
$\beta_1,\beta_2$ & sec/token & per-token execution costs (prefill / decode) \\
$\beta^{(m)}$ & mixed & coefficient vector at EM iteration $m$ \\
\bottomrule
\end{tabular}
\caption{Notation and units used in trace-only step-level coefficient estimation (baseline and EM).}
\label{tab:notation_summary}
\end{table}


% ============================================================
\subsection{Problem Setup and Step-Level Model}
\label{sec:problem_setup}

We consider a vLLM-style inference engine that advances execution through a
single logical busy-loop.
Each busy-loop iteration (a \emph{step}) executes one forward-pass cycle that
may process prefill tokens, decode tokens, or both, for a set of concurrently
active requests.
Steps are assumed to execute sequentially.

Let $k$ index busy-loop steps.
We model the duration of step $k$ as
\begin{equation}
\Delta t_k
=
\beta_0
+
\beta_1\,T^{\mathrm{pf}}_k
+
\beta_2\,T^{\mathrm{dec}}_k,
\label{eq:step_model}
\end{equation}
where $\beta_0\ge 0$ is a fixed per-step overhead (seconds/step),
$\beta_1,\beta_2\ge 0$ are per-token costs (seconds/token), and
$T^{\mathrm{pf}}_k$ and $T^{\mathrm{dec}}_k$ are the total numbers of prefill and
decode tokens processed in step $k$ across all requests.

This affine step-time model is the fundamental abstraction used by our
simulator. Accurate simulation therefore requires identifying $\beta$ from
trace-only data.

\paragraph{What traces provide.}
Production inference traces do not expose step boundaries or per-step execution
timings.
Instead, they provide a collection of \emph{phase instances}, indexed by $i$,
each corresponding to either the prefill phase or the decode phase of a single
request.
For each phase instance $i$, the trace provides:
(i) the request id $r_i$,
(ii) the phase type $\tau_i\in\{\textsf{prefill},\textsf{decode}\}$,
(iii) start and end times $(t_{i,s},t_{i,e})$ and duration
$\ell_i=t_{i,e}-t_{i,s}$, and
(iv) aggregate token counts for that request, including total prefill tokens
$P_{r_i}$ and total decode tokens $D_{r_i}$.
Since each phase instance uniquely identifies its request, we write $P_r$ and
$D_r$ without ambiguity.

From these fields, we derive a trace-inferred step count $N_i\in\mathbb{N}$ for
each phase instance.
For a decode phase, execution proceeds one token per step, so $N_i=D_r$.
For a prefill phase, execution is chunked: under prefill chunk size $C$, a
standard trace-inferred choice is
$N_i=\lceil P_r/C\rceil$.\footnote{
Prefix-cached tokens do not contribute to execution steps and are therefore
excluded from $P_r$. This treatment is consistent with the execution semantics
of vLLM-style inference engines.
} For a prefill phase instance $i$ of request $r_i$, we will also write
$N_{r_i}$ for the inferred number of prefill steps, so $N_i=N_{r_i}$ for prefill
instances.


\paragraph{Latent step allocation viewpoint (common to baseline and EM).}
The central obstacle in trace-only estimation is that, although each phase
instance $i$ has a known total step count $N_i$, the trace does not reveal
\emph{when within} $[t_{i,s},t_{i,e})$ those $N_i$ steps occurred, nor how the
per-step token totals $(T^{\mathrm{pf}}_k,T^{\mathrm{dec}}_k)$ varied across
time.
Both the baseline and EM estimators operate by constructing
\emph{trace-only attribution models} that allocate this latent step exposure
over wall-clock time using only trace-visible phase boundaries.
These attribution models are not claims about true step boundaries; they are
used only through time integrals on a grid induced by the trace.


% ============================================================
\subsection{Trace-Derived Token Pressures}
\label{sec:pressures}

\paragraph{Intuition.}
Inference executes as a sequence of discrete steps, but traces provide only
coarse phase-level summaries.
The goal is not to reconstruct the hidden step schedule; rather, we seek
step-level coefficients that explain how prefill and decode work contribute to
elapsed time \emph{when aggregated over many steps}.
We therefore represent step-level workload using \emph{token pressures}:
trace-derived functions of wall-clock time that quantify, in units of
tokens/step, how much prefill and decode work would be attributed to a
representative busy-loop step executed at that time.
These pressures depend only on phase overlap in wall-clock time and can be
computed exactly from phase boundaries.
Figure~\ref{fig:overlap_phases} illustrates the resulting overlap structure.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=\linewidth]{figs/overlap_phases.pdf}
  \caption{\textbf{Phase overlap and inferred workload over wall-clock time.}
Each horizontal bar represents a trace-visible prefill or decode phase of a
request, while dashed vertical lines indicate wall-clock instants at which a
step boundary may occur. Although traces expose only phase start/end times—not
step boundaries—the degree of phase overlap determines how much work is executed
per step when time is aggregated. The estimators below convert overlap in
wall-clock time into token exposures consistent with the step-level model.}
\label{fig:overlap_phases}
\end{figure}

\paragraph{Trace-induced time grid.}
Let $\{[g_j,g_{j+1})\}_{j=1}^J$ denote the piecewise-constant time grid formed by
the union of all trace phase boundaries.
All pressures and correction signals introduced below are constant on each grid
cell, and all estimation procedures use them only through integrals over phase
windows.

\paragraph{Terminology: instantaneous quantities and exposures.}
We distinguish between \emph{instantaneous} quantities, defined as functions of
wall-clock time $t$ on the trace-induced grid, and \emph{exposures}, which are
obtained by integrating instantaneous quantities over a phase window and serve
as regression features.
Instantaneous \emph{pressures} quantify work performed per execution step
(tokens/step).
Here ``tokens/step'' means the number of tokens processed by a single busy-loop
iteration.
Exposures are phase-level scalars with units of tokens, obtained either by
(i) converting time-integrated pressures using a step-density surrogate
(baseline), or (ii) forming step-averaged pressures via a step-allocation model
and aggregating over a fixed step count (EM).

\paragraph{Instantaneous token pressures.}
We define two trace-derived, piecewise-constant pressures (tokens/step).
The decode pressure $p^{\mathrm{dec}}(t)$ is the total decode-token work that
would be attributed to a representative busy-loop step executed at time $t$,
given the set of requests whose trace-visible decode phases overlap $t$.
Formally, let $d_r(t)\in\{0,1\}$ indicate whether request $r$ is in its
\textsf{decode} phase at wall-clock time $t$.
Since decode processes exactly one token per active decode request per step, we
have the trace-derived decode pressure
\begin{equation}
p^{\mathrm{dec}}(t) = \sum_r d_r(t),
\qquad \text{units: tokens/step}.
\label{eq:decode_pressure_def}
\end{equation}


For prefill, we start from the \emph{naive full-chunk} pressure
\begin{equation}
p^{\mathrm{pf}}_{\mathrm{full}}(t)=C\sum_r a_r(t),
\label{eq:pf_full_pressure}
\end{equation}
where $a_r(t)\in\{0,1\}$ indicates whether request $r$ is in its prefill phase at
time $t$.
This full-chunk pressure treats each active prefill request as contributing $C$
tokens per step.
Accounting for the final partial prefill chunk is handled separately:
the baseline introduces a correction in \emph{exposure space} via a
time-based signal $c(t)$, whereas EM incorporates the correction directly in
\emph{step space} by modifying an iteration-dependent effective prefill pressure
on the trace grid.


\subsubsection{Prefill Chunking and Partial-Chunk Correction Signals}
\label{sec:prefill_correction}

Decode contributes exactly one token per active request per step.
Prefill, however, proceeds in chunks of size $C$ tokens per step, except for the
final prefill step of a request, which may process fewer than $C$ tokens.
Naively treating all prefill steps as full chunks introduces systematic bias.

For a request $r$ with $P_r$ prefill tokens and $N_r$ prefill steps, the final
prefill step processes
\begin{equation}
\rho_r = P_r - C\,(N_r-1),
\qquad \rho_r \in (0,C],
\label{eq:rho_def}
\end{equation}
and the missing token mass relative to a full chunk is
\[
\mu_r = C - \rho_r \in [0,C).
\]

\paragraph{No partial-chunk correction (baseline ablation).}
The simplest baseline ignores partial chunks and uses
$p^{\mathrm{pf}}_{\mathrm{full}}(t)$ as the effective prefill pressure. This
serves as a natural ablation: it is fully trace-only and makes no assumptions
about where the final prefill step occurs within a prefill interval.

\paragraph{Uniform redistribution correction (implemented in integrated form).}
A more refined baseline accounts for $\mu_r$ by redistributing it uniformly over
the prefill interval of request $r$.
Let $[t_{r,s},t_{r,e})$ denote the trace-visible prefill interval of request $r$.
Define a per-request correction signal (tokens/second)
\begin{equation}
c_r^{\mathrm{uni}}(t)
=
\frac{\mu_r}{t_{r,e}-t_{r,s}}\,
\mathbf{1}\{t\in[t_{r,s},t_{r,e})\},
\qquad
\text{units: tokens/second},
\label{eq:uniform_corr_signal}
\end{equation}
and the aggregate correction signal
\begin{equation}
c^{\mathrm{uni}}(t)=\sum_r c_r^{\mathrm{uni}}(t).
\label{eq:uniform_corr_signal_sum}
\end{equation}
By construction,
$\int_{t_{r,s}}^{t_{r,e}} c_r^{\mathrm{uni}}(t)\,dt=\mu_r$, so total missing mass
is preserved exactly when integrating over time.

\paragraph{End-localized correction (implemented on the trace grid).}
Uniform redistribution implicitly treats the final prefill step as equally
likely to occur anywhere within the prefill interval.
An alternative trace-only assumption is that the final prefill step completes
near the trace-visible end time $t_{r,e}$.
Let $j(r)$ be the index of the unique trace-grid cell containing $t_{r,e}$ under
the half-open convention (with boundary clamping at the end).
Define the per-request end-localized correction signal (tokens/second)
\begin{equation}
c_r^{\mathrm{end}}(t)
=
\frac{\mu_r}{g_{j(r)+1}-g_{j(r)}}\,
\mathbf{1}\{t\in[g_{j(r)},g_{j(r)+1})\},
\qquad
\text{units: tokens/second},
\label{eq:end_corr_signal}
\end{equation}
and the aggregate correction signal
\begin{equation}
c^{\mathrm{end}}(t)=\sum_r c_r^{\mathrm{end}}(t).
\label{eq:end_corr_signal_sum}
\end{equation}
Again, $\int c_r^{\mathrm{end}}(t)\,dt=\mu_r$ by construction.

\paragraph{Remark (why correction signals appear in the baseline).}
In the baseline, pressures are represented in units of tokens/step and enter the
estimator only through time integrals that are converted into token totals using
a phase-local step-density proxy (Section~\ref{sec:baseline}).
The partial-chunk adjustment is therefore represented as a \emph{time-based}
signal $c(t)$ (tokens/sec): its time integral is already in tokens and can be
subtracted in exposure space without introducing any step-boundary assumptions.

\paragraph{Implementation note (no pointwise reconstruction required).}
Although we define pressures and corrections as functions of wall-clock time, the
estimators require only integrals over phase windows such as
$\int_{t_{i,s}}^{t_{i,e}} p^{\mathrm{pf}}_{\mathrm{full}}(t)\,dt$ and
$\int_{t_{i,s}}^{t_{i,e}} c(t)\,dt$.
These can be computed exactly by sweeping the trace-induced time grid.

\subsection{Baseline: Time-Integrated NNLS Estimation}
\label{sec:baseline}

We first present a simple, trace-only baseline estimator that fits phase
durations using time-integrated pressures.
Because pressures are expressed in \emph{tokens per step}, a time integral
$\int p(t)\,dt$ has units $(\text{tokens}/\text{step})\cdot \text{seconds}$.
To form regression features with units of \emph{tokens}---so that
$\beta_1,\beta_2$ retain the interpretation ``seconds per token''---we
convert time integrals into approximate \emph{step integrals} using a
phase-local constant step density.

\paragraph{Phase-local step density.}
For phase instance $i$, let $T_i=t_{i,e}-t_{i,s}$ be its observed duration and
let $N_i\in\mathbb{N}$ be the trace-inferred step count.
We define the (phase-local) step density proxy
\begin{equation}
\widehat\lambda_i \;=\; \frac{N_i}{T_i},
\qquad
\text{units: steps/second}.
\label{eq:baseline_lambda_hat}
\end{equation}
This assumes steps are spread approximately uniformly over the phase window,
and is used \emph{only} to convert time integrals into step-aggregated token
totals in the baseline.\footnote{Implementation detail: to avoid extreme $\widehat{\lambda}_i$ when $T_i$
is very small, we may compute $\widehat{\lambda}_i = N_i/\max\{T_i, T_{\min}\}$ for a
small $T_{\min}\ge 0$. The default $T_{\min}=0$ recovers Eq.~\eqref{eq:baseline_lambda_hat}.}


\paragraph{Integrated token exposures (baseline features).}
We define baseline integrated exposures
\begin{align}
A^{\mathrm{pf}}_i
&=
\int_{t_{i,s}}^{t_{i,e}}
\tilde p^{\mathrm{pf}}(t)\,\widehat\lambda_i\,dt,
&
A^{\mathrm{dec}}_i
&=
\int_{t_{i,s}}^{t_{i,e}}
p^{\mathrm{dec}}(t)\,\widehat\lambda_i\,dt,
\label{eq:baseline_integrated_exposures}
\end{align}
which have units of tokens because $\tilde p^{\mathrm{pf}}(t)$ and
$p^{\mathrm{dec}}(t)$ are in tokens/step and $\widehat\lambda_i$ is in
steps/second.

\paragraph{Baseline predictor.}
The baseline predicts phase duration as
\begin{equation}
\widehat T_i(\beta)
=
\beta_0\,N_i
+
\beta_1\,A^{\mathrm{pf}}_i
+
\beta_2\,A^{\mathrm{dec}}_i,
\label{eq:baseline_predictor}
\end{equation}
where $\beta_0$ has units seconds/step and $\beta_1,\beta_2$ have units
seconds/token, consistent with the step-level model in
Eq.~\eqref{eq:step_model}.

\paragraph{NNLS estimation.}
Given a set of phase instances $\mathcal{I}$, we estimate $\beta$ via
non-negative least squares:
\begin{equation}
\min_{\beta \in \mathbb{R}^3_{\ge 0}}
\sum_{i\in\mathcal{I}}
\left(
\widehat T_i(\beta) - T_i
\right)^2 .
\label{eq:baseline_nnls}
\end{equation}

We emphasize that the simulator generates its own step-level token counts 
and KV-cache dynamics; the only information learned from traces is the 
estimated $\beta$, which parameterizes the simulator but does not 
encode any trace-specific timing information.

The pseudocode for the baseline estimation procedure is in 
Algorithm \ref{alg:baseline}.

\begin{algorithm}[t]
\caption{Baseline algorithm for trace-only estimation}
\label{alg:baseline}
\begin{algorithmic}[1]
\Require Instances $\{(\text{request id }r_i,\ \text{type }\tau_i\in\{\textsf{prefill},\textsf{decode}\},\ t_{i,s},t_{i,e},\ P_i,D_i)\}_{i=1}^n$, chunk size $C$.
\Ensure $\hat\beta=(\hat\beta_0,\hat\beta_1,\hat\beta_2)\in\mathbb{R}^3_{\ge 0}$.
\State \textbf{Infer step counts.} $N_i\gets \lceil P_i/C\rceil$ if $\tau_i=\textsf{prefill}$; else $N_i\gets D_i$.
\State \textbf{Durations and step density.} $T_i\gets t_{i,e}-t_{i,s}$; $\widehat\lambda_i\gets N_i/T_i$.
\State \textbf{Global time grid.} $\mathcal{G}=\{g_0<\dots<g_M\}\gets\mathrm{sort}\big(\{t_{i,s}\}_i\cup\{t_{i,e}\}_i\big)$; segments are $[g_j,g_{j+1})$.
\State \textbf{Piecewise-constant signals on segments.} For each segment $j=0,\dots,M-1$:
\[
\begin{aligned}
p^{\mathrm{pf}}_{\mathrm{full},j}
&\gets C \cdot \bigl(\#\text{ requests prefilling during }[g_j,g_{j+1})\bigr),\\
p^{\mathrm{dec}}_{j}
&\gets \bigl(\#\text{ requests decoding during }[g_j,g_{j+1})\bigr).
\end{aligned}
\]
\State \textbf{Prefill correction (uniform redistribution).}
\For{each request $r$}
  \State Let $i$ denote the (unique) \textsf{prefill} phase instance of request $r$.
  \State Set missing mass $\mu_r \gets C N_i - P_i$ and uniform rate
  $\nu_r \gets \mu_r/(t_{i,e}-t_{i,s})$ \quad (tokens/sec).
\EndFor
\State For each segment $[g_j,g_{j+1})$, set the aggregate correction rate (tokens/sec)
\[
\begin{aligned}
u_j \;\;&\gets \sum_r \nu_r,
\quad \text{where the sum is over all requests $r$ whose \textsf{prefill} interval overlaps with }[g_j,g_{j+1}) .
\end{aligned}
\]
\For{each phase instance $i$}
  \State Let $\mathcal{J}(i)$ be the set of grid segments $j$ whose interval
  $[g_j,g_{j+1})$ overlaps the phase window $[t_{i,s},t_{i,e})$.
  \State For each $j\in\mathcal{J}(i)$, let
  $\Delta t_{ij} \gets \bigl|\,[t_{i,s},t_{i,e})\cap[g_j,g_{j+1})\,\bigr|$.
  \State Compute exact segment sums:
  \[
  \begin{aligned}
  J^{\mathrm{pf}}_i &\gets \sum_{j\in\mathcal{J}(i)} p^{\mathrm{pf}}_{\mathrm{full},j}\,\Delta t_{ij},\\
  J^{\mathrm{dec}}_i &\gets \sum_{j\in\mathcal{J}(i)} p^{\mathrm{dec}}_{j}\,\Delta t_{ij},\\
  U_i &\gets \sum_{j\in\mathcal{J}(i)} u_j\,\Delta t_{ij}.
  \end{aligned}
  \]
  \State Convert to token exposures:
  $A^{\mathrm{pf}}_i\gets \widehat\lambda_i J^{\mathrm{pf}}_i-U_i$,\;
  $A^{\mathrm{dec}}_i\gets \widehat\lambda_i J^{\mathrm{dec}}_i$.
\EndFor
\State \textbf{NNLS fit.}
$\hat\beta\gets\arg\min_{\beta\ge0}\sum_i(\beta_0N_i+\beta_1A^{\mathrm{pf}}_i+\beta_2A^{\mathrm{dec}}_i-T_i)^2$.
\State \Return $\hat\beta$.
\end{algorithmic}
\end{algorithm}



\paragraph{Limitation of the baseline.}
This estimator remains wall-clock--weighted because $A_i^{\mathrm{pf}}$ and
$A_i^{\mathrm{dec}}$ are formed by integrating over time within each phase,
using the phase-average step density proxy $\widehat\lambda_i$ rather than a
time-varying step density.
As a result, phases that coincide with long-step periods can exert
disproportionate influence, motivating the step-density--reweighted estimator
in Section~\ref{sec:em_continuous_kernelized}.

% ============================================================
\section{Featurized Execution and Client Coefficients}
\label{sec:featurized_coefficients}

BLIS is designed to operate across a wide range of deployment settings,
including different LLM architectures, GPU and interconnect backends, and
serving-engine configurations.
Re-estimating execution and client-overhead coefficients independently for
every setting would be both costly and statistically inefficient.
In this section, we introduce a \emph{featurized coefficient model} that enables
systematic transfer across environments while preserving the simplicity and
interpretability of the trace-decomposed baseline estimator.

\paragraph{Key idea.}
Rather than treating execution coefficients as environment-specific constants,
we model them as nonnegative linear functions of \emph{positive environment
features}.
This allows BLIS to:
(i) learn a single global model from heterogeneous trace and client data,
(ii) generalize to unseen LLM/GPU/configuration combinations, and
(iii) support lightweight recalibration when necessary.

% ------------------------------------------------------------
\subsection{Environments and positive features}

Let $e\in\mathcal{E}$ index a deployment environment.
An environment corresponds to a fixed combination of:
LLM architecture and size, GPU type and interconnect topology, and inference
engine configuration (e.g., chunk size, parallelism, scheduler settings).

Each environment is described by a feature vector
\[
x_e \in \mathbb{R}^{d_x}_{\ge 0},
\]
whose entries are \emph{nonnegative} quantities such as:
model parameter count, hidden dimension, KV-cache footprint, GPU FLOPs,
memory bandwidth, interconnect bandwidth/latency, and configuration flags
encoded as $0/1$.
We explicitly include a constant intercept feature to represent baseline costs.

For client-visible overheads (networking, serialization, streaming), we define
an analogous nonnegative feature vector
\[
z_e \in \mathbb{R}^{d_z}_{\ge 0}.
\]
In many deployments $x_e$ and $z_e$ coincide, but we keep them distinct to allow
client-side effects (e.g., network topology) to vary independently of GPU
execution.

% ------------------------------------------------------------
\subsection{Featurized coefficient model}

We parameterize execution and client coefficients as
\begin{equation}
\beta_e = x_e^\top B,
\qquad
\alpha_e = z_e^\top A,
\label{eq:featurized_linear_model}
\end{equation}
where
\[
B \in \mathbb{R}^{d_x \times 3}_{\ge 0},
\qquad
A \in \mathbb{R}^{d_z \times 3}_{\ge 0}.
\]

Here $\beta_e=(\beta_{e,0},\beta_{e,1},\beta_{e,2})$ are the step-level execution
coefficients (sec/step, sec/token for prefill, sec/token for decode), and
$\alpha_e=(\alpha_{e,0},\alpha_{e,1},\alpha_{e,2})$ are client-side overhead
coefficients (seconds, sec/token for input, sec/token for output).

Because features and matrices are nonnegative, Eq.~\eqref{eq:featurized_linear_model}
guarantees $\beta_e,\alpha_e\ge 0$ for \emph{all} environments, including those
not seen during training.
This preserves physical interpretability and avoids the need for nonlinear link
functions.

\paragraph{Interpretation.}
Each feature contributes additively and nonnegatively to each coefficient.
This structure is intentionally simple: it matches how system resources
contribute to execution cost and supports convex estimation at scale.

% ------------------------------------------------------------
\subsection{Learning execution maps from traces}

For each environment $e$, trace data provides phase instances
$i\in\mathcal{I}_e$ with observed duration $T_i$, step count $N_i$, and integrated
token exposures $(A_i^{\mathrm{pf}},A_i^{\mathrm{dec}})$ constructed exactly as in
the trace-decomposed baseline (Eq~\eqref{eq:baseline_integrated_exposures}).

Substituting $\beta_e=x_e^\top B$ into the trace predictor yields
\[
T_i
\approx
N_i(x_e^\top b_0)
+
A_i^{\mathrm{pf}}(x_e^\top b_1)
+
A_i^{\mathrm{dec}}(x_e^\top b_2),
\]
which is linear in the unknown entries of $B$.
Defining the per-instance feature row
\[
\phi_i :=
\big[
N_i x_e^\top,\;
A_i^{\mathrm{pf}} x_e^\top,\;
A_i^{\mathrm{dec}} x_e^\top
\big],
\]
we stack all trace instances across environments and estimate $B$ via a single
global nonnegative least-squares problem:
\begin{equation}
\min_{\mathrm{vec}(B)\ge 0}
\left\|
\Phi_{\mathrm{trace}}\,\mathrm{vec}(B) - T
\right\|_2^2.
\label{eq:nnls_B_featurized}
\end{equation}

\paragraph{Why this works.}
Trace-observed phase durations already include admission delay, batching delay,
and GPU contention.
As a result, $B$ is learned directly from server-visible execution behavior,
without requiring any explicit queueing model.

% ------------------------------------------------------------
\subsection{Learning client coefficients from trace-decomposed residuals}

For environments with paired client measurements, each request $r$ provides
end-to-end latency $y_r$ and trace-observed server-side latency
$S_r^{\mathrm{trace}}$.
We form the residual
\[
R_r := y_r - S_r^{\mathrm{trace}},
\]
which isolates client-visible effects not seen by the server.

Substituting $\alpha_e=z_e^\top A$ into the client model yields a linear regression
in $A$.
Defining
\[
\psi_r :=
\big[
1\cdot z_e^\top,\;
\ell_r^{\mathrm{in}} z_e^\top,\;
\ell_r^{\mathrm{out}} z_e^\top
\big],
\]
we estimate $A$ via global NNLS:
\begin{equation}
\min_{\mathrm{vec}(A)\ge 0}
\left\|
\Psi_{\mathrm{client}}\,\mathrm{vec}(A) - R
\right\|_2^2.
\label{eq:nnls_A_featurized}
\end{equation}

\paragraph{Separation of concerns.}
This decomposition cleanly separates estimation:
execution coefficients $B$ are learned solely from traces, while client
coefficients $A$ are learned solely from residuals after subtracting trace-visible
server latency.
Neither estimation step is contaminated by the other.

\paragraph{Transfer and one-shot recalibration.}
Given learned maps $(\widehat B,\widehat A)$, a new environment $e^\star$ requires
only its feature vectors $(x_{e^\star},z_{e^\star})$ to produce coefficients:
\[
\beta^{\mathrm{feat}}_{e^\star} := x_{e^\star}^\top \widehat B,
\qquad
\alpha^{\mathrm{feat}}_{e^\star} := z_{e^\star}^\top \widehat A.
\]
This supports \emph{zero-shot transfer} across LLMs, GPU/interconnect backends,
and engine configurations.

When traces are available for $e^\star$, BLIS performs \emph{one-shot
recalibration} by solving a trace NNLS objective regularized toward the featurized
prediction:
\begin{equation}
\widehat \beta_{e^\star}
=
\arg\min_{\beta\in\mathbb{R}^3_{\ge 0}}
\sum_{i\in\mathcal{I}_{e^\star}}
\left(
\beta_0 N_i
+
\beta_1 A_i^{\mathrm{pf}}
+
\beta_2 A_i^{\mathrm{dec}}
-
T_i
\right)^2
\;+\;
\lambda_{\mathrm{recal}}
\left\|
\beta-\beta^{\mathrm{feat}}_{e^\star}
\right\|_2^2.
\label{eq:recalibration_to_featurized_prior}
\end{equation}
This objective remains convex and retains the same coefficient semantics as the
baseline trace estimator.
The regularizer implements partial pooling: it allows environment-specific
deviations when supported by trace evidence, while preventing unstable fits when
only a small trace window is available.

\paragraph{One-shot recalibration of client-side overheads.}
For a new environment $e^\star$ with paired client and trace data, we form
trace-decomposed residuals
\[
R_r := y_r - S_r^{\mathrm{trace}},
\qquad r\in\mathcal{J}_{e^\star},
\]
where $y_r$ is end-to-end latency observed at the client and
$S_r^{\mathrm{trace}}$ is the server-side latency observable from the trace
(admission, batching, and execution).

Given the featurized prediction
\[
\alpha^{\mathrm{feat}}_{e^\star} := z_{e^\star}^\top \widehat A,
\]
BLIS recalibrates $\alpha_{e^\star}$ by solving a regularized nonnegative least
squares problem:
\begin{equation}
\widehat \alpha_{e^\star}
=
\arg\min_{\alpha\in\mathbb{R}^3_{\ge 0}}
\sum_{r\in\mathcal{J}_{e^\star}}
\left(
\alpha_0
+
\alpha_1 \ell_r^{\mathrm{in}}
+
\alpha_2 \ell_r^{\mathrm{out}}
-
R_r
\right)^2
\;+\;
\lambda_{\mathrm{recal}}^{\alpha}
\left\|
\alpha-\alpha^{\mathrm{feat}}_{e^\star}
\right\|_2^2.
\label{eq:alpha_recalibration_to_featurized_prior}
\end{equation}
This objective is convex and decoupled from server scheduling dynamics by
construction, since all server-side queueing and interference are absorbed into
$S_r^{\mathrm{trace}}$ and removed before fitting $\alpha$.


% ------------------------------------------------------------
\subsection{Identifiability and complexity}

Identifiability of $B$ and $A$ requires diversity in environment features.
If feature vectors are nearly collinear, the global design matrices become
ill-conditioned.
In practice, variation in model size, parallelism, GPU generation, and
interconnect properties is sufficient to yield well-conditioned estimates.

Feature construction is linear in the number of trace instances and client
requests.
The NNLS problems in Eqs.~\eqref{eq:nnls_B_featurized} and
\eqref{eq:nnls_A_featurized} are convex with $3d_x$ and $3d_z$ unknowns,
respectively, and can be solved efficiently using standard QP/NNLS solvers.

\paragraph{Summary.}
Featurized coefficients allow BLIS to learn portable execution and client-overhead
models from heterogeneous data, while preserving the simplicity, convexity, and
interpretability of the trace-decomposed baseline.

% ============================================================
\section{Continuous Kernelized EM with Boundary-Aware Step Densities}
\label{sec:em_continuous_kernelized}

We now present a fully continuous-time EM formulation that retains the
conceptual elegance of the idealized EM while achieving the same robustness
to instrumentation noise as the kernelized micro-grid EM, without introducing
any explicit discretization of time.
This formulation operates directly on continuous latent step densities and
may be viewed as the limit of the kernelized grid EM as grid resolution tends
to zero.

The key ingredients are:
(i) continuous boundary-aware phase membership kernels,
(ii) end-localized kernels for the final prefill step,
and (iii) optional low-pass smoothing of trace-derived pressures.
Together, these yield a numerically stable, trace-only EM algorithm with a
minimal number of tunable parameters.

% ------------------------------------------------------------
\paragraph{Step-rate model (iteration-indexed).}
At EM iteration $m$, the E-step allocates step mass using the step-rate induced
by the \emph{previous} iterate $(\beta^{(m-1)},\tilde p^{\mathrm{pf},(m-1)})$.
Define the local step-time proxy and step-rate (intensity)
\begin{equation}
\Delta^{(m)}(t)
=
\beta^{(m-1)}_0
+
\beta^{(m-1)}_1\,\tilde p^{\mathrm{pf},(m-1)}(t)
+
\beta^{(m-1)}_2\,p^{\mathrm{dec}}(t),
\qquad
\lambda^{(m)}(t)=\frac{1}{\Delta^{(m)}(t)},
\label{eq:continuous_step_rate}
\end{equation}
where $\lambda^{(m)}(t)$ has units of steps/sec and is interpreted as the
instantaneous density of the latent global busy-loop steps.

Although $\Delta^{(m)}(t)$ and $\lambda^{(m)}(t)$ are indexed by $m$, they depend
only on quantities from iteration $m-1$; the superscript $(m)$ indicates that
they are computed \emph{during} iteration $m$ and used in its E-step.

\paragraph{Well-definedness.}
For each iteration $m$, we assume $\Delta^{(m)}(t)>0$ for all $t$ in the trace
support (e.g., by enforcing $\beta^{(m-1)}_0\ge \beta_{0,\min}>0$ or, equivalently,
clamping $\Delta^{(m)}(t)\leftarrow \max\{\Delta^{(m)}(t),\Delta_{\min}\}$ for a
small $\Delta_{\min}>0$). This ensures $\lambda^{(m)}(t)$ is finite and the
normalizations below are well-defined.


% ------------------------------------------------------------
\paragraph{Boundary-aware phase membership kernels.}

Let phase instance $i$ have trace-visible start and end times
$(t_{i,s},t_{i,e})$.
We assume a positive-delay instrumentation model
$t^{\mathrm{obs}} = t^\star + \varepsilon$ with $\varepsilon \ge 0$.
Under this model, uncertainty about phase membership arises only near the
left side of observed boundaries.

We define a continuous membership function
$w_i(t)\in[0,1]$ that represents the plausibility that wall-clock time $t$
belongs to the true execution window of phase $i$.
Specifically,
\begin{equation}
w_i(t)
=
F_h(t - t_{i,s})\,
\bigl(1 - F_h(t - t_{i,e})\bigr),
\label{eq:phase_membership_kernel}
\end{equation}
where $F_h$ is a one-sided cumulative kernel with bandwidth $h>0$, satisfying:
\[
F_h(x)=0 \;\; \text{for } x\le 0,
\qquad
F_h(x)\uparrow 1 \;\; \text{as } x\gg h.
\]
A concrete choice is $F_h(x)=1-e^{-x/h}$ for $x\ge 0$.

By construction, $w_i(t)=1$ throughout the interior of the observed phase
window and deviates from unity only within an $O(h)$ neighborhood of the
boundaries.
No membership mass is assigned beyond the observed phase end.

% ------------------------------------------------------------
\paragraph{Optional pressure smoothing.}

Trace-derived pressures $p^{\mathrm{pf}}_{\mathrm{full}}(t)$ and
$p^{\mathrm{dec}}(t)$ may exhibit high-frequency artifacts due to timestamp
noise.
To suppress such artifacts, we optionally apply a symmetric low-pass kernel
$G_h$ with the same bandwidth $h$:
\[
p(t) \;\leftarrow\; (G_h * p)(t).
\]
This smoothing affects only numerical stability and does not alter the
statistical structure of the EM formulation.

% ------------------------------------------------------------
\paragraph{Latent step densities and E-step.}

For each phase instance $i$ with trace-inferred step count $N_i$, we model
the $N_i$ execution steps as latent events distributed in continuous time
according to the step-rate model. 
Conditioned on the previous iterate $(\beta^{(m-1)},\tilde p^{\mathrm{pf},(m-1)})$
(equivalently, on the step-rate $\lambda^{(m)}$ computed in
Eq.~\eqref{eq:continuous_step_rate}), the posterior expected step-density for phase
$i$ is
\begin{equation}
q_i^{(m)}(t)
=
\frac{w_i(t)\,\lambda^{(m)}(t)}
{\int w_i(u)\,\lambda^{(m)}(u)\,du}.
\label{eq:phase_step_density}
\end{equation}
which integrates to one over $\mathbb{R}$.
The expected step mass in an infinitesimal interval is then
\[
\mathbb{E}[N_i(dt)]
=
N_i\,q_i^{(m)}(t)\,dt.
\]

This continuous density is the direct analogue of fractional step allocation
in the kernelized grid EM.

% ------------------------------------------------------------
\paragraph{Final prefill step localization.}

For a prefill request $r$, exactly one of its $N_r$ steps corresponds to the
final partial chunk.
Let $\mu_r$ denote the missing token mass of that step.

We define a boundary-local kernel $\varphi_r(t)$ concentrated immediately
before the observed prefill end time $t_{r,e}$:
\begin{equation}
\varphi_r(t)
=
\frac{K_h(t_{r,e}-t)\,\mathbf{1}\{t\le t_{r,e}\}}
{\int K_h(t_{r,e}-u)\,\mathbf{1}\{u\le t_{r,e}\}\,du},
\label{eq:final_step_kernel}
\end{equation}
where $K_h$ is a one-sided kernel on $\mathbb{R}_{\ge 0}$ (e.g., exponential or
truncated Gaussian).
This kernel integrates to one and represents the posterior density of the
final step’s location under positive delay.

The corresponding final-step density (using the step-rate $\lambda^{(m)}$
computed in Eq.~\eqref{eq:continuous_step_rate}) is
\begin{equation}
q^{(m)}_{r,\mathrm{final}}(t)
=
\frac{\varphi_r(t)\,\lambda^{(m)}(t)}
{\int \varphi_r(u)\,\lambda^{(m)}(u)\,du}.
\label{eq:final_step_density}
\end{equation}

The remaining $(N_r-1)$ bulk prefill steps are distributed according to
$q_i^{(m)}(t)$.

% ------------------------------------------------------------
\paragraph{Exact partial-chunk correction.}

The expected rate of final-step events per unit time is $q^{(m)}_{r,\mathrm{final}}(t)$,
which integrates to one.
To convert missing-token mass into a per-step correction, we normalize by the
global step-rate:
\begin{equation}
\delta^{(m)}(t)
=
\frac{\sum_r \mu_r\, q^{(m)}_{r,\mathrm{final}}(t)}
{\lambda^{(m)}(t)},
\qquad
\text{units: tokens/step}.
\label{eq:continuous_partial_chunk}
\end{equation}

The effective prefill pressure is then
\begin{equation}
\tilde p^{\mathrm{pf},(m)}(t)
=
p^{\mathrm{pf}}_{\mathrm{full}}(t) - \delta^{(m)}(t).
\end{equation}
By construction, each request contributes exactly one partial chunk in
expectation, independent of timestamp noise or numerical discretization.

% ------------------------------------------------------------
\paragraph{M-step.}

Given the expected step densities, we compute step-averaged pressures
\begin{align}
\bar p^{\mathrm{pf},(m)}_i
&=
\int q_i^{(m)}(t)\,\tilde p^{\mathrm{pf},(m)}(t)\,dt,
\\
\bar p^{\mathrm{dec},(m)}_i
&=
\int q_i^{(m)}(t)\,p^{\mathrm{dec}}(t)\,dt.
\end{align}

Coefficients are updated by solving the same non-negative least squares problem
as in the idealized EM:
\[
\beta^{(m)}
=
\arg\min_{\beta\ge 0}
\sum_{i\in\mathcal{I}}
\left(
N_i\beta_0
+
[N_i\bar p^{\mathrm{pf},(m)}_i]\beta_1
+
[N_i\bar p^{\mathrm{dec},(m)}_i]\beta_2
-
\ell_i
\right)^2.
\]

% ------------------------------------------------------------
\paragraph{Initialization and convergence.}

The algorithm is initialized using the baseline trace-only NNLS estimator,
yielding $\beta^{(0)}$, and we set the initial effective prefill pressure to the
full-chunk pressure:
\[
\tilde p^{\mathrm{pf},(0)}(t)=p^{\mathrm{pf}}_{\mathrm{full}}(t).
\]
Iteration $m=1$ then computes $\lambda^{(1)}(t)$ from
$(\beta^{(0)},\tilde p^{\mathrm{pf},(0)})$ and performs the first step-aware
allocation. Subsequent iterations incorporate exact partial-chunk correction
through $\delta^{(m)}(t)$.

Each iteration monotonically decreases the observed-data squared-error
objective.
The continuous formulation reduces to the idealized EM under hard membership
kernels and to the kernelized grid EM under Riemann discretization.

\paragraph{Summary.}
This continuous kernelized EM formulation unifies the baseline, MM-style, and
grid-based EM approaches within a single statistically principled framework.
Boundary uncertainty is handled explicitly, partial-chunk correction remains
exact in expectation, and robustness is achieved without sacrificing analytical
clarity or introducing per-phase timing parameters.

% ============================================================
\subsection{Featurized Continuous EM for Execution Coefficients}
\label{sec:featurized_continuous_em}

We now extend the continuous kernelized EM formulation to the featurized setting
introduced in Section~\ref{sec:featurized_coefficients}.
This extension allows BLIS to learn a \emph{single global execution map}
that transfers across environments, while retaining the step-density–aware
refinement provided by the EM procedure.

As in the baseline case, featurization applies \emph{only} to server-side
execution coefficients.
Client-side coefficients $\alpha$ and the matrix $A$ are estimated independently
from trace-decomposed residuals and are unaffected by the EM procedure.

% ------------------------------------------------------------
\paragraph{Environment-indexed traces.}

Let $e\in\mathcal{E}$ index deployment environments, each described by a
nonnegative feature vector $x_e\in\mathbb{R}^{d_x}_{\ge 0}$.
For environment $e$, trace data provides a set of phase instances
$i\in\mathcal{I}_e$, each with:
trace-inferred step count $N_i$,
phase duration $\ell_i$,
and EM-derived step-averaged pressures
$\bar p^{\mathrm{pf},(m)}_i$ and $\bar p^{\mathrm{dec},(m)}_i$ at iteration $m$.

% ------------------------------------------------------------
\paragraph{Featurized execution model.}

We parameterize environment-specific execution coefficients as
\begin{equation}
\beta_e = x_e^\top B,
\qquad
B \in \mathbb{R}^{d_x \times 3}_{\ge 0},
\label{eq:featurized_em_beta}
\end{equation}
where columns of $B$ correspond to $(\beta_0,\beta_1,\beta_2)$.
This guarantees $\beta_e\ge 0$ for all environments and preserves the physical
interpretation of coefficients.

Substituting $\beta_e$ into the step-level model yields the EM predictor
\begin{equation}
\widehat \ell_i(B)
=
N_i (x_e^\top b_0)
+
N_i \bar p^{\mathrm{pf},(m)}_i (x_e^\top b_1)
+
N_i \bar p^{\mathrm{dec},(m)}_i (x_e^\top b_2),
\label{eq:featurized_em_predictor}
\end{equation}
which is linear in the unknown entries of $B$.

% ------------------------------------------------------------
\paragraph{E-step (unchanged).}

The E-step is identical to that of Section~\ref{sec:em_continuous_kernelized}.
Given the execution coefficients from the previous iteration,
$\beta^{(m-1)}_e = x_e^\top B^{(m-1)}$, we compute:
\begin{itemize}
\item the environment-specific step-rate
\(
\lambda^{(m)}_e(t)
\),
\item phase membership densities $q^{(m)}_i(t)$,
\item final-step densities $q^{(m)}_{r,\mathrm{final}}(t)$,
\item effective prefill pressure $\tilde p^{\mathrm{pf},(m)}_e(t)$,
\end{itemize}
exactly as in the unfeaturized continuous EM.

No additional assumptions or approximations are introduced by featurization.

% ------------------------------------------------------------
\paragraph{M-step (global, featurized).}

Given step-averaged pressures from the E-step, we update the global execution map
by solving a single nonnegative least squares problem across all environments:
\begin{equation}
B^{(m)}
=
\arg\min_{B\ge 0}
\sum_{e\in\mathcal{E}}
\sum_{i\in\mathcal{I}_e}
\left(
\widehat \ell_i(B) - \ell_i
\right)^2.
\label{eq:featurized_em_mstep}
\end{equation}

This objective is convex in $\mathrm{vec}(B)$ and can be solved using standard
NNLS or QP solvers.
The resulting update produces new environment-specific coefficients
$\beta^{(m)}_e=x_e^\top B^{(m)}$, which are then used in the next E-step.

% ------------------------------------------------------------
\paragraph{Initialization.}

The featurized EM algorithm is initialized using the featurized baseline
estimator from Section~\ref{sec:featurized_coefficients}.
Specifically, we set
\[
B^{(0)} := \widehat B_{\mathrm{baseline}},
\qquad
\beta^{(0)}_e = x_e^\top B^{(0)},
\qquad
\tilde p^{\mathrm{pf},(0)}_e(t) = p^{\mathrm{pf}}_{\mathrm{full}}(t).
\]

This ensures that the first EM iteration corresponds to a step-density–aware
reweighting of the baseline solution.

% ------------------------------------------------------------
\paragraph{Regularization and partial pooling (optional).}

To improve numerical stability when feature vectors are correlated or when some
environments have limited trace coverage, we optionally add a ridge penalty:
\begin{equation}
\lambda_B \| B - B^{(0)} \|_F^2
\end{equation}
to Eq.~\eqref{eq:featurized_em_mstep}.
This induces partial pooling across environments and prevents overfitting
without altering the E-step or coefficient semantics.

% ------------------------------------------------------------
\paragraph{Interpretation and scope.}

The featurized continuous EM procedure learns a portable, step-level execution
model that:
(i) captures fine-grained step-density effects from traces,
(ii) transfers across LLM, GPU, and configuration regimes,
and (iii) remains trace-only and convex in its M-step.

Client-side latency coefficients $\alpha$ and the feature map $A$ are unchanged
by this procedure and are estimated independently using trace-decomposed
residuals, as described in Section~\ref{sec:featurized_coefficients}.

\paragraph{Summary.}
Featurized continuous EM combines the statistical efficiency of shared execution
maps with the fidelity of step-aware latent allocation, yielding a unified,
trace-only framework for learning transferable inference execution models.


\bibliographystyle{plain}
\bibliography{blis}

\end{document}
