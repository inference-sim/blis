\documentclass[11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{times}
\usepackage{graphicx}
\usepackage{amsmath, amssymb}
\usepackage{mathtools} % for \xmapsto
\usepackage{bm}
\usepackage[most]{tcolorbox}
\usepackage{tabularx}
\usepackage{pifont} % optional for symbols
% --- Flowcharts (TikZ) ---
\usepackage{tikz}
\usetikzlibrary{arrows.meta,positioning,shapes.geometric,fit,calc}
\usepackage{xcolor}
% Algorithms (compact)
\usepackage{algorithm}
\usepackage{algpseudocode}

% Optional: slightly tighter algorithm typography
\algrenewcommand\algorithmicrequire{\textbf{Input:}}
\algrenewcommand\algorithmicensure{\textbf{Output:}}
\algrenewcommand\algorithmiccomment[1]{\hfill{\footnotesize$\triangleright$~#1}}

\usepackage{hyperref}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{booktabs,longtable,array,makecell}
\setlength{\LTleft}{0pt}   % no left indent for longtable
\setlength{\LTright}{0pt}  % no right indent for longtable
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}p{#1}}


% --- Theorems, lemmas, claims, proofs ---
\usepackage{amsthm}

\theoremstyle{plain} % bold headings, italic body
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{claim}[theorem]{Claim}

\theoremstyle{definition} % bold heading, upright body
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark} % italic heading, upright body
\newtheorem{remark}[theorem]{Remark}
\newtheorem{note}[theorem]{Note}

% Optional: unnumbered versions
\newtheorem*{theorem*}{Theorem}
\newtheorem*{lemma*}{Lemma}
\newtheorem*{claim*}{Claim}

% Proof environment is provided by amsthm:
% \begin{proof} ... \end{proof}
% Customize QED symbol if you like:
\renewcommand{\qedsymbol}{$\blacksquare$}

\newcommand{\code}[1]{\texttt{#1}}

\definecolor{prefillcolor}{RGB}{173,216,230}  % Light blue
\definecolor{decodecolor}{RGB}{144,238,144}   % Light green
\definecolor{gridcolor}{RGB}{200,200,200}     % Light gray
\definecolor{highlightcolor}{RGB}{255,182,193} % Light pink for integration region


% (Optional) If you want equations numbered by section (since you already use amsmath):
\numberwithin{equation}{section}



\title{BLIS: Blackbox Inference Performance Estimator}
\author{AI Platform Optimization Team}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This paper introduces BLIS, a system for blackbox inference performance estimation 
designed to model inference request flows and latency.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

\section{Related Work}
\label{sec:related-work}

\section{Design overview}
\label{sec:design-overview}

BLIS is primarily geared towards vLLM; we discuss extensions 
to other inference platforms (e.g., sglang) in future work.

\subsection{vLLM overview}
\label{subsec:vllm-overview}
We will focus on handling of an inference request by vLLM. vLLM
has two main components: the API server and the engine core; these operate as separate threads and
communicate through a message queue.

\subsubsection{API server thread}
The API server is implemented using FastAPI. Its role is to handle client-facing tasks without blocking on actual model execution.
The API thread is responsible for:
\begin{enumerate}
    \item Receiving and tokenizing incoming requests from clients.
    \item Enqueuing tokenized requests into the message queue.
    \item Streaming back partial responses as they become available from the engine.
    \item Detokenizing completed sequences and packaging them into response
          objects.
    \item Emitting the final response back to the client.
\end{enumerate}

\subsubsection{Engine core thread}
The engine core runs the central \emph{inference loop}. This loop is driven by a
scheduler that repeatedly:
\begin{enumerate}
    \item Collects pending requests from the queue, forming a dynamic batch.
    \item Determines whether a request is in a \emph{prefill} (first token)
          or \emph{decode} (subsequent tokens) phase.
    \item Executes the forward pass on the GPU for that batch.
    \item Updates KV cache blocks, manages allocation and eviction, and applies
          prefix caching optimizations when possible.
    \item Returns generated tokens back to the API layer, either for streaming
          or final response.
\end{enumerate}
We will refer to this engine loop as the \emph{busy loop} since it continuously
steps through request-batches at a fine-grained timescale.

\subsection{Design of BLIS}
\label{subsec:blis-design}

\section{Latency model}
\label{sec:latency-model}

\subsection{Request types}
\label{subsubsec:request-types}

Requests can be of two types: \textit{prefill-only} or \textit{decode}. 
Prefill-only requests have their maximum output length set to $1$, and \textit{decode}
requests have this set to a number greater than $1$.

\subsection{Request life-cycle}
\label{subsubsec:request-life-cycle}

\begin{figure}[!ht]
  \centering
  \includegraphics[height=0.8\textwidth]{figs/reqlifecycle.png}
  \caption{Request Lifecycle}
  \label{fig:req-lifecycle}
\end{figure}

The life of an inference request in the system passes through multiple stages 
as  illustrated in Figure \ref{fig:req-lifecycle} and described below.

\paragraph*{1. Ingress.}
The client emits an inference request $i$ to the server. This contributes 
a network latency that depends linearly on the number of input tokens 
$\ell_i^{\text{in}}$.

\paragraph*{2. Pre-process.}
Upon arrival, the API server tokenizes the prompt, performs light
validation, and enqueues the request into the message queue. This contributes 
a latency that depends linearly on $\ell_i^{\text{in}}$.

\paragraph*{3. Scheduling.} The request waits in the queue until there is 
sufficient GPU capacity. During this waiting period, other requests get to participate
in the busy loop iterations.

\paragraph*{4. Chunked prefill.} The uncached portion of this request is 
prefilled one chunk at a time. The latency of this stage equals the sum of the 
duration of the busy loop iterations in which this request prefills.

\paragraph*{5. Decode.} The latency of this stage equals the sum of the duration
of the busy loop iterations in which this request is in its decode phase. 
The first output token is generated during the prefill phase, and subsequent 
tokens are generated in the decode iterations.

\paragraph*{6. Post-process.} Once all output tokens are produced, 
the API server detokenizes them and
packages the final response. The latency of this stage depends linearly on the 
number of output tokens $\ell_i^{\text{out}}$.

\paragraph*{7. Egress.}
The server emits response $i$ to the
client. This contributes a network latency that depends linearly 
on the number of output tokens $\ell_i^{\text{out}}$.


\section{Trace-Only Estimation of Step-Level Execution Coefficients}
\label{sec:step_beta_estimation}

This section presents a trace-only methodology for estimating
\emph{step-level} execution coefficients of a vLLM-style inference engine.
We proceed in three stages.
First, we introduce a simple but principled baseline estimator that fits
phase durations using time-integrated workload signals.
Second, we refine this estimator using an iterative, step-density--reweighted
procedure inspired by majorization--minimization (MM).
Finally, we discuss robustness, workload design for calibration, and practical
limitations.

Throughout, we emphasize trace-only operation: no step boundaries, 
per-step timings, or engine instrumentation are assumed.

\paragraph{Notation and units (summary).}
Table~\ref{tab:notation_summary} summarizes the principal symbols used in
this section and their units.

\begin{table}[ht!]
\centering
\small
\begin{tabular}{lll}
\toprule
\textbf{Symbol} & \textbf{Units} & \textbf{Meaning} \\
\midrule
$k$ & step index & busy-loop iteration (latent execution step) \\
$r$ & request index & inference request; each has exactly one prefill phase and one decode phase \\
$i$ & phase index & a specific phase (prefill or decode) of a specific request \\
$m$ & iteration index & outer iteration of the step-density reweighted estimator \\[0.5ex]

$t_{i,s},\,t_{i,e}$ & seconds & start and end times of phase $i$ \\
$\ell_i$ & seconds & observed duration of phase $i$ \\
$N_i$ & steps & trace-inferred step count for phase $i$ (held fixed during estimation) \\
$\tau_i$ & categorical (unitless) & phase type of instance $i$ (\textsf{prefill} or \textsf{decode}) \\
$T^{\mathrm{pf}}_k,\,T^{\mathrm{dec}}_k$ & tokens & prefill/decode token totals processed in step $k$ \\[0.5ex]

$C$ & tokens/step & prefill chunk size \\
$P_r$ & tokens & total (uncached) prefill tokens for request $r$ \\
$D_r$ & tokens & total decode tokens for request $r$ \\
$\rho_r$ & tokens & tokens processed in the final prefill step of request $r$, Eq.~\eqref{eq:rho_def} \\
$\mu_r$ & tokens & missing token mass of final prefill chunk, $\mu_r=C-\rho_r$ \\[0.5ex]

$a_r(t)$ & unitless & indicator that request $r$ is in prefill at time $t$ (trace-visible) \\
$p^{\mathrm{pf}}_{\mathrm{full}}(t)$ & tokens/step & naive full-chunk prefill pressure, $C\sum_r a_r(t)$ \\
$p^{\mathrm{dec}}(t)$ & tokens/step & decode pressure (tokens/step) from overlapping decode phases \\[0.5ex]

$\widehat{\lambda}_i$ & steps/sec & baseline phase-local step density proxy, $\widehat{\lambda}_i=N_i/\ell_i$ \\
$c(t)$ & tokens/sec & baseline partial-chunk correction signal (uniform/end/none) \\[0.5ex]

$A_i^{\mathrm{pf}}$ & tokens & baseline prefill exposure, Eq.~\eqref{eq:baseline_integrated_exposures_pf} \\
$A_i^{\mathrm{dec}}$ & tokens & baseline decode exposure, Eq.~\eqref{eq:baseline_integrated_exposures_dec} \\[0.5ex]

$\Delta^{(m)}(t)$ & sec/step & iteration-$m$ local step-time model, Eq.~\eqref{eq:local_step_time_m} \\
$\lambda^{(m)}(t)$ & steps/sec & iteration-$m$ step density, $\lambda^{(m)}(t)=1/\Delta^{(m)}(t)$, Eq.~\eqref{eq:step_density_m} \\[0.5ex]

$q_i^{(m)}(t)$ & 1/sec & iteration-$m$ phase time-density proportional to $\lambda^{(m)}$ over $[t_{i,s},t_{i,e})$ \\
$q_r^{(m)}(t)$ & 1/sec & iteration-$m$ request time-density proportional to $\lambda^{(m)}$ over $[t_{r,s},t_{r,e})$ \\
$\mathcal{W}_r^{(m)}$ & time subset & iteration-$m$ last-step window for request $r$, Eq.~\eqref{eq:last_step_window_m} \\[0.5ex]

$c_r^{(m)}(t)$ & tokens/sec & iteration-$m$ request-level correction signal, Eq.~\eqref{eq:cr_beta_informed_m} \\
$c^{(m)}(t)$ & tokens/sec & iteration-$m$ aggregate correction signal, $c^{(m)}(t)=\sum_r c_r^{(m)}(t)$ \\
$\tilde p^{\mathrm{pf},(m)}(t)$ & tokens/step & iteration-$m$ effective prefill pressure, Eq.~\eqref{eq:prefill_pressure_beta_informed_m} \\[0.5ex]

$\bar p_i^{\mathrm{pf},(m)}$ & tokens/step & iteration-$m$ step-averaged prefill pressure, Eq.~\eqref{eq:step_averaged_pressures_m} \\
$\bar p_i^{\mathrm{dec},(m)}$ & tokens/step & iteration-$m$ step-averaged decode pressure, Eq.~\eqref{eq:step_averaged_pressures_m} \\[0.5ex]

$\beta_0$ & sec/step & fixed per-step overhead coefficient \\
$\beta_1,\beta_2$ & sec/token & per-token execution costs (prefill / decode) \\
$\beta^{(m)}$ & mixed & coefficient vector at outer iteration $m$ \\
\bottomrule
\end{tabular}
\caption{Notation and units used in trace-only step-level coefficient estimation.}
\label{tab:notation_summary}
\end{table}



% ============================================================
\subsection{Problem Setup and Step-Level Model}
\label{sec:problem_setup}

We consider a vLLM-style inference engine that advances execution through
a single logical busy-loop.
Each busy-loop iteration (a \emph{step}) executes one forward-pass cycle
that may process prefill tokens, decode tokens, or both, for a set of
concurrently active requests.
Steps are assumed to execute sequentially.

Let $k$ index busy-loop steps.
We model the duration of step $k$ as
\begin{equation}
\Delta t_k
=
\beta_0
+
\beta_1\,T^{\mathrm{pf}}_k
+
\beta_2\,T^{\mathrm{dec}}_k,
\label{eq:step_model}
\end{equation}
where:
\begin{itemize}
  \item $\beta_0 \ge 0$ is a fixed per-step overhead (seconds/step),
  \item $\beta_1,\beta_2 \ge 0$ are per-token costs (seconds/token),
  \item $T^{\mathrm{pf}}_k$ and $T^{\mathrm{dec}}_k$ are the total numbers of
        prefill and decode tokens processed in step $k$ across all requests.
\end{itemize}

This affine step-time model is the fundamental abstraction used by our
simulator. Accurate simulation therefore requires identifying the
coefficient vector $\beta=(\beta_0,\beta_1,\beta_2)$.

\paragraph{What traces provide.}
Production inference traces do not expose step boundaries or per-step execution
timings. Instead, they provide a sequence of \emph{phase instances}, indexed by
$i$, each corresponding to either the prefill phase or the decode phase of a
single request.

For each phase instance $i$, the trace provides:
\begin{itemize}
  \item the request id $r_i$
  \item the phase type $\tau_i$ (\textit{prefill} or \textit{decode}),
  \item a start time $t_{i,s}$ and end time $t_{i,e}$,
  \item an observed phase duration $\ell_i = t_{i,e} - t_{i,s}$,
  \item aggregate token counts associated with the phase, including the number
        of \emph{prefill tokens} or \emph{decode tokens}.
\end{itemize}

For a phase instance $i$ associated with request $r_i$, we denote by $P_{r_i}$
the total number of prefill tokens for that request and by $D_{r_i}$ the total
number of decode tokens for its decode phase. Since each phase 
instance uniquely identifies its request, we write $P_r$ 
(prefill tokens of request $r$) and $D_r$ (decode tokens of request $r$) 
without ambiguity. From this 
information, we derive a step count $N_i \in \mathbb{N}$.
For a decode phase, execution proceeds one token per step, and hence
$N_i$ equals the number of decode tokens.
For a prefill phase, execution is chunked: under a prefill chunk size $C$,
a standard trace-inferred choice is
$N_i = \lceil P_r / C \rceil$.\footnote{
Prefix-cached tokens do not contribute to execution steps and are therefore
excluded from $P_r$. This treatment is consistent with the execution semantics
of vLLM-style inference engines.
}

% ============================================================
\subsection{Trace-Derived Token Pressures}
\label{sec:pressures}

\paragraph{Intuition.}
Inference executes as a sequence of discrete steps, but production traces expose
only coarse, phase-level summaries.
For each prefill or decode phase of a request, the trace records when the phase
starts and ends and how many tokens are processed, but not how those tokens are
distributed across individual steps.
The goal is not to reconstruct the hidden step schedule, but to recover
step-level execution coefficients that describe how different types of work
contribute to elapsed time when execution is aggregated over many steps.

The baseline estimator bridges this gap by compressing step-level behavior into
phase-level quantities that are consistent with the trace.
It treats each phase as a superposition of many unobserved steps and represents
the effective step-level workload of each type using token pressures.
These pressures describe, as a function of time, how much prefill and decode
work is being processed per execution step across all concurrently active
requests, without requiring access to step boundaries or per-step timings.
This intuition is illustrated in Figure \ref{fig:overlap_phases}.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=\linewidth]{figs/overlap_phases.pdf}
  \caption{\textbf{Phase overlap and inferred workload over wall-clock time.}
Each horizontal bar represents a trace-visible prefill or decode phase 
of a request, while dashed vertical lines indicate wall-clock time 
instants at which an execution step boundary may occur. 
The dashed line at time $t = 8.0$ falls at the end of a step during 
which all requests are concurrently active, so both prefill and decode 
pressures are at their maximum for that step. Although traces 
expose only phase start and end times—not step boundaries—the 
degree of phase overlap determines how much work is executed per 
step when time is aggregated. The baseline estimator leverages this 
intuition by converting overlap in wall-clock time into integrated 
token exposures that are consistent with the underlying step-level 
execution model.}
\label{fig:overlap_phases}
\end{figure}

Given these pressures and the trace-inferred number of steps per phase, the
baseline constructs simple phase-level summaries—used as regression
features—that quantify how much prefill and decode work each phase “exposes” to
the underlying step model.
Each phase duration can then be written as a linear function of these summaries
and the unknown step-level coefficients, turning coefficient recovery into an
ordinary regression problem.
Solving this regression produces a single, globally consistent set of execution
coefficients from trace-only data and provides the starting point for the
step-density–reweighted refinements introduced next.

\paragraph{Trace-only attribution (important).}
All densities introduced below (e.g., $q_i$ and $q_r$) are \emph{attribution models} that allocate
\emph{latent step exposure} over wall-clock time using only trace-visible phase boundaries.
They are not claims about true step boundaries or per-step timings; the estimator uses them only through
time integrals on the trace-induced grid.


\paragraph{Terminology: instantaneous quantities and exposures.}
We distinguish between \emph{instantaneous} quantities, which are defined as
functions of wall-clock time $t$ on the trace-induced grid, and
\emph{exposures}, which are obtained by integrating instantaneous quantities
over a phase window and serve as regression features.
Instantaneous \emph{pressures} quantify work performed per execution step
(tokens/step). The \emph{step density} $\lambda^{(m)}(t)$ (steps/sec) quantifies
how frequently busy-loop steps occur in wall-clock time under the iteration-$m$
local step-time model (Section~\ref{sec:local_step_density}).
Exposures are phase- or request-level scalars with units of tokens, obtained
either by converting time-integrated pressures using a step density proxy
(baseline) or by aggregating step-averaged pressures over a fixed step count
(iterative estimator).
Throughout, all quantities are defined and combined in a manner consistent
with their physical units.


\paragraph{Instantaneous token pressures.}
We define two trace-derived, piecewise-constant \emph{instantaneous pressures}
(tokens/step) on the trace-induced time grid.
The decode pressure $p^{\mathrm{dec}}(t)$ is the total decode-token work that would
be attributed to a representative busy-loop step executed at wall-clock time $t$,
given the set of requests whose trace-visible decode phases overlap $t$.
For prefill, we use the \emph{naive full-chunk} pressure
\[
p^{\mathrm{pf}}_{\mathrm{full}}(t)=C\sum_r a_r(t),
\]
which treats each active prefill request as contributing $C$ tokens per step.
Any accounting for the final partial prefill chunk is represented separately,
either through a correction \emph{in exposure space} in the baseline
(via $c(t)$), or through an \emph{iteration-indexed effective pressure}
$\tilde p^{\mathrm{pf},(m)}(t)$ constructed during iteration $m$ from the
\emph{previous} iterate $\beta^{(m-1)}$ via the induced step density
(Section~\ref{sec:beta_informed_prefill_correction}).

\subsubsection{Prefill Chunking and Partial-Chunk Correction Signals}
\label{sec:prefill_correction}

Decode contributes exactly one token per active request per step.
Prefill, however, proceeds in chunks of size $C$ tokens per step, except for
the final prefill step of a request, which may process fewer than $C$ tokens.
Naively treating all prefill steps as full chunks introduces systematic bias.

For a request $r$ with $P_r$ prefill tokens and $N_r$ prefill steps, the final
prefill step processes
\begin{equation}
\rho_r = P_r - C\,(N_r-1),
\qquad \rho_r \in (0,C].
\label{eq:rho_def}
\end{equation}
The missing token mass relative to a full chunk is
\[
\mu_r = C - \rho_r \in [0,C).
\]

\paragraph{Naive full-chunk pressure.}
Let $a_r(t)\in\{0,1\}$ indicate whether request $r$ is in prefill at time $t$,
as determined from trace-visible phase boundaries.
The naive full-chunk prefill pressure is
\begin{equation}
p^{\mathrm{pf}}_{\mathrm{full}}(t)
=
C \sum_r a_r(t).
\end{equation}

\paragraph{No partial-chunk correction (baseline ablation).}
The simplest baseline ignores partial chunks entirely and uses
$p^{\mathrm{pf}}_{\mathrm{full}}(t)$ as the effective prefill pressure.
Under this choice, the estimator treats every prefill step as a full chunk,
and the missing mass $\mu_r$ is not explicitly accounted for.

This option serves as a natural ablation: it is fully trace-only, requires no
additional assumptions about where the final prefill step occurs in time, and
provides a lower bound on estimator fidelity when chunking effects are mild.

\paragraph{Uniform redistribution correction (implemented in integrated form).}
A more refined baseline accounts for the missing mass $\mu_r$ by redistributing
it uniformly over the prefill interval of request $r$, but it does so in a way
that is dimensionally consistent with the baseline feature construction.

Let $[t_{r,s}, t_{r,e})$ denote the trace-visible prefill interval of request $r$.
Define a \emph{correction signal} (tokens/second)
\begin{equation}
c_r^{\mathrm{uni}}(t)
=
\frac{\mu_r}{t_{r,e}-t_{r,s}}\,
\mathbf{1}\{t\in[t_{r,s},t_{r,e})\},
\qquad
\text{units: tokens/second},
\label{eq:uniform_corr_signal}
\end{equation}
and let the aggregate correction signal be
\begin{equation}
c^{\mathrm{uni}}(t)=\sum_r c_r^{\mathrm{uni}}(t).
\label{eq:uniform_corr_signal_sum}
\end{equation}
By construction, $\int_{t_{r,s}}^{t_{r,e}} c_r^{\mathrm{uni}}(t)\,dt=\mu_r$,
so total missing token mass is preserved exactly when integrating over time.

\paragraph{End-localized partial-chunk correction (implemented on the trace grid).}
Uniform redistribution implicitly assumes the final prefill step could occur
anywhere within the prefill interval. An alternative trace-only assumption is
that the final prefill step completes near the trace-visible end time $t_{r,e}$.

Let $\{[g_j,g_{j+1})\}$ denote the trace-induced time grid formed by all phase
boundaries, and let $j(r)$ be the index of the unique grid segment containing
$t_{r,e}$ (under the half-open convention, with boundary clamping at the end).
Define the per-request end-localized correction signal (tokens/second)
\begin{equation}
c_r^{\mathrm{end}}(t)
=
\frac{\mu_r}{g_{j(r)+1}-g_{j(r)}}\,
\mathbf{1}\{t\in[g_{j(r)},g_{j(r)+1})\},
\qquad
\text{units: tokens/second},
\label{eq:end_corr_signal}
\end{equation}
and the aggregate correction signal
\begin{equation}
c^{\mathrm{end}}(t)=\sum_r c_r^{\mathrm{end}}(t).
\label{eq:end_corr_signal_sum}
\end{equation}
Again, $\int c_r^{\mathrm{end}}(t)\,dt=\mu_r$ by construction.

\paragraph{Remark (why we use correction signals in the baseline).}
In the baseline, instantaneous token pressures are represented in units of
tokens/step and only enter the estimator through time integrals that are
subsequently converted into token totals via the phase-local step-density proxy
$\widehat\lambda_i$ (Eq.~\eqref{eq:baseline_lambda_hat}). The partial-chunk 
correction is therefore represented as a \emph{time-based}
correction signal $c(t)$ (tokens/sec) so that its time integral is 
directly in tokens and can be subtracted in exposure space without introducing any
step-boundary assumptions.


\paragraph{Implementation note (no pointwise reconstruction required).}
Although we define token pressures and correction weights as functions of
wall-clock time, the baseline estimator never requires evaluating them
instantaneously.
All quantities enter the estimator only through time integrals over phase
intervals, such as
$\int_{t_{i,s}}^{t_{i,e}} p^{\mathrm{pf}}_{\mathrm{full}}(t)\,dt$
and
$\int_{t_{i,s}}^{t_{i,e}} c(t)\,dt$.
These integrals can be computed exactly from trace-visible phase boundaries by
sweeping over a piecewise-constant time grid and applying per-request correction
masses whose placement within a grid interval does not affect correctness.



\subsection{Baseline: Time-Integrated NNLS Estimation}
\label{sec:baseline}

We first present a simple, trace-only baseline estimator that fits phase
durations using time-integrated pressures.
Because pressures are expressed in \emph{tokens per step}, a time integral
$\int p(t)\,dt$ has units $(\text{tokens}/\text{step})\cdot \text{seconds}$.
To form regression features with units of \emph{tokens}---so that
$\beta_1,\beta_2$ retain the interpretation ``seconds per token''---we
convert time integrals into approximate \emph{step integrals} using a
phase-local constant step density.

\begin{algorithm}[t]
\caption{Baseline trace-only estimation}
\label{alg:baseline_intuition}
\begin{algorithmic}[1]
\Require Phase traces $\{(r_i,\tau_i,t_{i,s},t_{i,e},P_{r_i},D_{r_i})\}$, chunk size $C$
\Ensure Estimated execution coefficients $\hat\beta$

\For{each phase instance $i$}
  \State infer step count $N_i$ from trace-visible tokens
  \Comment{definition of $N_i$; §\ref{sec:problem_setup}}
\EndFor

\State construct global time grid from all phase boundaries
\Comment{trace-induced grid for piecewise-constant instantaneous pressures; §\ref{sec:pressures}}

\For{each time interval in the grid}
  \State measure concurrent prefill and decode activity
  \Comment{overlap $\Rightarrow$ \emph{instantaneous} pressures $p^{\mathrm{pf}}_{\mathrm{full}}(t),p^{\mathrm{dec}}(t)$ on the grid; §\ref{sec:pressures}}
\EndFor

\For{each prefill request $r$}
  \State apply trace-only partial-chunk correction (optional)
  \Comment{missing mass $\mu_r$ and trace-only correction signal $c(t)$ (tokens/sec); §\ref{sec:prefill_correction}, Eq.~\eqref{eq:rho_def}, Eq.~\eqref{eq:uniform_corr_signal_sum}/Eq.~\eqref{eq:end_corr_signal_sum}}
\EndFor

\For{each phase instance $i$}
  \State compute phase-level prefill and decode exposures
  \Comment{baseline exposures $A_i^{\mathrm{pf}},A_i^{\mathrm{dec}}$ from instantaneous pressures and phase-local step density; §\ref{sec:baseline}, Eq.~\eqref{eq:baseline_lambda_hat}, Eq.~\eqref{eq:baseline_integrated_exposures_pf}, Eq.~\eqref{eq:baseline_integrated_exposures_dec}}
\EndFor

\State fit execution coefficients via NNLS
\Comment{baseline predictor and NNLS objective; §\ref{sec:baseline}, Eq.~\eqref{eq:baseline_predictor}, Eq.~\eqref{eq:baseline_nnls}}

\State \Return $\hat\beta$
\end{algorithmic}
\end{algorithm}



\paragraph{Phase-local step density.}
For phase instance $i$, let $\ell_i=t_{i,e}-t_{i,s}$ be its observed duration and
let $N_i\in\mathbb{N}$ be the trace-inferred step count.
We define the (phase-local) \emph{step density proxy} (steps/sec)
\begin{equation}
\widehat\lambda_i \;=\; \frac{N_i}{\ell_i}.
\label{eq:baseline_lambda_hat}
\end{equation}

This assumes steps are spread approximately uniformly over the phase window,
and is used \emph{only} to convert time integrals into step-aggregated token
totals in the baseline.\footnote{Implementation detail: to avoid extreme $\widehat{\lambda}_i$ when $\ell_i$
is very small, we may compute $\widehat{\lambda}_i = N_i/\max\{\ell_i, \ell_{\min}\}$ for a
small $\ell_{\min}\ge 0$. The default $\ell_{\min}=0$ recovers Eq.~\eqref{eq:baseline_lambda_hat}.} 
We emphasize that $\widehat{\lambda}_i$ is a surrogate step-density used only
to convert time-integrated pressures into step-aggregated token exposures.


\paragraph{Phase-level exposures (baseline regression features).}
Let $p^{\mathrm{pf}}_{\mathrm{full}}(t)=C\sum_r a_r(t)$ denote the naive full-chunk
prefill pressure (tokens/step), and let $p^{\mathrm{dec}}(t)$ denote the decode
pressure (tokens/step). Let $c(t)$ denote a chosen partial-chunk correction signal
(tokens/second), e.g., $c^{\mathrm{uni}}(t)$ from Eq.~\eqref{eq:uniform_corr_signal_sum},
$c^{\mathrm{end}}(t)$ from Eq.~\eqref{eq:end_corr_signal_sum}, or $c(t)\equiv 0$ (ablation).

We define baseline phase-level exposures (tokens) as
\begin{align}
A^{\mathrm{pf}}_i
&=
\left[ \widehat\lambda_i
\int_{t_{i,s}}^{t_{i,e}} p^{\mathrm{pf}}_{\mathrm{full}}(t)\,dt \right]
\;-\;
\int_{t_{i,s}}^{t_{i,e}} c(t)\,dt,
\label{eq:baseline_integrated_exposures_pf}
\\
A^{\mathrm{dec}}_i
&=
\widehat\lambda_i
\int_{t_{i,s}}^{t_{i,e}} p^{\mathrm{dec}}(t)\,dt.
\label{eq:baseline_integrated_exposures_dec}
\end{align}
The integrals of $p^{\mathrm{pf}}_{\mathrm{full}}(t)$ and $p^{\mathrm{dec}}(t)$ have
units $(\text{tokens}/\text{step})\cdot \text{seconds}$, which are converted into
tokens by $\widehat\lambda_i$ (steps/second), while $\int c(t)\,dt$ is already in tokens.
Accordingly, the correction term is \emph{not} multiplied by $\widehat\lambda_i$.


\paragraph{Baseline predictor.}
The baseline predicts phase duration as
\begin{equation}
\widehat \ell_i(\beta)
=
\beta_0\,N_i
+
\beta_1\,A^{\mathrm{pf}}_i
+
\beta_2\,A^{\mathrm{dec}}_i,
\label{eq:baseline_predictor}
\end{equation}
where $\beta_0$ has units seconds/step and $\beta_1,\beta_2$ have units
seconds/token, consistent with the step-level model in
Eq.~\eqref{eq:step_model}.

\paragraph{NNLS estimation.}
Given a set of phase instances $\mathcal{I}$, we estimate $\beta$ via
non-negative least squares:
\begin{equation}
\min_{\beta \in \mathbb{R}^3_{\ge 0}}
\sum_{i\in\mathcal{I}}
\left(
\widehat \ell_i(\beta) - \ell_i
\right)^2 .
\label{eq:baseline_nnls}
\end{equation}

We emphasize that the simulator generates its own step-level token counts 
and KV-cache dynamics; the only information learned from traces is the 
estimated $\beta$, which parameterizes the simulator but does not 
encode any trace-specific timing information.

\paragraph{Why reweighting helps.}
This estimator remains wall-clock--weighted because the baseline exposures 
$A_i^{\mathrm{pf}}$ and $A_i^{\mathrm{dec}}$ are formed
by integrating instantaneous pressures over wall-clock time 
within each phase,
using the phase-average step density proxy $\widehat\lambda_i$ rather than a
time-varying step density.
As a result, phases that coincide with long-step periods can exert
disproportionate influence, motivating the step-density--reweighted estimator
in Section~\ref{sec:step_density}.


% ============================================================
\subsection{Step-Density Reweighted Estimation}
\label{sec:step_density}

We now introduce an iterative estimator that shifts fitting from wall-clock
weighting toward step-level weighting. The step-density reweighted estimator can be combined with any of the
trace-only prefill correction schemes from
Section~\ref{sec:prefill_correction}.
In addition, it enables a $\beta$-informed refinement that localizes the
prefill partial chunk using the inferred step-density model.

\begin{algorithm}[t]
\caption{Iterative step-density reweighted estimation}
\label{alg:iterative_intuition}
\begin{algorithmic}[1]
\Require Phase traces, chunk size $C$, initial coefficients $\beta^{(0)}$, tolerance $\epsilon$
\Ensure Refined execution coefficients $\beta^\star$

\State initialize $\beta^{(0)} \gets$ baseline NNLS estimate
\Comment{§\ref{sec:baseline}, Eq.~\eqref{eq:baseline_nnls}}

\For{$m=1,2,\dots$}
  \State compute $\Delta^{(m)}(t)$ and $\lambda^{(m)}(t)$ from $\beta^{(m-1)}$
  \Comment{\emph{instantaneous} step-time and step-density on the trace grid; §\ref{sec:local_step_density}, Eq.~\eqref{eq:local_step_time_m}, Eq.~\eqref{eq:step_density_m}}

  \State assign step exposure across time intervals
  \Comment{compute phase/request attribution densities $q_i^{(m)},q_r^{(m)}$ from $\lambda^{(m)}$ on the trace grid; §\ref{sec:local_step_density}}

  \For{each prefill request $r$}
    \State localize final partial chunk using step density
    \Comment{$\beta$-informed placement of missing prefill tokens within the prefill window via request-level attribution and last-step window; §\ref{sec:beta_informed_prefill_correction}, Eq.~\eqref{eq:qr_def_m}--Eq.~\eqref{eq:last_step_window_m}}
  \EndFor

  \For{each phase instance $i$}
    \State compute step-averaged prefill and decode pressures
    \Comment{step-averaged (i.e., step-weighted) pressures from \emph{instantaneous} pressures and phase-level attribution; §\ref{sec:step_averaged_pressures}, Eq.~\eqref{eq:step_averaged_pressures_m}}
  \EndFor

  \State refit $\beta^{(m)}$ using frozen iteration-$m$ features
  \Comment{NNLS on step-weighted exposures $N_i\bar p_i^{\mathrm{pf}}$ and $N_i\bar p_i^{\mathrm{dec}}$ (tokens), holding attribution fixed; §\ref{sec:step_density}, Eq.~\eqref{eq:sr_nnls_m}}

  \If{$\|\beta^{(m)}-\beta^{(m-1)}\|_2 < \epsilon$} 
    \State \textbf{break}
  \EndIf
\EndFor

\State set $\beta^\star \gets \beta^{(m)}$
\State \Return $\beta^\star$

\end{algorithmic}
\end{algorithm}

\paragraph{Iteration semantics.}
At the start of iteration $m$, the coefficient vector $\beta^{(m-1)}$ and all
features computed in iteration $m-1$ are available.
All quantities carrying superscript $m$ are computed \emph{during} iteration
$m$ using $\beta^{(m-1)}$ and previously computed features, and are held fixed
while solving for $\beta^{(m)}$.

\paragraph{Update map interpretation.}
The iterative estimator can be viewed as the repeated application of a
deterministic update map.
Specifically, given the previous iterate and lagged effective prefill pressure,
the algorithm applies
\[
(\beta^{(m-1)},\tilde p^{\mathrm{pf},(m-1)})
\;\xmapsto{\;\mathcal{T}\;}
(\beta^{(m)},\tilde p^{\mathrm{pf},(m)}),
\]
where the update map $\mathcal{T}$ is realized through the following
trace-only construction chain:
\begin{align*}
& \beta^{(m-1)}
\;\mapsto\;
\Delta^{(m)}(t)
\;\mapsto\;
\lambda^{(m)}(t)
\;\mapsto\; 
\bigl(q_i^{(m)}(t), q_r^{(m)}(t), F_r^{(m)}(t), \mathcal{W}_r^{(m)}\bigr)
\;\mapsto\; \\
& \bigl(c_r^{(m)}(t), c^{(m)}(t), \tilde p^{\mathrm{pf},(m)}(t)\bigr)
\;\mapsto\;
\bigl(\bar p_i^{\mathrm{pf},(m)}, \bar p_i^{\mathrm{dec},(m)}\bigr)
\;\mapsto\;
\beta^{(m)}.
\end{align*}
All quantities with superscript $(m)$ are computed during iteration $m$ from
$\beta^{(m-1)}$ and trace-derived signals, then held fixed while solving the
NNLS subproblem for $\beta^{(m)}$.


\subsubsection{Local Step-Time Model and Step Density (Iteration-Indexed)}
\label{sec:local_step_density}

The iterative estimator proceeds in outer iterations indexed by $m$.
At the start of iteration $m$, we treat the current coefficient vector $\beta^{(m-1)}$
as fixed and construct a trace-only step-density proxy on the trace-induced
time grid. All attribution quantities computed from this proxy (e.g.,
$q_i^{(m)}$, $q_r^{(m)}$, $\mathcal{W}_r^{(m)}$, $c^{(m)}$, and
$\bar p_i^{(\cdot),(m)}$) are then held fixed while solving for $\beta^{(m)}$
via an NNLS subproblem.

Given instantaneous pressures and the effective prefill pressure $\tilde p^{\mathrm{pf},(m-1)}(t)$
computed in the previous iteration (and held fixed), define the iteration-$m$
local step-time model as
\begin{equation}
\Delta^{(m)}(t)
=
\beta^{(m-1)}_0
+
\beta^{(m-1)}_1\,\tilde p^{\mathrm{pf},(m-1)}(t)
+
\beta^{(m-1)}_2\,p^{\mathrm{dec}}(t),
\qquad \Delta^{(m)}(t) > 0.
\label{eq:local_step_time_m}
\end{equation}

The induced \emph{step density} (steps/sec) is
\begin{equation}
\lambda^{(m)}(t) = \frac{1}{\Delta^{(m)}(t)}.
\label{eq:step_density_m}
\end{equation}

% ============================================================
\subsubsection{$\beta$-Informed Partial-Chunk Correction (Iteration-Indexed)}
\label{sec:beta_informed_prefill_correction}

Using $\lambda^{(m)}(t)$ from Eq.~\eqref{eq:step_density_m}, define the
normalized step-time attribution density over request $r$'s prefill window:
\begin{equation}
q_r^{(m)}(t)
=
\frac{\lambda^{(m)}(t)}{\int_{t_{r,s}}^{t_{r,e}} \lambda^{(m)}(u)\,du},
\qquad
\int_{t_{r,s}}^{t_{r,e}} q_r^{(m)}(t)\,dt = 1.
\label{eq:qr_def_m}
\end{equation}

Define the corresponding CDF
\begin{equation}
F_r^{(m)}(t)=\int_{t_{r,s}}^{t} q_r^{(m)}(u)\,du,
\label{eq:Fr_def_m}
\end{equation}
and the last-step window as
\begin{equation}
\mathcal{W}_r^{(m)}
=
\Bigl\{
t\in[t_{r,s},t_{r,e})
:
F_r^{(m)}(t)\ge 1-\tfrac{1}{N_r}
\Bigr\}.
\label{eq:last_step_window_m}
\end{equation}

The per-request correction signal (tokens/sec) is
\begin{equation}
c_r^{(m)}(t)
=
\mu_r\,
\frac{q_r^{(m)}(t)\,\mathbf{1}\{t\in\mathcal{W}_r^{(m)}\}}
{\int_{\mathcal{W}_r^{(m)}} q_r^{(m)}(u)\,du},
\label{eq:cr_beta_informed_m}
\end{equation}
with aggregate correction $c^{(m)}(t)=\sum_r c_r^{(m)}(t)$.

The effective prefill pressure is
\begin{equation}
\tilde p^{\mathrm{pf},(m)}(t)
=
p^{\mathrm{pf}}_{\mathrm{full}}(t)
-
\frac{c^{(m)}(t)}{\lambda^{(m)}(t)}.
\label{eq:prefill_pressure_beta_informed_m}
\end{equation}

Since $c^{(m)}(t)/\lambda^{(m)}(t)$ has units
$(\text{tokens}/\text{second})/(\text{steps}/\text{second})=\text{tokens}/\text{step}$,
$\tilde p^{\mathrm{pf},(m)}(t)$ remains a tokens/step pressure.

We note that all instantaneous quantities above enter only through integrals over the trace-induced grid
$\{[g_j,g_{j+1})\}$, on which pressures (and hence $\lambda^{(m)}$) are piecewise-constant.

\subsubsection{Normalized Step Density within a Phase (Iteration-Indexed)}

For phase instance $i$, define the iteration-$(m)$ phase mass
\[
\Lambda_i^{(m)}
=
\int_{t_{i,s}}^{t_{i,e}} \lambda^{(m)}(t)\,dt,
\qquad
q_i^{(m)}(t)
=
\frac{\lambda^{(m)}(t)}{\Lambda_i^{(m)}}.
\]
By construction, $\int_{t_{i,s}}^{t_{i,e}} q_i^{(m)}(t)\,dt = 1$.
All phase-level attribution quantities are computed during iteration $m$
and held fixed while solving for $\beta^{(m)}$.

\subsubsection{Step-Weighted Exposures}
\label{sec:step_averaged_pressures}

\begin{equation}
\bar p^{\mathrm{pf},(m)}_i
=
\int_{t_{i,s}}^{t_{i,e}} \tilde p^{\mathrm{pf},(m)}(t)\, q_i^{(m)}(t)\,dt,
\qquad
\bar p^{\mathrm{dec},(m)}_i
=
\int_{t_{i,s}}^{t_{i,e}} p^{\mathrm{dec}}(t)\, q_i^{(m)}(t)\,dt.
\label{eq:step_averaged_pressures_m}
\end{equation}

\paragraph{From step-averaged pressure to exposure.}
The quantities $\bar p_i^{\mathrm{pf},(m)}$ and $\bar p_i^{\mathrm{dec},(m)}$
are \emph{step-averaged pressures} (tokens/step), obtained by averaging
instantaneous pressures with respect to the iteration-$m$ step-time attribution
$q_i^{(m)}(t)$.
The corresponding \emph{step-weighted exposures} used in the NNLS subproblem are
$N_i\bar p_i^{\mathrm{pf},(m)}$ and $N_i\bar p_i^{\mathrm{dec},(m)}$ (tokens).

In the special case where $\lambda^{(m)}(t)$ is approximately constant over
$[t_{i,s},t_{i,e})$, the step-time attribution is approximately uniform and the
resulting step-weighted exposures reduce to the baseline construction (up to
numerical integration error).

\paragraph{Units and interpretation.}
$\bar p_i^{\mathrm{pf},(m)}$ and $\bar p_i^{\mathrm{dec},(m)}$ have units of tokens/step,
so $N_i\bar p_i^{\mathrm{pf},(m)}$ and $N_i\bar p_i^{\mathrm{dec},(m)}$ are step-aggregated
token exposures (tokens) for phase $i$ under the iteration-$m$ attribution induced by
$\lambda^{(m)}(t)$.
If $\lambda^{(m)}(t)$ is approximately constant over $[t_{i,s},t_{i,e})$, then
$q_i^{(m)}(t)$ is approximately uniform and these step-weighted exposures reduce to the
baseline construction (up to numerical integration error and any chosen prefill correction).


\subsubsection{MM-Style Iterative NNLS}

\begin{equation}
\beta^{(m)}
=
\arg\min_{\beta\ge 0}
\sum_{i\in\mathcal{I}}
\left(
N_i\beta_0
+
\bigl[N_i\bar p^{\mathrm{pf},(m)}_i\bigr]\beta_1
+
\bigl[N_i\bar p^{\mathrm{dec},(m)}_i\bigr]\beta_2
-
\ell_i
\right)^2 .
\label{eq:sr_nnls_m}
\end{equation}

where $\bar p^{\mathrm{pf},(m)}_i$ and $\bar p^{\mathrm{dec},(m)}_i$ are computed 
via Eqs.~\eqref{eq:local_step_time_m}--\eqref{eq:prefill_pressure_beta_informed_m} and \eqref{eq:step_averaged_pressures_m}
and are held fixed while optimizing over $\beta$.

Optional damping
$\beta^{(m)} \leftarrow (1-\eta)\beta^{(m-1)} + \eta\beta^{(m)}$
improves numerical stability. We iterate until 
$\|\beta^{(m)} - \beta^{(m-1)}\|_2 < \epsilon$ or a maximum
iteration count is reached. 

\paragraph{Robust loss and fixed-point view.}
To mitigate outliers, we optionally replace the squared loss in
Eq.~\eqref{eq:sr_nnls_m} with a Huber loss $\rho_\delta$ applied to each residual.

% ============================================================
\subsection{Limitations and Scope}
\label{sec:limitations}

This approach assumes a single sequential busy-loop and accurate knowledge of
engine scheduling rules.
Partial-chunk corrections remain approximate due to unobserved step
boundaries.
Identifiability requires sufficient variation in token pressures.
Despite these limitations, the method provides a practical and non-intrusive
path to step-level calibration using only production traces.

\paragraph{Simulator linkage.}
The estimated coefficients $\beta^\star$ are used directly in the step-time
model (Eq.~\eqref{eq:step_model}) of the discrete-event simulator.


\end{document}
