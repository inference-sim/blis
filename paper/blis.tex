\documentclass[11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{times}
\usepackage[title, titletoc]{appendix}
\usepackage{graphicx}
\usepackage{amsmath, amssymb}
\usepackage{mathtools} % for \xmapsto
\usepackage{bm}
\usepackage[most]{tcolorbox}
\usepackage{tabularx}
\usepackage{pifont} % optional for symbols
% --- Flowcharts (TikZ) ---
\usepackage{tikz}
\usetikzlibrary{arrows.meta,positioning,shapes.geometric,fit,calc}
\usepackage{xcolor}
% Algorithms (compact)
\usepackage{algorithm}
\usepackage{algpseudocode}

% Optional: slightly tighter algorithm typography
\algrenewcommand\algorithmicrequire{\textbf{Input:}}
\algrenewcommand\algorithmicensure{\textbf{Output:}}
\algrenewcommand\algorithmiccomment[1]{\hfill{\footnotesize$\triangleright$~#1}}

\usepackage{hyperref}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{booktabs,longtable,array,makecell}
\setlength{\LTleft}{0pt}   % no left indent for longtable
\setlength{\LTright}{0pt}  % no right indent for longtable
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}p{#1}}


% --- Theorems, lemmas, claims, proofs ---
\usepackage{amsthm}

\theoremstyle{plain} % bold headings, italic body
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{claim}[theorem]{Claim}

\theoremstyle{definition} % bold heading, upright body
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark} % italic heading, upright body
\newtheorem{remark}[theorem]{Remark}
\newtheorem{note}[theorem]{Note}

% Optional: unnumbered versions
\newtheorem*{theorem*}{Theorem}
\newtheorem*{lemma*}{Lemma}
\newtheorem*{claim*}{Claim}

% Proof environment is provided by amsthm:
% \begin{proof} ... \end{proof}
% Customize QED symbol if you like:
\renewcommand{\qedsymbol}{$\blacksquare$}

\newcommand{\code}[1]{\texttt{#1}}

\definecolor{prefillcolor}{RGB}{173,216,230}  % Light blue
\definecolor{decodecolor}{RGB}{144,238,144}   % Light green
\definecolor{gridcolor}{RGB}{200,200,200}     % Light gray
\definecolor{highlightcolor}{RGB}{255,182,193} % Light pink for integration region


% (Optional) If you want equations numbered by section (since you already use amsmath):
\numberwithin{equation}{section}



\title{Factorizing LLM Inference Performance 
\\[1ex] \large Transferable Execution, Cache, and Workload Models for Fast Latency Prediction}


\author{AI Platform Optimization Team}
\date{\today}

\begin{document}

\maketitle

\section{Front matter -- placeholder}
\label{sec:front}

% ============================================================
\subsection{vLLM Execution Model (Serving Semantics)}
\label{subsec:vllm-overview}

We abstract vLLM as a representative example of a modern GPU-based LLM inference
engine.
Our objective is not to document vLLM’s implementation, but to extract the
minimal execution semantics required to model request latency and throughput
from production traces.
The resulting abstraction applies to a broad class of vLLM-style inference
engines that employ dynamic batching, chunked prefill, and iterative decoding.

At a high level, the system consists of a front-end API layer and a back-end
execution engine, connected by an asynchronous queue.
Only the execution engine participates directly in GPU scheduling and is
modeled at step-level granularity.

% ------------------------------------------------------------
\subsubsection{API layer (abstracted)}

The API layer handles client-facing responsibilities, including request ingress,
tokenization, response streaming, and final detokenization.
It operates asynchronously with respect to GPU execution and does not influence
the formation or scheduling of GPU batches.

In our model, the API layer contributes latency that depends linearly on the
number of input and output tokens.
These costs are treated as request-local and are modeled separately from the
engine execution loop.

% ------------------------------------------------------------
\subsubsection{Engine core and busy-loop semantics}

The engine core executes a centralized inference loop driven by a scheduler.
We model this loop as a sequence of discrete \emph{busy-loop iterations}, or
\emph{steps}, each corresponding to a single GPU forward pass over a dynamically
constructed batch of active requests.

We assume the following execution invariants, consistent with vLLM-style
engines:

\begin{enumerate}
  \item Each busy-loop iteration executes exactly one forward pass on the GPU.
  \item A request participates in at most one phase (prefill or decode) per
        iteration.
  \item During decode, a request advances by at most one output token per
        iteration.
  \item During prefill, a request advances by a fixed chunk size of tokens per
        iteration, except possibly for the final prefill iteration.
  \item The duration of each iteration depends on the total number of prefill
        and decode tokens processed in that iteration.
\end{enumerate}

These invariants induce a natural \emph{step-level execution model}, which
serves as the foundation for the latency model and coefficient estimation
procedures developed in the remainder of the paper.

We now formalize the latency model used by BLIS.
The model decomposes request latency into a sequence of stages corresponding to
request ingress, engine execution, and response egress.
Only stages that interact with the engine busy loop are modeled at step-level
granularity; all other stages are treated as linear token-dependent delays.

% ------------------------------------------------------------
\subsection{Request Types}
\label{subsubsec:request-types}

We distinguish between two classes of inference requests based on their maximum
output length:

\begin{itemize}
  \item \emph{Prefill-only requests}, which generate exactly one output token.
  \item \emph{Decode requests}, which generate one or more output tokens.
\end{itemize}

This distinction affects how requests participate in the engine busy loop.
Both request types incur prefill execution; only decode requests participate in
subsequent decode iterations.

% ------------------------------------------------------------
\subsection{Request Lifecycle}
\label{subsubsec:request-life-cycle}

\begin{figure}[!ht]
  \centering
  \includegraphics[height=0.8\textwidth]{figs/reqlifecycle.png}
  \caption{Request lifecycle in a vLLM-style inference engine.}
  \label{fig:req-lifecycle}
\end{figure}

The lifecycle of an inference request proceeds through the stages illustrated
in Figure~\ref{fig:req-lifecycle} and described below.

\paragraph*{1. Ingress.}
A client emits an inference request $i$ to the server.
This stage contributes a network latency that depends linearly on the number of
input tokens $\nu_i^{\mathrm{in}}$.

\paragraph*{2. Pre-processing.}
The API layer tokenizes the input, performs lightweight validation, and enqueues
the request for execution.
This stage contributes a latency that depends linearly on
$\nu_i^{\mathrm{in}}$ and does not interact with GPU scheduling.

\paragraph*{3. Scheduling.}
The request waits in the engine queue until it is admitted into a busy-loop
iteration.
During this period, other requests may be serviced by the engine.

\paragraph*{4. Chunked prefill.}
The uncached portion of the request prompt is processed during prefill.
The request participates in one or more busy-loop iterations, each processing a
fixed-size chunk of tokens, except possibly for the final iteration.
The prefill latency equals the sum of the durations of the iterations in which
the request participates in prefill.

\paragraph*{5. Decode.}
For decode requests, the engine generates output tokens iteratively.
Each busy-loop iteration advances at most one output token for the request.
The decode latency equals the sum of the durations of the iterations in which
the request participates in decode.
The first output token is generated during the prefill phase; subsequent tokens
are generated during decode.

\paragraph*{6. Post-processing.}
Once all output tokens are produced, the API layer detokenizes the generated
sequence and prepares the final response.
This stage contributes a latency that depends linearly on the number of output
tokens $\nu_i^{\mathrm{out}}$.

\paragraph*{7. Egress.}
The server transmits the response back to the client.
This contributes a network latency that depends linearly on
$\nu_i^{\mathrm{out}}$.

\paragraph{Modeling focus.}
Among these stages, only prefill and decode interact with the engine busy loop
and contribute to step-level execution time.
All other stages are treated as additive, request-local delays and are not
modeled at step granularity.

\section{Contributions}
\label{sec:contributions}

\paragraph{Overview.}
This paper introduces a unified, factorized framework for modeling the performance of modern large language model (LLM) inference systems. The central insight is that end-to-end inference latency can be decomposed into a small number of reusable, composable components: execution physics, queueing dynamics, cache behavior, and workload demand. We show that each component can be learned or parameterized independently from measurements or simulation, and then recomposed to predict request-level latency distributions under new workloads, cache configurations, and deployment environments---without re-running full-fidelity simulation. This factorization enables a new class of fast, transferable, and distribution-aware performance analyses that are infeasible with existing simulators or analytical models.

% ------------------------------------------------------------
\subsection*{1. Factorized inference performance abstraction}

\paragraph{Contribution.}
We introduce a principled factorization of LLM inference performance into four orthogonal layers: (i) execution physics, capturing how GPU execution time depends on batch composition; (ii) queueing dynamics, capturing admission, interference, and concurrency effects; (iii) cache behavior, capturing prefix reuse and multi-tier KV residency; and (iv) workload demand, capturing arrival processes, request sizes, and session structure. This factorization breaks the tight coupling between workload, hardware, and simulator state that dominates existing approaches, and allows each layer to be learned, validated, and reused independently.

\paragraph{Technique.}
Formally, we express the completion latency of a request $r$ as a composition
\[
L_r
=
\mathcal{Q}\big(
\mathcal{E}(\beta_e),\;
\mathcal{C}(\theta_c),\;
\mathcal{W}(w)
\big),
\]
where $\mathcal{E}$ is an execution model parameterized by environment-specific coefficients $\beta_e$, $\mathcal{C}$ is a cache abstraction parameterized by $\theta_c$, and $\mathcal{W}$ is a workload generator parameterized by a profile $w$. Crucially, $\mathcal{E}$ depends only on the deployment environment, $\mathcal{C}$ depends on cache policy and capacity, and $\mathcal{W}$ depends only on client-side behavior. The queueing operator $\mathcal{Q}$ composes these components via a step-indexed stochastic process. This separation ensures that changing workloads or cache policies does not require relearning execution physics, and vice versa.

% ------------------------------------------------------------
\subsection*{2. BLIS: learned execution physics for step-synchronous inference}

\paragraph{Contribution.}
We present BLIS, a learned execution model that compresses the complex GPU execution behavior of vLLM-style inference engines into a compact, physically interpretable step-time law. BLIS captures dynamic batching, interference between prefill and decode, and chunked execution, while remaining agnostic to workload composition. This learned execution physics serves as the foundation for all higher-level queueing and cache analyses in the paper.

\paragraph{Technique.}
BLIS models inference as a sequence of step-synchronous GPU forward passes. Each step $k$ processes $T_k^{\mathrm{pf}}$ prefill tokens and $T_k^{\mathrm{dec}}$ decode tokens and has duration
\[
\Delta t_k
=
\beta_{e,0}
+
\beta_{e,1} T_k^{\mathrm{pf}}
+
\beta_{e,2} T_k^{\mathrm{dec}},
\]
where $\beta_e \in \mathbb{R}^3_{\ge 0}$ is environment-specific. We learn $\beta_e$ via a continuous EM procedure over production traces, using phase-overlap integrals to infer latent step workloads without observing step boundaries. To enable transfer across environments, we further parameterize $\beta_e = x_e^\top B$ using environment feature vectors $x_e$, yielding a global execution map $B$ that generalizes to unseen deployments.

% ------------------------------------------------------------
\subsection*{3. Aggregate step-indexed queueing model}

\paragraph{Contribution.}
We introduce an aggregate, step-indexed queueing model that captures admission control, batching interference, and concurrency effects in LLM inference without tracking individual requests. Unlike classical queueing models, our model operates at the granularity of GPU steps and preserves the coupling between batch composition and service time, while remaining fast enough for large-scale Monte Carlo analysis.

\paragraph{Technique.}
The system state at step boundary $k$ is represented by population counts
\[
S_k = \big(H_k^{\mathrm{pf}},\, H_k^{\mathrm{dec}},\, Q_k\big),
\]
where $H_k^{\mathrm{pf}}$ and $H_k^{\mathrm{dec}}$ are histograms of admitted requests with remaining prefill steps and decode tokens, and $Q_k$ is an external admission queue. Transitions are defined deterministically by decrementing remaining work and stochastically by admitting new arrivals and injecting arrivals during step durations. Step durations $\Delta_k$ are computed using the BLIS execution law, yielding a discrete-time Markov chain whose transition structure is workload-invariant, but whose induced stationary distribution depends on workload parameters.

% ------------------------------------------------------------
\subsection*{4. Transferable multi-tier KV cache abstraction}

\paragraph{Contribution.}
We propose a transferable abstraction for modeling KV cache behavior that supports multiple cache tiers (GPU, CPU, storage) and captures the impact of prefix reuse on both execution cost and admission feasibility. This abstraction replaces explicit cache simulation with a compact statistical object that can be learned once and reused across workloads and queueing analyses.

\paragraph{Technique.}
We model cache behavior via a learned cache transfer function that maps reuse signals to tier outcomes. Concretely, for each workload class $c$, we estimate a reuse-distance distribution $F_c(d)$ over prefix keys. Given tier capacities $(C_{\mathrm{gpu}},C_{\mathrm{cpu}},C_{\mathrm{ssd}})$, tier hit probabilities are
\[
h^{\mathrm{gpu}}_c = F_c(C_{\mathrm{gpu}}),\quad
h^{\mathrm{cpu}}_c = F_c(C_{\mathrm{gpu}}{+}C_{\mathrm{cpu}}) - F_c(C_{\mathrm{gpu}}),
\]
with analogous expressions for lower tiers and misses. On each request arrival, the cache abstraction samples a tier outcome, determines the uncached prefill tokens $P$, and applies tier-specific latency penalties. This object composes directly with the queueing model and enables fast exploration of cache-policy tradeoffs.

% ------------------------------------------------------------
\subsection*{5. Performance-sufficient workload model}

\paragraph{Contribution.}
We introduce a workload model that is sufficient for performance prediction in LLM serving systems, capturing not only request size distributions but also session structure, temporal locality, and workflow composition. The model is expressive enough to represent multi-turn chat, RAG pipelines, agentic loops, and tool calls, while remaining compact and measurable from real telemetry.

\paragraph{Technique.}
Workloads are modeled as populations of sessions, each generating a sequence of turns with inter-turn gaps measured in time. Each turn instantiates a workflow (e.g., chat, RAG, agent step) that emits one or more inference requests with conditional input/output token distributions. Prefix-key access streams are generated alongside requests, enabling cache reuse modeling. The workload is fully specified by a parameter vector
\[
w = (\lambda_{\mathrm{sess}},\, G_{\Delta},\, \pi_c,\, \mathcal{L}_c^{\mathrm{in}},\, \mathcal{L}_c^{\mathrm{out}},\, F_c),
\]
which can be estimated from lightweight client and server logs via a measurement-to-profile compilation pipeline.

% ------------------------------------------------------------
\subsection*{6. Tagged-request analysis for distributional latency prediction}

\paragraph{Contribution.}
We show how to recover request-level latency distributions---including tail metrics such as P95 and P99---from an aggregate queueing model using tagged-request analysis. This enables distribution-aware performance prediction without per-request simulation, a capability absent from prior inference modeling work.

\paragraph{Technique.}
During aggregate DTMC simulation, admitted requests lose identity and are represented only through population counts. To estimate request-level delays, we inject a small number of tagged requests that retain identity and evolve alongside the aggregate background. Tagged requests experience the same step durations and admission decisions as background traffic, but are tracked individually to measure time-to-admission, time-to-first-token, and completion latency. By repeating this process over many Monte Carlo runs, we obtain accurate latency distributions while preserving the computational efficiency of the aggregate model.


\section{Experimental Design}
\label{sec:experiments}

We design experiments to validate both predictive accuracy and the qualitative insights enabled by our factorized model.
Our evaluation focuses on three questions: fidelity, transferability, and insight.

\subsection{Execution and cache fidelity}
We first validate BLIS and the KV Cache Transfer Function against full-fidelity simulation and/or production measurements.
We compare predicted and observed step times, effective uncached prefill tokens, and tier hit rates across multiple cache configurations and workload mixes.

\subsection{Distributional latency prediction under workload shift}
We evaluate the ability of our model to predict TTFT and end-to-end latency distributions under workload shift.
Using execution and cache models learned once, we vary workload composition (e.g., chat vs.\ RAG vs.\ agentic), session length, and inter-turn gaps.
We compare predicted and measured latency CDFs, with particular emphasis on P95 and P99 accuracy.

\subsection{Cache and workload design-space exploration}
We demonstrate that our model enables fast exploration of cache tier sizing and policy decisions.
We sweep GPU, CPU, and storage KV capacities under fixed workloads, and sweep workload locality under fixed cache budgets.
We show regime changes where naive metrics (e.g., hit ratio) fail to predict tail latency, while our model correctly identifies optimal operating points.

\subsection{Performance and scalability}
Finally, we compare runtime and scalability against full request-level simulation.
We show orders-of-magnitude speedups that enable Monte Carlo analysis and interactive design-space exploration that are infeasible with existing simulators.


% ============================================================
\section{Problem formulation}
\label{sec:problem_formulation}

% ------------------------------------------------------------
\subsection{Observability model: what we measure (and what we do not)}
\label{subsec:observability_model}

\paragraph{Environments.}
Let $e\in\mathcal{E}$ index a deployment environment, defined by a fixed
combination of: LLM (architecture/size), GPU type/interconnect, and engine
configuration (e.g., chunk size, parallelism, scheduler settings).
Each environment has nonnegative feature vectors
\[
x_e \in \mathbb{R}^{d_x}_{\ge 0},\qquad
z_e \in \mathbb{R}^{d_z}_{\ge 0},
\]
where $x_e$ captures execution-relevant features (e.g., parameter count, hidden
dimension, GPU FLOPs/bandwidth, interconnect characteristics, configuration
flags), and $z_e$ captures client-visible overhead features (often similar, but
allowed to differ; e.g., network topology).

\paragraph{Trace-side observations.}
In each environment $e$, we observe a collection of \emph{phase instances}
$\mathcal{I}_e$ extracted from production traces.
Each phase instance $i\in\mathcal{I}_e$ corresponds to either the \textsf{prefill}
phase or the \textsf{decode} phase of a particular request, and provides:
\begin{itemize}
  \item request identifier $r_i$;
  \item phase type $\tau_i\in\{\textsf{prefill},\textsf{decode}\}$;
  \item phase start/end timestamps $(t_{i,s},t_{i,e})$ and duration
        $\ell_i := t_{i,e}-t_{i,s}$;
  \item request-level aggregate token totals associated with $r_i$:
        prefill tokens $P_{r_i}$ and decode tokens $D_{r_i}$.
\end{itemize}
We assume $P_r$ counts only the \emph{uncached} prompt tokens that are actually
executed during prefill (prefix-cached tokens are excluded).

\paragraph{Client-side observations.}
For a subset of requests (or for separate client experiments) in environment $e$,
we observe per-request end-to-end latency $y_r$ and token lengths
$\nu_r^{\mathrm{in}}$ (input tokens) and $\nu_r^{\mathrm{out}}$ (output tokens).
Client-side $y_r$ includes networking, serialization, streaming, and all server
effects (queueing + execution); it is not assumed to expose internal phase
timestamps.

\paragraph{Unobserved quantities.}
Crucially, neither traces nor client data reveal:
\begin{itemize}
  \item busy-loop step boundaries;
  \item per-step runtimes;
  \item per-step token totals $(T_k^{\mathrm{pf}},T_k^{\mathrm{dec}})$.
\end{itemize}
Our estimators therefore operate by converting trace-visible
phase overlaps into \emph{time-integrated workload exposures} suitable for
regressing a step-level execution model.

% ------------------------------------------------------------
\subsection{Problem statement: featurized execution and client models}
\label{subsec:problem_statement_featurized}

\paragraph{Step-level execution model.}
We model a vLLM-style engine as a single busy-loop that executes a sequence of
discrete \emph{steps} (GPU forward passes), indexed by $k$.
Step $k$ processes some number of prefill tokens $T_k^{\mathrm{pf}}$ and decode
tokens $T_k^{\mathrm{dec}}$ across a dynamically constructed batch of active
requests, and has duration
\begin{equation}
\Delta t_k
=
\beta_{e,0}
+
\beta_{e,1}\,T_k^{\mathrm{pf}}
+
\beta_{e,2}\,T_k^{\mathrm{dec}},
\label{eq:featurized_step_model}
\end{equation}
where environment-specific coefficients
\[
\beta_e=(\beta_{e,0},\beta_{e,1},\beta_{e,2})
\]
have units
(sec/step, sec/token for prefill, sec/token for decode), respectively.

\paragraph{Client overhead model.}
Client-visible latency includes additional request-local overheads not visible
in server traces (e.g., client--server networking, request/response
serialization, streaming effects).
We model these overheads as
\begin{equation}
L^{\mathrm{client}}_{r,e}
=
\alpha_{e,0}
+
\alpha_{e,1}\,\nu_r^{\mathrm{in}}
+
\alpha_{e,2}\,\nu_r^{\mathrm{out}},
\label{eq:client_overhead_model}
\end{equation}
with environment-specific coefficients
$\alpha_e=(\alpha_{e,0},\alpha_{e,1},\alpha_{e,2})$ (units: seconds, sec/token,
sec/token).

\paragraph{Featurized coefficient maps.}
Rather than re-estimating $\beta_e$ and $\alpha_e$ independently for each
environment, we learn nonnegative linear feature maps:
\begin{equation}
\beta_e = x_e^\top B,
\qquad
\alpha_e = z_e^\top A,
\label{eq:featurized_maps}
\end{equation}
where
\[
B \in \mathbb{R}^{d_x \times 3}_{\ge 0},
\qquad
A \in \mathbb{R}^{d_z \times 3}_{\ge 0}.
\]
Nonnegativity of $(x_e,z_e,A,B)$ ensures physical interpretability and guarantees
$\beta_e,\alpha_e\ge 0$ for all environments (including unseen ones).

\paragraph{Learning objective}
Our estimators constructs, from trace-side observables, phase-level
regression features that approximate the \emph{step-aggregated} work experienced
during each trace phase instance. These features allow learning $B$ from traces
alone. Separately, we learn $A$ from client latencies after subtracting the
trace-visible server-side component.

Formally, the problem is:
\begin{itemize}
  \item \textbf{Trace-only:} estimate $B$ such that, for each phase instance
  $i\in\mathcal{I}_e$, the observed duration $\ell_i$ is well-predicted by a
  featurized version of the step model aggregated over the inferred step count.
  \item \textbf{Client residual:} estimate $A$ such that client residual latency
  (end-to-end minus trace-visible server latency) is well-predicted by the
  linear overhead model in Eq.~\eqref{eq:client_overhead_model}.
\end{itemize}

% ============================================================
\section{Continuous EM for Execution Coefficients}
\label{sec:featurized_continuous_em}

\subsection{Algorithmic intuition}
\label{subsec:algorithm_intuition}

At a high level, Continuous EM alternates between two questions.
First: given current estimates of execution speed, where in time would the
engine’s busy-loop steps most plausibly have occurred within each observed
phase window?
Second: given that implied placement of steps, what execution speeds best
explain the observed phase durations?
The algorithm iterates between these two steps, progressively refining both
the inferred step allocation and the execution coefficients until they are
mutually consistent.


\subsection{Trace pressures}
\label{subsec:trace_pressures}

This subsection defines (i) how we infer per-phase step counts from token totals,
(ii) how we compute trace-derived token ``pressures'' and integrated exposures,
and (iii) how we stack a global nonnegative least-squares (NNLS) problem to learn
$B$ and $A$.

% -------------------------
\subsubsection{Trace-inferred step counts and partial prefill chunking}
\label{subsubsec:featurized_step_counts}

\paragraph{Prefill and decode step counts.}
For each request $r$ in environment $e$, let $P_r$ be its uncached prefill token
count and $D_r$ be its decode token count.
Let $C$ denote the engine's prefill chunk size (tokens/step) for environment $e$.
We infer:
\begin{itemize}
  \item \textbf{Decode:} decode advances at most one token per step per active
  request; hence a decode phase instance has inferred step count
  \(
    N_i = D_{r_i}
  \)
  when $\tau_i=\textsf{decode}$.
  \item \textbf{Prefill:} prefill proceeds in chunks of up to $C$ tokens/step,
  with the final step possibly partial; hence a prefill phase instance of request
  $r$ has inferred step count
  \begin{equation}
  N_r^{\mathrm{pf}} = \left\lceil \frac{P_r}{C} \right\rceil,
  \qquad
  N_i = N_{r_i}^{\mathrm{pf}}
  \ \ \text{when }\tau_i=\textsf{prefill}.
  \label{eq:prefill_step_count}
  \end{equation}
\end{itemize}

\paragraph{Final partial prefill chunk.}
For a request $r$ with $N_r^{\mathrm{pf}}$ prefill steps, the last prefill step
processes
\begin{equation}
\rho_r
=
P_r - C\,(N_r^{\mathrm{pf}}-1),
\qquad
\rho_r\in(0,C],
\label{eq:rho_def_featurized}
\end{equation}
so the \emph{missing mass} relative to a full chunk is
\begin{equation}
\mu_r = C - \rho_r \in [0,C).
\label{eq:mu_def_featurized}
\end{equation}
Ignoring $\mu_r$ systematically overstates prefill token work.

% -------------------------
\subsubsection{Trace-derived pressures and integrated token exposures}
\label{subsubsec:pressures_exposures}

\paragraph{Trace-induced time grid.}
Let $\mathcal{G}=\{g_1<g_2<\cdots<g_{J+1}\}$ be the sorted set of all phase
boundaries $\{t_{i,s}\}\cup\{t_{i,e}\}$ within environment $e$.
This partitions time into half-open grid cells $[g_j,g_{j+1})$, on which phase
membership is constant.

\paragraph{Implementation (grid-exact integrals).}
All integrals are computed exactly on the trace-induced grid: 
for any piecewise-constant signal $f(t)$,
\[
\int_{t_{i,s}}^{t_{i,e}} f(t)\,dt
=
\sum_{j=1}^J f_j \cdot \bigl|\,[t_{i,s},t_{i,e})\cap[g_j,g_{j+1})\,\bigr|,
\]
where $f_j$ is the constant value of $f(t)$ on cell $[g_j,g_{j+1})$.

\paragraph{Instantaneous phase indicators.}
Define binary indicators at wall-clock time $t$:
\[
a_r(t)=\mathbf{1}\{r \text{ is in prefill at }t\},
\qquad
d_r(t)=\mathbf{1}\{r \text{ is in decode at }t\}.
\]
These are trace-visible because prefill/decode phase windows are observed.

\paragraph{Token pressures (units: tokens/step).}
We introduce time-varying \emph{token pressures} that describe the amount of
token work attributable, in expectation, to a representative busy-loop step
executed at wall-clock time $t$.
These pressures are derived from trace-visible phase overlaps and abstract away
the exact placement of individual steps.

For decode, the pressure is uniquely determined by phase membership:
\begin{align}
p^{\mathrm{dec}}(t)
&:= \sum_r d_r(t),
&&\text{(decode: one token per active decode request per step)}.
\label{eq:pressure_decode_featurized}
\end{align}

For prefill, we first define a full-chunk \emph{baseline} pressure
$p^{\mathrm{pf}}_{\mathrm{full}}(t)$:
\begin{align}
p^{\mathrm{pf}}_{\mathrm{full}}(t)
&:= C \sum_r a_r(t),
&&\text{(prefill: full-chunk assumption)}.
\label{eq:pressure_prefill_full_featurized}
\end{align}
This baseline corresponds to the approximation that each active prefill request
contributes a full chunk of $C$ tokens per step.
To account for the final partial prefill chunk, we introduce an \emph{effective}
(prefill) pressure $\mathrm{pf}_e(t;\beta_e)$, defined in
Section~\ref{sec:partial_prefill_chunk_correction}.

% ------------------------------------------------------------
\subsection{Latent step model and step-rate function}

We consider a fixed environment $e$ with feature vector $x_e$ and trace data
$\mathcal{I}_e$.
Execution proceeds as a sequence of latent busy-loop steps indexed by $k$.
Each step processes some number of prefill and decode tokens and has duration
\begin{equation}
\Delta t_k
=
\beta_{e,0}
+
\beta_{e,1}\,T^{\mathrm{pf}}_k
+
\beta_{e,2}\,T^{\mathrm{dec}}_k,
\label{eq:em_step_model}
\end{equation}
where $\beta_e=(\beta_{e,0},\beta_{e,1},\beta_{e,2})\in\mathbb{R}^3_{\ge 0}$.

\paragraph{Local step-time proxy}
In EM iteration $m=1,2,\ldots$, we set $\beta_e^{(m-1)} := x_e^\top B^{(m-1)}$
and use the \emph{effective} prefill pressure evaluated at the previous iterate,
\[
p^{\mathrm{pf},(m-1)}_e(t)
:=
p^{\mathrm{pf}}_{\mathrm{full}}(t)
-
\delta_e\!\left(t;\beta_e^{(m-1)}\right),
\]
to form the iteration-specific proxy
\begin{equation}
\Delta_{e,(m)}(t)
=
\beta^{(m-1)}_{e,0}
+
\beta^{(m-1)}_{e,1}\,p^{\mathrm{pf},(m-1)}_e(t)
+
\beta^{(m-1)}_{e,2}\,p^{\mathrm{dec}}(t).
\label{eq:em_step_time_proxy}
\end{equation}
This ``one-iterate lag'' ensures that the E-step uses a fully specified
(prefill) workload signal while avoiding a fixed-point solve within an EM
iteration.

\paragraph{Step-rate (intensity).}
The corresponding step-rate is defined as
\begin{equation}
\lambda_e(t;\beta_e)
=
\frac{1}{\Delta_e(t;\beta_e)},
\qquad
\text{units: steps/sec}.
\label{eq:em_step_rate}
\end{equation}
In EM iteration $m$, we evaluate Eq.~\eqref{eq:em_step_rate} using the
iteration-specific proxy $\Delta_{e,(m)}(t)$ from
Eq.~\eqref{eq:em_step_time_proxy} (i.e., we set
$\lambda_{e,(m)}(t):=1/\Delta_{e,(m)}(t)$).

We assume $\Delta_e(t;\beta_e)>0$ for all $t$ in the trace support, 
which can be ensured by clamping
$\Delta_e(t)\leftarrow\max\{\Delta_e(t),\Delta_{\min}\}$ for a small
$\Delta_{\min}>0$.

% ------------------------------------------------------------
\subsection{Phase membership and latent step densities}

For each phase instance $i\in\mathcal{I}_e$ with observed start and end times
$(t_{i,s},t_{i,e})$ and inferred step count $N_i$, we introduce a continuous
phase-membership function $w_i(t)\in[0,1]$ that captures uncertainty near
instrumented boundaries.
A simple boundary-aware choice is
\begin{equation}
w_i(t)
=
F_h(t-t_{i,s})\,
\bigl(1-F_h(t-t_{i,e})\bigr),
\label{eq:em_phase_membership}
\end{equation}
where $F_h$ is a one-sided cumulative kernel with bandwidth $h>0$
(e.g., $F_h(x)=1-e^{-x/h}$ for $x\ge 0$).
By construction, $w_i(t)=1$ throughout the interior of the observed phase window
and decays smoothly to zero near its boundaries.

Conditioned on execution coefficients $\beta_e$, we model the latent placement
of the $N_i$ steps of phase $i$ as independent draws from a continuous density
proportional to the step-rate.
The posterior step-density for phase $i$ is
\begin{equation}
q_i(t \mid \beta_e)
=
\frac{w_i(t)\,\lambda_e(t;\beta_e)}
{\int w_i(u)\,\lambda_e(u;\beta_e)\,du},
\label{eq:em_phase_step_density}
\end{equation}
which integrates to one.
The expected number of steps of phase $i$ occurring in an infinitesimal interval
$dt$ is $N_i\,q_i(t\mid\beta_e)\,dt$.

% ------------------------------------------------------------
\subsection{Partial prefill chunk correction}
\label{sec:partial_prefill_chunk_correction}

For a prefill request $r$, exactly one of its $N_r^{\mathrm{pf}}$ prefill steps
processes fewer than $C$ tokens.
Let $\mu_r$ denote the missing token mass of that final step
(defined in Eq.~\eqref{eq:mu_def_featurized}).

We localize the final prefill step near the observed end of the prefill phase
using a boundary-local kernel $\varphi_r(t)$ supported on $(-\infty,t_{r,e}]$,
normalized to integrate to one. 
Concretely, we define $\varphi_r(t)$ as a one-sided exponential kernel:
\begin{equation}
\varphi_r(t)
=
\frac{1}{h}\,
\exp\!\left(-\frac{t_{r,e}-t}{h}\right)
\mathbf{1}\{t \le t_{r,e}\},
\label{eq:final_prefill_kernel}
\end{equation}
where $h>0$ is a bandwidth parameter controlling the temporal localization.
By construction, $\varphi_r(t)$ is nonnegative, supported on
$(-\infty,t_{r,e}]$, and satisfies
$\int \varphi_r(t)\,dt = 1$.

The corresponding posterior density of the final prefill step is
\begin{equation}
q^{\mathrm{final}}_r(t \mid \beta_e)
=
\frac{\varphi_r(t)\,\lambda_e(t;\beta_e)}
{\int \varphi_r(u)\,\lambda_e(u;\beta_e)\,du}.
\label{eq:em_final_step_density}
\end{equation}

The expected missing-token correction per step is then
\begin{equation}
\delta_e(t;\beta_e)
=
\frac{\sum_r \mu_r\,q^{\mathrm{final}}_r(t\mid\beta_e)}
{\lambda_e(t;\beta_e)},
\qquad
\text{units: tokens/step}.
\label{eq:em_partial_chunk_correction}
\end{equation}

The effective prefill pressure used in EM is
\begin{equation}
p^{\mathrm{pf}}_e(t)
=
p^{\mathrm{pf}}_{\mathrm{full}}(t)
-
\delta_e(t;\beta_e),
\label{eq:em_effective_prefill_pressure}
\end{equation}
which ensures that each request contributes exactly one partial prefill chunk
in expectation.

\paragraph{Dependence on $\beta_e$.}
Because $\delta_e(t;\beta_e)$ in Eq.~\eqref{eq:em_partial_chunk_correction}
depends on the step-rate $\lambda_e(t;\beta_e)$, the effective prefill pressure
$p^{\mathrm{pf}}_e(t)$ in Eq.~\eqref{eq:em_effective_prefill_pressure}
is $\beta_e$-dependent.
To emphasize this, we will sometimes write
$p^{\mathrm{pf}}_e(t;\beta_e)$, though we drop the explicit argument when
unambiguous.

% ------------------------------------------------------------
\subsection{EM objective and updates}

\paragraph{Overview of the EM procedure.}
The continuous EM algorithm proceeds in iterations indexed by
$m = 0,1,2,\ldots$ and operates on the \emph{global} feature-to-execution map
$B$.
At iteration $m \geq 1$, environment-specific execution coefficients are obtained as
$\beta_e^{(m-1)} := x_e^\top B^{(m-1)}$ for each environment $e$.
These coefficients are used in the E-step to infer trace-consistent latent step
allocations and step-averaged token exposures.
In the M-step, the global map $B$ is updated via a pooled nonnegative
least-squares solve across all training environments.
Iterations continue until convergence of $B$ (up to a prescribed tolerance) or
until a fixed maximum number of iterations is reached. We handle the special case
of Iteration $0$ in Section \ref{subsec:initialization}.

\paragraph{E-step.}
Given the current global map $B^{(m-1)}$, we set $\beta_e^{(m-1)} := x_e^\top B^{(m-1)}$
for each environment $e$. In iteration $m$, all E-step quantities are 
evaluated at $\beta_e^{(m-1)}$. We compute:
\begin{itemize}
  \item the step-rate $\lambda_e(t;\beta_e^{(m-1)})$;
  \item phase step densities $q_i(t\mid\beta_e^{(m-1)})$;
  \item effective prefill pressure $p^{\mathrm{pf}}_e(t)$.
\end{itemize}

Using these quantities, form step-averaged pressures
\begin{align}
\bar p^{\mathrm{pf}}_{i,(m)}
&=
\int q_i(t\mid\beta_e^{(m-1)})\,
p^{\mathrm{pf}}_e(t)\,dt,
\\
\bar p^{\mathrm{dec}}_{i,(m)}
&=
\int q_i(t\mid\beta_e^{(m-1)})\,
p^{\mathrm{dec}}(t)\,dt.
\end{align}

\paragraph{M-step (global pooled NNLS for $B$).}
In the M-step, we update the global execution map $B$ while enforcing the
feature-based parameterization $\beta_e = x_e^\top B$ for all environments.
Given step-averaged pressures
$(\bar p^{\mathrm{pf}}_{i,(m)},\bar p^{\mathrm{dec}}_{i,(m)})$
from the E-step, we form pooled regression rows for all phase instances
$i\in\mathcal{I}_{\mathrm{train}}$ across all environments.

For a phase instance $i$ belonging to environment $e(i)$, define
\begin{equation}
\phi_{i,(m)}
:=
\big[
N_i x_{e(i)}^\top,\;
N_i\bar p^{\mathrm{pf}}_{i,(m)} x_{e(i)}^\top,\;
N_i\bar p^{\mathrm{dec}}_{i,(m)} x_{e(i)}^\top
\big],
\label{eq:phi_em_row}
\end{equation}
and let $\theta := \mathrm{vec}(B)$ stack the columns of $B$.
The M-step solves the pooled nonnegative least-squares problem
\begin{equation}
\widehat B^{(m)}
=
\arg\min_{B\in\mathbb{R}^{d_x\times 3}_{\ge 0}}
\sum_{i\in\mathcal{I}_{\mathrm{train}}}
\left(
\phi_{i,(m)}\,\mathrm{vec}(B) - \ell_i
\right)^2 .
\label{eq:em_mstep_pooled_B}
\end{equation}
This jointly updates the execution coefficients across all environments,
enforcing a shared feature-to-performance map.

\subsection{Iteration 0: Initialization via aggregated workloads}
\label{subsec:initialization}

\paragraph{Initialization as a trivial E-step.}
Initialization can be interpreted as a degenerate first EM iteration in which
the latent structure is intentionally weakened.
For each phase instance $i$ with observed window
$[t_{i,s},t_{i,e})$, we set the latent step density to be uniform:
\begin{equation}
q_i^{(0)}(t)
:=
\frac{1}{\ell_i}\,
\mathbf{1}\!\left\{ t \in [t_{i,s}, t_{i,e}) \right\},
\qquad
\ell_i := t_{i,e} - t_{i,s}.
\label{eq:uniform_q_initialization}
\end{equation}
Under this assignment, no step-rate $\lambda_e(t;\beta_e)$ is evaluated, no
partial-chunk correction $\delta_e(t;\beta_e)$ is introduced, and no kernel
localization is applied.

\paragraph{Induced regression features.}
With $q_i^{(0)}(t)$ uniform, the step-averaged token pressures reduce to simple
time averages over the observed phase window:
\begin{align}
\bar p^{\mathrm{pf,base}}_i
&:=
\frac{1}{\ell_i}
\int_{t_{i,s}}^{t_{i,e}}
p^{\mathrm{pf}}_{\mathrm{full}}(t)\,dt,
\\
\bar p^{\mathrm{dec,base}}_i
&:=
\frac{1}{\ell_i}
\int_{t_{i,s}}^{t_{i,e}}
p^{\mathrm{dec}}(t)\,dt.
\end{align}

\paragraph{Identical M-step.}
Using these features, the M-step at iteration $m=0$ is carried out exactly as in
Eq.~\eqref{eq:em_mstep_pooled_B}, yielding the initial estimate $B^{(0)}$.



% ------------------------------------------------------------
\subsection{Transfer and optional NNLS recalibration for a new environment}
\label{subsec:transfer}

Let $e^\star$ be a new deployment environment with feature vector $x_{e^\star}$.

\paragraph{Zero-shot transfer.}
After training, BLIS produces a single global execution map $\widehat B$ learned
from pooled trace data across environments.
For a new environment $e^\star$---including one with no trace data---execution
coefficients are predicted in a zero-shot manner as
\begin{equation}
\beta^{\mathrm{feat}}_{e^\star}
:=
x_{e^\star}^\top \widehat B .
\label{eq:zero_shot_beta}
\end{equation}
This feature-based prediction is physically valid by construction and enables
immediate latency and throughput estimation without requiring any traces from
$e^\star$.

\paragraph{Optional NNLS recalibration with trace data.}
When trace data $\mathcal{I}_{e^\star}$ is available for the new environment,
BLIS optionally performs a lightweight environment-local recalibration to refine
$\beta_{e^\star}$ while keeping the global map $\widehat B$ fixed.
Using trace-derived step counts $N_i$ and surrogate step-averaged pressures
$(\bar p^{\mathrm{pf,base}}_i,\bar p^{\mathrm{dec,base}}_i)$ from
Section~\ref{subsec:initialization}, we solve the convex nonnegative
least-squares problem
\begin{equation}
\widehat\beta_{e^\star}
=
\arg\min_{\beta\in\mathbb{R}^3_{\ge 0}}
\Bigg[
\sum_{i\in\mathcal{I}_{e^\star}}
\left(
N_i\beta_0
+
N_i\bar p^{\mathrm{pf,base}}_i\,\beta_1
+
N_i\bar p^{\mathrm{dec,base}}_i\,\beta_2
-
\ell_i
\right)^2
+
\lambda_{\mathrm{recal}}
\left\|
\beta - \beta^{\mathrm{feat}}_{e^\star}
\right\|_2^2
\Bigg].
\label{eq:nnls_recalibration_beta}
\end{equation}
The regularization parameter $\lambda_{\mathrm{recal}}\ge 0$ controls the degree
of trust in the feature-based prediction: $\lambda_{\mathrm{recal}}=0$ yields a
purely trace-driven fit, while larger values encourage adherence to the
zero-shot prior.

\paragraph{Separation of roles.}
Pooled EM is used \emph{only} during training to learn the global execution map
$\widehat B$.
Transfer to a new environment relies on zero-shot prediction via features, with
optional NNLS recalibration as a strictly local post-processing step.
This separation preserves generalization, avoids overfitting in low-data
regimes, and keeps transfer lightweight and robust.


% -------------------------
\section{Learning $A$ from client residuals}
\label{sec:nnls_A_client}

\paragraph{Trace-visible server latency per request.}
For a request $r$ in environment $e$, traces provide its prefill phase duration
$\ell^{\mathrm{pf}}_r$ and (if applicable) decode phase duration
$\ell^{\mathrm{dec}}_r$. Define the trace-visible server-side latency
\begin{equation}
S^{\mathrm{trace}}_{r,e}
:=
\ell^{\mathrm{pf}}_r + \ell^{\mathrm{dec}}_r,
\label{eq:server_trace_latency_def}
\end{equation}
where $\ell^{\mathrm{dec}}_r:=0$ for prefill-only requests.

\paragraph{Client residual.}
For requests where we observe end-to-end client latency $y_r$, define
\begin{equation}
R_{r,e} := y_r - S^{\mathrm{trace}}_{r,e}.
\label{eq:client_residual_def}
\end{equation}
By construction, $R_{r,e}$ isolates client-visible effects not captured by
server traces (networking, serialization, streaming).

\paragraph{Featurized client model and NNLS for $A$.}
Substituting $\alpha_e=z_e^\top A$ into Eq.~\eqref{eq:client_overhead_model}
yields
\[
R_{r,e}
\approx
(z_e^\top a_0)
+
\nu_r^{\mathrm{in}}(z_e^\top a_1)
+
\nu_r^{\mathrm{out}}(z_e^\top a_2),
\]
where $a_0,a_1,a_2$ are columns of $A$.
Define the per-request row
\[
\psi_r
:=
\big[
1\cdot z_e^\top,\;
\nu_r^{\mathrm{in}} z_e^\top,\;
\nu_r^{\mathrm{out}} z_e^\top
\big],
\]
and stack across all observed client requests to solve the global NNLS:
\begin{equation}
\widehat A
=
\arg\min_{A\in\mathbb{R}^{d_z\times 3}_{\ge 0}}
\sum_{(r,e)\in\mathcal{D}_{\mathrm{client}}}
\left(
\psi_r\,\mathrm{vec}(A) - R_{r,e}
\right)^2.
\label{eq:global_nnls_A}
\end{equation}

\paragraph{Separation of concerns.}
The estimation of $B$ uses \emph{only} trace instances and is unaffected by
client networking effects. The estimation of $A$ uses \emph{only} client
residuals after removing trace-visible server latency. Thus, execution and
client overhead learning are cleanly decoupled.

\subsection{One-shot recalibration of client-side overheads}
\label{subsec:client-side-recalibration}

For a new environment $e^\star$ with paired client and trace data, we form
trace-decomposed residuals
\[
R_r := y_r - S_r^{\mathrm{trace}},
\qquad r\in\mathcal{J}_{e^\star},
\]
where $y_r$ is end-to-end latency observed at the client and
$S_r^{\mathrm{trace}}$ is the server-side latency observable from the trace
(admission, batching, and execution).

Given the featurized prediction
\[
\alpha^{\mathrm{feat}}_{e^\star} := z_{e^\star}^\top \widehat A,
\]
BLIS recalibrates $\alpha_{e^\star}$ by solving a regularized nonnegative least
squares problem:
\begin{equation}
\widehat \alpha_{e^\star}
=
\arg\min_{\alpha\in\mathbb{R}^3_{\ge 0}}
\sum_{r\in\mathcal{J}_{e^\star}}
\left(
\alpha_0
+
\alpha_1 \nu_r^{\mathrm{in}}
+
\alpha_2 \nu_r^{\mathrm{out}}
-
R_r
\right)^2
\;+\;
\lambda_{\mathrm{recal}}^{\alpha}
\left\|
\alpha-\alpha^{\mathrm{feat}}_{e^\star}
\right\|_2^2.
\label{eq:alpha_recalibration_to_featurized_prior}
\end{equation}
This objective is convex and decoupled from server scheduling dynamics by
construction, since all server-side queueing and interference are absorbed into
$S_r^{\mathrm{trace}}$ and removed before fitting $\alpha$.

\bibliographystyle{plain}
\bibliography{blis}

\end{document}
