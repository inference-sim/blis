\documentclass[11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{times}
\usepackage[title, titletoc]{appendix}
\usepackage{graphicx}
\usepackage{amsmath, amssymb}
\usepackage{mathtools} % for \xmapsto
\usepackage{bm}
\usepackage[most]{tcolorbox}
\usepackage{tabularx}
\usepackage{pifont} % optional for symbols
% --- Flowcharts (TikZ) ---
\usepackage{tikz}
\usetikzlibrary{arrows.meta,positioning,shapes.geometric,fit,calc}
\usepackage{xcolor}
% Algorithms (compact)
\usepackage{algorithm}
\usepackage{algpseudocode}

% Optional: slightly tighter algorithm typography
\algrenewcommand\algorithmicrequire{\textbf{Input:}}
\algrenewcommand\algorithmicensure{\textbf{Output:}}
\algrenewcommand\algorithmiccomment[1]{\hfill{\footnotesize$\triangleright$~#1}}

\usepackage{hyperref}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{booktabs,longtable,array,makecell}
\setlength{\LTleft}{0pt}   % no left indent for longtable
\setlength{\LTright}{0pt}  % no right indent for longtable
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}p{#1}}


% --- Theorems, lemmas, claims, proofs ---
\usepackage{amsthm}

\theoremstyle{plain} % bold headings, italic body
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{claim}[theorem]{Claim}

\theoremstyle{definition} % bold heading, upright body
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark} % italic heading, upright body
\newtheorem{remark}[theorem]{Remark}
\newtheorem{note}[theorem]{Note}

% Optional: unnumbered versions
\newtheorem*{theorem*}{Theorem}
\newtheorem*{lemma*}{Lemma}
\newtheorem*{claim*}{Claim}

% Proof environment is provided by amsthm:
% \begin{proof} ... \end{proof}
% Customize QED symbol if you like:
\renewcommand{\qedsymbol}{$\blacksquare$}

\newcommand{\code}[1]{\texttt{#1}}

\definecolor{prefillcolor}{RGB}{173,216,230}  % Light blue
\definecolor{decodecolor}{RGB}{144,238,144}   % Light green
\definecolor{gridcolor}{RGB}{200,200,200}     % Light gray
\definecolor{highlightcolor}{RGB}{255,182,193} % Light pink for integration region


% (Optional) If you want equations numbered by section (since you already use amsmath):
\numberwithin{equation}{section}



\title{BLIS: Blackbox Inference Performance Estimator}
\author{AI Platform Optimization Team}
\date{\today}

\begin{document}

\maketitle

% ============================================================
\subsection{vLLM Execution Model (Serving Semantics)}
\label{subsec:vllm-overview}

We abstract vLLM as a representative example of a modern GPU-based LLM inference
engine.
Our objective is not to document vLLM’s implementation, but to extract the
minimal execution semantics required to model request latency and throughput
from production traces.
The resulting abstraction applies to a broad class of vLLM-style inference
engines that employ dynamic batching, chunked prefill, and iterative decoding.

At a high level, the system consists of a front-end API layer and a back-end
execution engine, connected by an asynchronous queue.
Only the execution engine participates directly in GPU scheduling and is
modeled at step-level granularity.

% ------------------------------------------------------------
\subsubsection{API layer (abstracted)}

The API layer handles client-facing responsibilities, including request ingress,
tokenization, response streaming, and final detokenization.
It operates asynchronously with respect to GPU execution and does not influence
the formation or scheduling of GPU batches.

In our model, the API layer contributes latency that depends linearly on the
number of input and output tokens.
These costs are treated as request-local and are modeled separately from the
engine execution loop.

% ------------------------------------------------------------
\subsubsection{Engine core and busy-loop semantics}

The engine core executes a centralized inference loop driven by a scheduler.
We model this loop as a sequence of discrete \emph{busy-loop iterations}, or
\emph{steps}, each corresponding to a single GPU forward pass over a dynamically
constructed batch of active requests.

We assume the following execution invariants, consistent with vLLM-style
engines:

\begin{enumerate}
  \item Each busy-loop iteration executes exactly one forward pass on the GPU.
  \item A request participates in at most one phase (prefill or decode) per
        iteration.
  \item During decode, a request advances by at most one output token per
        iteration.
  \item During prefill, a request advances by a fixed chunk size of tokens per
        iteration, except possibly for the final prefill iteration.
  \item The duration of each iteration depends on the total number of prefill
        and decode tokens processed in that iteration.
\end{enumerate}

These invariants induce a natural \emph{step-level execution model}, which
serves as the foundation for the latency model and coefficient estimation
procedures developed in the remainder of the paper.

% ============================================================
\section{Latency Model}
\label{sec:latency-model}

We now formalize the latency model used by BLIS.
The model decomposes request latency into a sequence of stages corresponding to
request ingress, engine execution, and response egress.
Only stages that interact with the engine busy loop are modeled at step-level
granularity; all other stages are treated as linear token-dependent delays.

% ------------------------------------------------------------
\subsection{Request Types}
\label{subsubsec:request-types}

We distinguish between two classes of inference requests based on their maximum
output length:

\begin{itemize}
  \item \emph{Prefill-only requests}, which generate exactly one output token.
  \item \emph{Decode requests}, which generate one or more output tokens.
\end{itemize}

This distinction affects how requests participate in the engine busy loop.
Both request types incur prefill execution; only decode requests participate in
subsequent decode iterations.

% ------------------------------------------------------------
\subsection{Request Lifecycle}
\label{subsubsec:request-life-cycle}

\begin{figure}[!ht]
  \centering
  \includegraphics[height=0.8\textwidth]{figs/reqlifecycle.png}
  \caption{Request lifecycle in a vLLM-style inference engine.}
  \label{fig:req-lifecycle}
\end{figure}

The lifecycle of an inference request proceeds through the stages illustrated
in Figure~\ref{fig:req-lifecycle} and described below.

\paragraph*{1. Ingress.}
A client emits an inference request $i$ to the server.
This stage contributes a network latency that depends linearly on the number of
input tokens $\ell_i^{\mathrm{in}}$.

\paragraph*{2. Pre-processing.}
The API layer tokenizes the input, performs lightweight validation, and enqueues
the request for execution.
This stage contributes a latency that depends linearly on
$\ell_i^{\mathrm{in}}$ and does not interact with GPU scheduling.

\paragraph*{3. Scheduling.}
The request waits in the engine queue until it is admitted into a busy-loop
iteration.
During this period, other requests may be serviced by the engine.

\paragraph*{4. Chunked prefill.}
The uncached portion of the request prompt is processed during prefill.
The request participates in one or more busy-loop iterations, each processing a
fixed-size chunk of tokens, except possibly for the final iteration.
The prefill latency equals the sum of the durations of the iterations in which
the request participates in prefill.

\paragraph*{5. Decode.}
For decode requests, the engine generates output tokens iteratively.
Each busy-loop iteration advances at most one output token for the request.
The decode latency equals the sum of the durations of the iterations in which
the request participates in decode.
The first output token is generated during the prefill phase; subsequent tokens
are generated during decode.

\paragraph*{6. Post-processing.}
Once all output tokens are produced, the API layer detokenizes the generated
sequence and prepares the final response.
This stage contributes a latency that depends linearly on the number of output
tokens $\ell_i^{\mathrm{out}}$.

\paragraph*{7. Egress.}
The server transmits the response back to the client.
This contributes a network latency that depends linearly on
$\ell_i^{\mathrm{out}}$.

\paragraph{Modeling focus.}
Among these stages, only prefill and decode interact with the engine busy loop
and contribute to step-level execution time.
All other stages are treated as additive, request-local delays and are not
modeled at step granularity.


% ============================================================
\section{Featurized Baseline Estimation from Trace and Client Observations}
\label{sec:featurized_baseline}

This section unifies (i) trace-only estimation of step-level execution
coefficients and (ii) Thus, execution and server-side queueing/interference 
are absorbed into traces, while client-only overhead is learned 
from residuals. The key outcome is a portable, environment-conditioned 
latency model in which \emph{server execution} coefficients and 
\emph{client-visible overhead} coefficients are learned via nonnegative 
linear maps from environment features. Throughout, we assume 
\emph{no per-step instrumentation}: traces do not expose busy-loop step 
boundaries or per-step runtimes.

% ------------------------------------------------------------
\subsection{Observability model: what we measure (and what we do not)}
\label{subsec:observability_model}

\paragraph{Environments.}
Let $e\in\mathcal{E}$ index a deployment environment, defined by a fixed
combination of: LLM (architecture/size), GPU type/interconnect, and engine
configuration (e.g., chunk size, parallelism, scheduler settings).
Each environment has nonnegative feature vectors
\[
x_e \in \mathbb{R}^{d_x}_{\ge 0},\qquad
z_e \in \mathbb{R}^{d_z}_{\ge 0},
\]
where $x_e$ captures execution-relevant features (e.g., parameter count, hidden
dimension, GPU FLOPs/bandwidth, interconnect characteristics, configuration
flags), and $z_e$ captures client-visible overhead features (often similar, but
allowed to differ; e.g., network topology).

\paragraph{Trace-side observations.}
In each environment $e$, we observe a collection of \emph{phase instances}
$\mathcal{I}_e$ extracted from production traces.
Each phase instance $i\in\mathcal{I}_e$ corresponds to either the \textsf{prefill}
phase or the \textsf{decode} phase of a particular request, and provides:
\begin{itemize}
  \item request identifier $r_i$;
  \item phase type $\tau_i\in\{\textsf{prefill},\textsf{decode}\}$;
  \item phase start/end timestamps $(t_{i,s},t_{i,e})$ and duration
        $\ell_i := t_{i,e}-t_{i,s}$;
  \item request-level aggregate token totals associated with $r_i$:
        prefill tokens $P_{r_i}$ and decode tokens $D_{r_i}$.
\end{itemize}
We assume $P_r$ counts only the \emph{uncached} prompt tokens that are actually
executed during prefill (prefix-cached tokens are excluded).

\paragraph{Client-side observations.}
For a subset of requests (or for separate client experiments) in environment $e$,
we observe per-request end-to-end latency $y_r$ and token lengths
$\ell_r^{\mathrm{in}}$ (input tokens) and $\ell_r^{\mathrm{out}}$ (output tokens).
Client-side $y_r$ includes networking, serialization, streaming, and all server
effects (queueing + execution); it is not assumed to expose internal phase
timestamps.

\paragraph{Unobserved quantities.}
Crucially, neither traces nor client data reveal:
\begin{itemize}
  \item busy-loop step boundaries;
  \item per-step runtimes;
  \item per-step token totals $(T_k^{\mathrm{pf}},T_k^{\mathrm{dec}})$.
\end{itemize}
The baseline estimator therefore operates by converting trace-visible
phase overlaps into \emph{time-integrated workload exposures} suitable for
regressing a step-level execution model.

% ------------------------------------------------------------
\subsection{Problem statement: featurized execution and client models}
\label{subsec:problem_statement_featurized}

\paragraph{Step-level execution model.}
We model a vLLM-style engine as a single busy-loop that executes a sequence of
discrete \emph{steps} (GPU forward passes), indexed by $k$.
Step $k$ processes some number of prefill tokens $T_k^{\mathrm{pf}}$ and decode
tokens $T_k^{\mathrm{dec}}$ across a dynamically constructed batch of active
requests, and has duration
\begin{equation}
\Delta t_k
=
\beta_{e,0}
+
\beta_{e,1}\,T_k^{\mathrm{pf}}
+
\beta_{e,2}\,T_k^{\mathrm{dec}},
\label{eq:featurized_step_model}
\end{equation}
where environment-specific coefficients
\[
\beta_e=(\beta_{e,0},\beta_{e,1},\beta_{e,2})
\]
have units
(sec/step, sec/token for prefill, sec/token for decode), respectively.

\paragraph{Client overhead model.}
Client-visible latency includes additional request-local overheads not visible
in server traces (e.g., client--server networking, request/response
serialization, streaming effects).
We model these overheads as
\begin{equation}
L^{\mathrm{client}}_{r,e}
=
\alpha_{e,0}
+
\alpha_{e,1}\,\ell^{\mathrm{in}}_r
+
\alpha_{e,2}\,\ell^{\mathrm{out}}_r,
\label{eq:client_overhead_model}
\end{equation}
with environment-specific coefficients
$\alpha_e=(\alpha_{e,0},\alpha_{e,1},\alpha_{e,2})$ (units: seconds, sec/token,
sec/token).

\paragraph{Featurized coefficient maps.}
Rather than re-estimating $\beta_e$ and $\alpha_e$ independently for each
environment, we learn nonnegative linear feature maps:
\begin{equation}
\beta_e = x_e^\top B,
\qquad
\alpha_e = z_e^\top A,
\label{eq:featurized_maps}
\end{equation}
where
\[
B \in \mathbb{R}^{d_x \times 3}_{\ge 0},
\qquad
A \in \mathbb{R}^{d_z \times 3}_{\ge 0}.
\]
Nonnegativity of $(x_e,z_e,A,B)$ ensures physical interpretability and guarantees
$\beta_e,\alpha_e\ge 0$ for all environments (including unseen ones).

\paragraph{Learning objective (baseline).}
The baseline estimator constructs, from trace-side observables, phase-level
regression features that approximate the \emph{step-aggregated} work experienced
during each trace phase instance. These features allow learning $B$ from traces
alone. Separately, we learn $A$ from client latencies after subtracting the
trace-visible server-side component.

Formally, the problem is:
\begin{itemize}
  \item \textbf{Trace-only:} estimate $B$ such that, for each phase instance
  $i\in\mathcal{I}_e$, the observed duration $\ell_i$ is well-predicted by a
  featurized version of the step model aggregated over the inferred step count.
  \item \textbf{Client residual:} estimate $A$ such that client residual latency
  (end-to-end minus trace-visible server latency) is well-predicted by the
  linear overhead model in Eq.~\eqref{eq:client_overhead_model}.
\end{itemize}

% ------------------------------------------------------------
\subsection{Featurized baseline estimator: definitions, features, and NNLS}
\label{subsec:featurized_baseline_estimator}

This subsection defines (i) how we infer per-phase step counts from token totals,
(ii) how we compute trace-derived token ``pressures'' and integrated exposures,
and (iii) how we stack a global nonnegative least-squares (NNLS) problem to learn
$B$ and $A$.

% -------------------------
\subsubsection{Trace-inferred step counts and partial prefill chunking}
\label{subsubsec:featurized_step_counts}

\paragraph{Prefill and decode step counts.}
For each request $r$ in environment $e$, let $P_r$ be its uncached prefill token
count and $D_r$ be its decode token count.
Let $C$ denote the engine's prefill chunk size (tokens/step) for environment $e$.
We infer:
\begin{itemize}
  \item \textbf{Decode:} decode advances at most one token per step per active
  request; hence a decode phase instance has inferred step count
  \(
    N_i = D_{r_i}
  \)
  when $\tau_i=\textsf{decode}$.
  \item \textbf{Prefill:} prefill proceeds in chunks of up to $C$ tokens/step,
  with the final step possibly partial; hence a prefill phase instance of request
  $r$ has inferred step count
  \begin{equation}
  N_r^{\mathrm{pf}} = \left\lceil \frac{P_r}{C} \right\rceil,
  \qquad
  N_i = N_{r_i}^{\mathrm{pf}}
  \ \ \text{when }\tau_i=\textsf{prefill}.
  \label{eq:prefill_step_count}
  \end{equation}
\end{itemize}

\paragraph{Final partial prefill chunk.}
For a request $r$ with $N_r^{\mathrm{pf}}$ prefill steps, the last prefill step
processes
\begin{equation}
\rho_r
=
P_r - C\,(N_r^{\mathrm{pf}}-1),
\qquad
\rho_r\in(0,C],
\label{eq:rho_def_featurized}
\end{equation}
so the \emph{missing mass} relative to a full chunk is
\begin{equation}
\mu_r = C - \rho_r \in [0,C).
\label{eq:mu_def_featurized}
\end{equation}
Ignoring $\mu_r$ systematically overstates prefill token work. The baseline
handles this via a trace-only correction in \emph{exposure space}
(Section~\ref{subsubsec:pressures_exposures}).

% -------------------------
\subsubsection{Trace-derived pressures and integrated token exposures}
\label{subsubsec:pressures_exposures}

\paragraph{Trace-induced time grid.}
Let $\mathcal{G}=\{g_1<g_2<\cdots<g_{J+1}\}$ be the sorted set of all phase
boundaries $\{t_{i,s}\}\cup\{t_{i,e}\}$ within environment $e$.
This partitions time into half-open grid cells $[g_j,g_{j+1})$, on which phase
membership is constant.

\paragraph{Instantaneous phase indicators.}
Define binary indicators at wall-clock time $t$:
\[
a_r(t)=\mathbf{1}\{r \text{ is in prefill at }t\},
\qquad
d_r(t)=\mathbf{1}\{r \text{ is in decode at }t\}.
\]
These are trace-visible because prefill/decode phase windows are observed.

\paragraph{Token pressures (units: tokens/step).}
We define piecewise-constant \emph{pressures} that quantify how many tokens would
be attributed to a representative step executed at time $t$:
\begin{align}
p^{\mathrm{dec}}(t)
&:= \sum_r d_r(t),
&&\text{(decode: 1 token per active decode request per step)}
\label{eq:pressure_decode_featurized}
\\
p^{\mathrm{pf}}_{\mathrm{full}}(t)
&:= C\sum_r a_r(t).
&&\text{(prefill: full-chunk assumption)}
\label{eq:pressure_prefill_full_featurized}
\end{align}
The baseline starts from $p^{\mathrm{pf}}_{\mathrm{full}}(t)$ and subtracts a
trace-only correction for partial chunks.

\paragraph{Prefill partial-chunk correction signal (units: tokens/sec).}
We use a trace-only \emph{uniform redistribution} correction that preserves
missing token mass exactly in time integrals.
Let $[t^{\mathrm{pf}}_{r,s},t^{\mathrm{pf}}_{r,e})$ denote request $r$'s observed
prefill interval (from its prefill phase instance). Define
\begin{equation}
c_r(t)
=
\frac{\mu_r}{t^{\mathrm{pf}}_{r,e}-t^{\mathrm{pf}}_{r,s}}
\,
\mathbf{1}\{t\in[t^{\mathrm{pf}}_{r,s},t^{\mathrm{pf}}_{r,e})\},
\qquad
c(t)=\sum_r c_r(t),
\label{eq:prefill_correction_uniform_featurized}
\end{equation}
so $\int c_r(t)\,dt=\mu_r$ and $\int c(t)\,dt=\sum_r \mu_r$.

\paragraph{Phase-local step density proxy (units: steps/sec).}
For each phase instance $i$ with inferred step count $N_i$ and observed duration
$\ell_i=t_{i,e}-t_{i,s}$, define
\begin{equation}
\widehat\lambda_i := \frac{N_i}{\ell_i}.
\label{eq:lambda_hat_featurized}
\end{equation}
This proxy is used \emph{only} to convert time-integrated pressures
(tokens/step $\times$ sec) into token totals (tokens) for regression features.

\paragraph{Integrated exposures (units: tokens).}
For each phase instance $i$, define its baseline token exposures
\begin{align}
A^{\mathrm{pf}}_i
&:=
\widehat\lambda_i
\int_{t_{i,s}}^{t_{i,e}}
p^{\mathrm{pf}}_{\mathrm{full}}(t)\,dt
\;-\;
\int_{t_{i,s}}^{t_{i,e}}
c(t)\,dt,
\label{eq:Apf_def_featurized}
\\
A^{\mathrm{dec}}_i
&:=
\widehat\lambda_i
\int_{t_{i,s}}^{t_{i,e}}
p^{\mathrm{dec}}(t)\,dt.
\label{eq:Adec_def_featurized}
\end{align}
The subtraction in Eq.~\eqref{eq:Apf_def_featurized} corrects the overcount from
treating all prefill steps as full chunks: time-integrating $c(t)$ yields the
exact missing token mass attributed (uniformly) to the phase window, without
introducing any step-boundary assumptions.

\paragraph{Implementation (grid-exact integrals).}
All integrals above are computed exactly on the trace-induced grid:
for any piecewise-constant signal $s(t)$,
\[
\int_{t_{i,s}}^{t_{i,e}} s(t)\,dt
=
\sum_{j=1}^J s_j \cdot \bigl|\,[t_{i,s},t_{i,e})\cap[g_j,g_{j+1})\,\bigr|,
\]
where $s_j$ is the constant value of $s(t)$ on cell $[g_j,g_{j+1})$.

% -------------------------
\subsubsection{Featurized predictors and global NNLS for $B$ (trace-only)}
\label{subsubsec:nnls_B_trace_only}

\paragraph{Phase-level predictor in environment $e$.}
For phase instance $i\in\mathcal{I}_e$, the baseline predicts its duration as
\begin{equation}
\widehat \ell_i(B)
=
N_i\,\beta_{e,0}
+
A_i^{\mathrm{pf}}\,\beta_{e,1}
+
A_i^{\mathrm{dec}}\,\beta_{e,2},
\qquad
\beta_e = x_e^\top B.
\label{eq:featurized_baseline_phase_predictor}
\end{equation}
This preserves the semantics of the step model:
$N_i$ counts steps, while $(A_i^{\mathrm{pf}},A_i^{\mathrm{dec}})$ approximate
step-aggregated token totals experienced during the phase.

\paragraph{Linearization in $\mathrm{vec}(B)$.}
Let $b_0,b_1,b_2\in\mathbb{R}^{d_x}_{\ge 0}$ denote the columns of $B$.
Then Eq.~\eqref{eq:featurized_baseline_phase_predictor} becomes
\[
\widehat \ell_i(B)
=
N_i(x_e^\top b_0)
+
A_i^{\mathrm{pf}}(x_e^\top b_1)
+
A_i^{\mathrm{dec}}(x_e^\top b_2),
\]
which is linear in the entries of $B$.
Define the per-instance row (dimension $1\times 3d_x$)
\begin{equation}
\phi_i
:=
\big[
N_i x_e^\top,\;
A_i^{\mathrm{pf}} x_e^\top,\;
A_i^{\mathrm{dec}} x_e^\top
\big],
\label{eq:phi_row_def}
\end{equation}
and let $\theta := \mathrm{vec}(B)$ stack the columns of $B$.
Then $\widehat \ell_i(B)=\phi_i \theta$.

\paragraph{Global trace NNLS for $B$.}
Stacking all phase instances across environments, we solve:
\begin{equation}
\widehat B
=
\arg\min_{B\in\mathbb{R}^{d_x\times 3}_{\ge 0}}
\sum_{e\in\mathcal{E}}
\sum_{i\in\mathcal{I}_e}
\left(
\widehat \ell_i(B) - \ell_i
\right)^2.
\label{eq:global_nnls_B}
\end{equation}
This is a convex NNLS problem in $\theta=\mathrm{vec}(B)$.

\paragraph{Interpretation.}
Because $\ell_i$ includes admission delay, batching delay, contention, and
execution within the server, $\widehat B$ is learned directly from
\emph{server-visible} behavior using traces only, without requiring a separate
queueing model.

% -------------------------
\subsubsection{Learning $A$ from client residuals (decoupled from execution)}
\label{subsubsec:nnls_A_client}

\paragraph{Trace-visible server latency per request.}
For a request $r$ in environment $e$, traces provide its prefill phase duration
$\ell^{\mathrm{pf}}_r$ and (if applicable) decode phase duration
$\ell^{\mathrm{dec}}_r$. Define the trace-visible server-side latency
\begin{equation}
S^{\mathrm{trace}}_{r,e}
:=
\ell^{\mathrm{pf}}_r + \ell^{\mathrm{dec}}_r,
\label{eq:server_trace_latency_def}
\end{equation}
where $\ell^{\mathrm{dec}}_r:=0$ for prefill-only requests.

\paragraph{Client residual.}
For requests where we observe end-to-end client latency $y_r$, define
\begin{equation}
R_{r,e} := y_r - S^{\mathrm{trace}}_{r,e}.
\label{eq:client_residual_def}
\end{equation}
By construction, $R_{r,e}$ isolates client-visible effects not captured by
server traces (networking, serialization, streaming).

\paragraph{Featurized client model and NNLS for $A$.}
Substituting $\alpha_e=z_e^\top A$ into Eq.~\eqref{eq:client_overhead_model}
yields
\[
R_{r,e}
\approx
(z_e^\top a_0)
+
\ell_r^{\mathrm{in}}(z_e^\top a_1)
+
\ell_r^{\mathrm{out}}(z_e^\top a_2),
\]
where $a_0,a_1,a_2$ are columns of $A$.
Define the per-request row
\[
\psi_r
:=
\big[
1\cdot z_e^\top,\;
\ell_r^{\mathrm{in}} z_e^\top,\;
\ell_r^{\mathrm{out}} z_e^\top
\big],
\]
and stack across all observed client requests to solve the global NNLS:
\begin{equation}
\widehat A
=
\arg\min_{A\in\mathbb{R}^{d_z\times 3}_{\ge 0}}
\sum_{(r,e)\in\mathcal{D}_{\mathrm{client}}}
\left(
\psi_r\,\mathrm{vec}(A) - R_{r,e}
\right)^2.
\label{eq:global_nnls_A}
\end{equation}

\paragraph{Separation of concerns.}
The estimation of $B$ uses \emph{only} trace instances and is unaffected by
client networking effects. The estimation of $A$ uses \emph{only} client
residuals after removing trace-visible server latency. Thus, execution and
client overhead learning are cleanly decoupled.

% -------------------------
\subsubsection{Algorithm summary (featurized baseline)}
\label{subsubsec:featurized_baseline_algorithm_summary}

Given environment features $(x_e,z_e)$ and data (traces and client latencies),
the baseline proceeds as:
\begin{enumerate}
  \item \textbf{From traces:} infer step counts $N_i$ and construct
  pressures $p^{\mathrm{pf}}_{\mathrm{full}}(t)$, $p^{\mathrm{dec}}(t)$ on the
  trace-induced grid; compute uniform correction $c(t)$; compute exposures
  $(A_i^{\mathrm{pf}},A_i^{\mathrm{dec}})$ for each phase instance; solve the
  global NNLS \eqref{eq:global_nnls_B} to obtain $\widehat B$.
  \item \textbf{From client data:} compute trace-visible server latency
  $S^{\mathrm{trace}}_{r,e}$ and residuals $R_{r,e}$; solve NNLS
  \eqref{eq:global_nnls_A} to obtain $\widehat A$.
\end{enumerate}

The resulting featurized coefficients for any environment $e$ are:
\[
\widehat\beta_e = x_e^\top \widehat B,
\qquad
\widehat\alpha_e = z_e^\top \widehat A,
\]
which can be used by BLIS to parameterize simulation in a portable manner.

\paragraph{Transfer and one-shot recalibration.}
Given learned maps $(\widehat B,\widehat A)$, a new environment $e^\star$ requires
only its feature vectors $(x_{e^\star},z_{e^\star})$ to produce coefficients:
\[
\beta^{\mathrm{feat}}_{e^\star} := x_{e^\star}^\top \widehat B,
\qquad
\alpha^{\mathrm{feat}}_{e^\star} := z_{e^\star}^\top \widehat A.
\]
This supports \emph{zero-shot transfer} across LLMs, GPU/interconnect backends,
and engine configurations.

When traces are available for $e^\star$, BLIS performs \emph{one-shot
recalibration} by solving a trace NNLS objective regularized toward the featurized
prediction:
\begin{equation}
\widehat \beta_{e^\star}
=
\arg\min_{\beta\in\mathbb{R}^3_{\ge 0}}
\sum_{i\in\mathcal{I}_{e^\star}}
\left(
\beta_0 N_i
+
\beta_1 A_i^{\mathrm{pf}}
+
\beta_2 A_i^{\mathrm{dec}}
-
\ell_i
\right)^2
\;+\;
\lambda_{\mathrm{recal}}
\left\|
\beta-\beta^{\mathrm{feat}}_{e^\star}
\right\|_2^2.
\label{eq:recalibration_to_featurized_prior}
\end{equation}
Here $(N_i, A_i^{\mathrm{pf}}, A_i^{\mathrm{dec}})$
are constructed from traces in $e$ exactly as in the baseline estimator.
This objective remains convex and retains the same coefficient semantics as the
baseline trace estimator.
The regularizer implements partial pooling: it allows environment-specific
deviations when supported by trace evidence, while preventing unstable fits when
only a small trace window is available.

\paragraph{One-shot recalibration of client-side overheads.}
For a new environment $e^\star$ with paired client and trace data, we form
trace-decomposed residuals
\[
R_r := y_r - S_r^{\mathrm{trace}},
\qquad r\in\mathcal{J}_{e^\star},
\]
where $y_r$ is end-to-end latency observed at the client and
$S_r^{\mathrm{trace}}$ is the server-side latency observable from the trace
(admission, batching, and execution).

Given the featurized prediction
\[
\alpha^{\mathrm{feat}}_{e^\star} := z_{e^\star}^\top \widehat A,
\]
BLIS recalibrates $\alpha_{e^\star}$ by solving a regularized nonnegative least
squares problem:
\begin{equation}
\widehat \alpha_{e^\star}
=
\arg\min_{\alpha\in\mathbb{R}^3_{\ge 0}}
\sum_{r\in\mathcal{J}_{e^\star}}
\left(
\alpha_0
+
\alpha_1 \ell_r^{\mathrm{in}}
+
\alpha_2 \ell_r^{\mathrm{out}}
-
R_r
\right)^2
\;+\;
\lambda_{\mathrm{recal}}^{\alpha}
\left\|
\alpha-\alpha^{\mathrm{feat}}_{e^\star}
\right\|_2^2.
\label{eq:alpha_recalibration_to_featurized_prior}
\end{equation}
This objective is convex and decoupled from server scheduling dynamics by
construction, since all server-side queueing and interference are absorbed into
$S_r^{\mathrm{trace}}$ and removed before fitting $\alpha$.


% ------------------------------------------------------------
\subsection{Identifiability and complexity}

Identifiability of $B$ and $A$ requires diversity in environment features.
If feature vectors are nearly collinear, the global design matrices become
ill-conditioned.
In practice, variation in model size, parallelism, GPU generation, and
interconnect properties is sufficient to yield well-conditioned estimates.

Feature construction is linear in the number of trace instances and client
requests. The NNLS problems in this estimator are convex with 
$3d_x$ and $3d_z$ unknowns, respectively, and can be solved efficiently 
using standard QP/NNLS solvers.

\paragraph{Summary.}
Featurized coefficients allow BLIS to learn portable execution and client-overhead
models from heterogeneous data, while preserving the simplicity, convexity, and
interpretability of the trace-decomposed baseline.

% ============================================================
\section{Continuous Kernelized EM with Boundary-Aware Step Densities}
\label{sec:em_continuous_kernelized}

There are three key ingredients in this algorithm.
(i) continuous boundary-aware phase membership kernels,
(ii) end-localized kernels for the final prefill step,
and (iii) optional low-pass smoothing of trace-derived pressures.
Together, these yield a numerically stable, trace-only EM algorithm with a
minimal number of tunable parameters.

% ------------------------------------------------------------
\paragraph{Step-rate model (iteration-indexed).}
At EM iteration $m$, the E-step allocates step mass using the step-rate induced
by the \emph{previous} iterate $(\beta^{(m-1)},\tilde p^{\mathrm{pf},(m-1)})$.
Define the local step-time proxy and step-rate (intensity)
\begin{equation}
\Delta^{(m)}(t)
=
\beta^{(m-1)}_0
+
\beta^{(m-1)}_1\,\tilde p^{\mathrm{pf},(m-1)}(t)
+
\beta^{(m-1)}_2\,p^{\mathrm{dec}}(t),
\qquad
\lambda^{(m)}(t)=\frac{1}{\Delta^{(m)}(t)},
\label{eq:continuous_step_rate}
\end{equation}
where $\lambda^{(m)}(t)$ has units of steps/sec and is interpreted as the
instantaneous density of the latent global busy-loop steps.

Although $\Delta^{(m)}(t)$ and $\lambda^{(m)}(t)$ are indexed by $m$, they depend
only on quantities from iteration $m-1$; the superscript $(m)$ indicates that
they are computed \emph{during} iteration $m$ and used in its E-step.

\paragraph{Well-definedness.}
For each iteration $m$, we assume $\Delta^{(m)}(t)>0$ for all $t$ in the trace
support (e.g., by enforcing $\beta^{(m-1)}_0\ge \beta_{0,\min}>0$ or, equivalently,
clamping $\Delta^{(m)}(t)\leftarrow \max\{\Delta^{(m)}(t),\Delta_{\min}\}$ for a
small $\Delta_{\min}>0$). This ensures $\lambda^{(m)}(t)$ is finite and the
normalizations below are well-defined.

% ------------------------------------------------------------
\paragraph{Boundary-aware phase membership kernels.}

Let phase instance $i$ have trace-visible start and end times
$(t_{i,s},t_{i,e})$.
We assume a positive-delay instrumentation model
$t^{\mathrm{obs}} = t^\star + \varepsilon$ with $\varepsilon \ge 0$.
Under this model, uncertainty about phase membership arises only near the
left side of observed boundaries.

We define a continuous membership function
$w_i(t)\in[0,1]$ that represents the plausibility that wall-clock time $t$
belongs to the true execution window of phase $i$.
Specifically,
\begin{equation}
w_i(t)
=
F_h(t - t_{i,s})\,
\bigl(1 - F_h(t - t_{i,e})\bigr),
\label{eq:phase_membership_kernel}
\end{equation}
where $F_h$ is a one-sided cumulative kernel with bandwidth $h>0$, satisfying:
\[
F_h(x)=0 \;\; \text{for } x\le 0,
\qquad
F_h(x)\uparrow 1 \;\; \text{as } x\gg h.
\]
A concrete choice is $F_h(x)=1-e^{-x/h}$ for $x\ge 0$.

By construction, $w_i(t)=1$ throughout the interior of the observed phase
window and deviates from unity only within an $O(h)$ neighborhood of the
boundaries.
No membership mass is assigned beyond the observed phase end.

% ------------------------------------------------------------
\paragraph{Optional pressure smoothing.}

Trace-derived pressures $p^{\mathrm{pf}}_{\mathrm{full}}(t)$ and
$p^{\mathrm{dec}}(t)$ may exhibit high-frequency artifacts due to timestamp
noise.
To suppress such artifacts, we optionally apply a symmetric low-pass kernel
$G_h$ with the same bandwidth $h$:
\[
p(t) \;\leftarrow\; (G_h * p)(t).
\]
This smoothing affects only numerical stability and does not alter the
statistical structure of the EM formulation.

% ------------------------------------------------------------
\paragraph{Latent step densities and E-step.}

For each phase instance $i$ with trace-inferred step count $N_i$, we model
the $N_i$ execution steps as latent events distributed in continuous time
according to the step-rate model. 
Conditioned on the previous iterate $(\beta^{(m-1)},\tilde p^{\mathrm{pf},(m-1)})$
(equivalently, on the step-rate $\lambda^{(m)}$ computed in
Eq.~\eqref{eq:continuous_step_rate}), the posterior expected step-density for phase
$i$ is
\begin{equation}
q_i^{(m)}(t)
=
\frac{w_i(t)\,\lambda^{(m)}(t)}
{\int w_i(u)\,\lambda^{(m)}(u)\,du}.
\label{eq:phase_step_density}
\end{equation}
which integrates to one over $\mathbb{R}$.
The expected step mass in an infinitesimal interval is then
\[
\mathbb{E}[N_i(dt)]
=
N_i\,q_i^{(m)}(t)\,dt.
\]

This continuous density is the direct analogue of fractional step allocation
in the kernelized grid EM.

% ------------------------------------------------------------
\paragraph{Final prefill step localization.}

For a prefill request $r$, exactly one of its $N_r$ steps corresponds to the
final partial chunk.
Let $\mu_r$ denote the missing token mass of that step.

We define a boundary-local kernel $\varphi_r(t)$ concentrated immediately
before the observed prefill end time $t_{r,e}$:
\begin{equation}
\varphi_r(t)
=
\frac{K_h(t_{r,e}-t)\,\mathbf{1}\{t\le t_{r,e}\}}
{\int K_h(t_{r,e}-u)\,\mathbf{1}\{u\le t_{r,e}\}\,du},
\label{eq:final_step_kernel}
\end{equation}
where $K_h$ is a one-sided kernel on $\mathbb{R}_{\ge 0}$ (e.g., exponential or
truncated Gaussian).
This kernel integrates to one and represents the posterior density of the
final step’s location under positive delay.

The corresponding final-step density (using the step-rate $\lambda^{(m)}$
computed in Eq.~\eqref{eq:continuous_step_rate}) is
\begin{equation}
q^{(m)}_{r,\mathrm{final}}(t)
=
\frac{\varphi_r(t)\,\lambda^{(m)}(t)}
{\int \varphi_r(u)\,\lambda^{(m)}(u)\,du}.
\label{eq:final_step_density}
\end{equation}

The remaining $(N_r-1)$ bulk prefill steps are distributed according to
$q_i^{(m)}(t)$.

% ------------------------------------------------------------
\paragraph{Exact partial-chunk correction.}

The expected rate of final-step events per unit time is $q^{(m)}_{r,\mathrm{final}}(t)$,
which integrates to one.
To convert missing-token mass into a per-step correction, we normalize by the
global step-rate:
\begin{equation}
\delta^{(m)}(t)
=
\frac{\sum_r \mu_r\, q^{(m)}_{r,\mathrm{final}}(t)}
{\lambda^{(m)}(t)},
\qquad
\text{units: tokens/step}.
\label{eq:continuous_partial_chunk}
\end{equation}

The effective prefill pressure is then
\begin{equation}
\tilde p^{\mathrm{pf},(m)}(t)
=
p^{\mathrm{pf}}_{\mathrm{full}}(t) - \delta^{(m)}(t).
\end{equation}
By construction, each request contributes exactly one partial chunk in
expectation, independent of timestamp noise or numerical discretization.

% ------------------------------------------------------------
\paragraph{M-step.}

Given the expected step densities, we compute step-averaged pressures
\begin{align}
\bar p^{\mathrm{pf},(m)}_i
&=
\int q_i^{(m)}(t)\,\tilde p^{\mathrm{pf},(m)}(t)\,dt,
\\
\bar p^{\mathrm{dec},(m)}_i
&=
\int q_i^{(m)}(t)\,p^{\mathrm{dec}}(t)\,dt.
\end{align}

Coefficients are updated by solving the same non-negative least squares problem
as in the idealized EM:
\[
\beta^{(m)}
=
\arg\min_{\beta\ge 0}
\sum_{i\in\mathcal{I}}
\left(
N_i\beta_0
+
[N_i\bar p^{\mathrm{pf},(m)}_i]\beta_1
+
[N_i\bar p^{\mathrm{dec},(m)}_i]\beta_2
-
\ell_i
\right)^2.
\]

% ------------------------------------------------------------
\paragraph{Initialization and convergence.}

The algorithm is initialized using the baseline trace-only NNLS estimator,
yielding $\beta^{(0)}$, and we set the initial effective prefill pressure to the
full-chunk pressure:
\[
\tilde p^{\mathrm{pf},(0)}(t)=p^{\mathrm{pf}}_{\mathrm{full}}(t).
\]
Iteration $m=1$ then computes $\lambda^{(1)}(t)$ from
$(\beta^{(0)},\tilde p^{\mathrm{pf},(0)})$ and performs the first step-aware
allocation. Subsequent iterations incorporate exact partial-chunk correction
through $\delta^{(m)}(t)$.

Each iteration monotonically decreases the observed-data squared-error
objective.
The continuous formulation reduces to the idealized EM under hard membership
kernels and to the kernelized grid EM under Riemann discretization.

\paragraph{Summary.}
This continuous kernelized EM formulation unifies the baseline, MM-style, and
grid-based EM approaches within a single statistically principled framework.
Boundary uncertainty is handled explicitly, partial-chunk correction remains
exact in expectation, and robustness is achieved without sacrificing analytical
clarity or introducing per-phase timing parameters.

% ============================================================
\subsection{Featurized Continuous EM for Execution Coefficients}
\label{sec:featurized_continuous_em}

We now extend the continuous kernelized EM formulation to the featurized setting
introduced in Section~\ref{sec:featurized_coefficients}.
This extension allows BLIS to learn a \emph{single global execution map}
that transfers across environments, while retaining the step-density–aware
refinement provided by the EM procedure.

As in the baseline case, featurization applies \emph{only} to server-side
execution coefficients.
Client-side coefficients $\alpha$ and the matrix $A$ are estimated independently
from trace-decomposed residuals and are unaffected by the EM procedure.

% ------------------------------------------------------------
\paragraph{Environment-indexed traces.}

Let $e\in\mathcal{E}$ index deployment environments, each described by a
nonnegative feature vector $x_e\in\mathbb{R}^{d_x}_{\ge 0}$.
For environment $e$, trace data provides a set of phase instances
$i\in\mathcal{I}_e$, each with:
trace-inferred step count $N_i$,
phase duration $\ell_i$,
and EM-derived step-averaged pressures
$\bar p^{\mathrm{pf},(m)}_i$ and $\bar p^{\mathrm{dec},(m)}_i$ at iteration $m$.

% ------------------------------------------------------------
\paragraph{Featurized execution model.}

We parameterize environment-specific execution coefficients as
\begin{equation}
\beta_e = x_e^\top B,
\qquad
B \in \mathbb{R}^{d_x \times 3}_{\ge 0},
\label{eq:featurized_em_beta}
\end{equation}
where columns of $B$ correspond to $(\beta_0,\beta_1,\beta_2)$.
This guarantees $\beta_e\ge 0$ for all environments and preserves the physical
interpretation of coefficients.

Substituting $\beta_e$ into the step-level model yields the EM predictor
\begin{equation}
\widehat \ell_i(B)
=
N_i (x_e^\top b_0)
+
N_i \bar p^{\mathrm{pf},(m)}_i (x_e^\top b_1)
+
N_i \bar p^{\mathrm{dec},(m)}_i (x_e^\top b_2),
\label{eq:featurized_em_predictor}
\end{equation}
which is linear in the unknown entries of $B$.

% ------------------------------------------------------------
\paragraph{E-step (unchanged).}

The E-step is identical to that of Section~\ref{sec:em_continuous_kernelized}.
Given the execution coefficients from the previous iteration,
$\beta^{(m-1)}_e = x_e^\top B^{(m-1)}$, we compute:
\begin{itemize}
\item the environment-specific step-rate
\(
\lambda^{(m)}_e(t)
\),
\item phase membership densities $q^{(m)}_i(t)$,
\item final-step densities $q^{(m)}_{r,\mathrm{final}}(t)$,
\item effective prefill pressure $\tilde p^{\mathrm{pf},(m)}_e(t)$,
\end{itemize}
exactly as in the unfeaturized continuous EM.

No additional assumptions or approximations are introduced by featurization.

% ------------------------------------------------------------
\paragraph{M-step (global, featurized).}

Given step-averaged pressures from the E-step, we update the global execution map
by solving a single nonnegative least squares problem across all environments:
\begin{equation}
B^{(m)}
=
\arg\min_{B\ge 0}
\sum_{e\in\mathcal{E}}
\sum_{i\in\mathcal{I}_e}
\left(
\widehat \ell_i(B) - \ell_i
\right)^2.
\label{eq:featurized_em_mstep}
\end{equation}

This objective is convex in $\mathrm{vec}(B)$ and can be solved using standard
NNLS or QP solvers.
The resulting update produces new environment-specific coefficients
$\beta^{(m)}_e=x_e^\top B^{(m)}$, which are then used in the next E-step.

% ------------------------------------------------------------
\paragraph{Initialization.}

The featurized EM algorithm is initialized using the featurized baseline
estimator from Section~\ref{sec:featurized_coefficients}.
Specifically, we set
\[
B^{(0)} := \widehat B_{\mathrm{baseline}},
\qquad
\beta^{(0)}_e = x_e^\top B^{(0)},
\qquad
\tilde p^{\mathrm{pf},(0)}_e(t) = p^{\mathrm{pf}}_{\mathrm{full}}(t).
\]

This ensures that the first EM iteration corresponds to a step-density–aware
reweighting of the baseline solution.

% ------------------------------------------------------------
\paragraph{Regularization and partial pooling (optional).}

To improve numerical stability when feature vectors are correlated or when some
environments have limited trace coverage, we optionally add a ridge penalty:
\begin{equation}
\lambda_B \| B - B^{(0)} \|_F^2
\end{equation}
to Eq.~\eqref{eq:featurized_em_mstep}.
This induces partial pooling across environments and prevents overfitting
without altering the E-step or coefficient semantics.

% ------------------------------------------------------------
\paragraph{Interpretation and scope.}

The featurized continuous EM procedure learns a portable, step-level execution
model that:
(i) captures fine-grained step-density effects from traces,
(ii) transfers across LLM, GPU, and configuration regimes,
and (iii) remains trace-only and convex in its M-step.

Client-side latency coefficients $\alpha$ and the feature map $A$ are unchanged
by this procedure and are estimated independently using trace-decomposed
residuals, as described in Section~\ref{sec:featurized_coefficients}.

\paragraph{Summary.}
Featurized continuous EM combines the statistical efficiency of shared execution
maps with the fidelity of step-aware latent allocation, yielding a unified,
trace-only framework for learning transferable inference execution models.


\bibliographystyle{plain}
\bibliography{blis}

\end{document}
