\documentclass[11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{times}
\usepackage{graphicx}
\usepackage{amsmath, amssymb}
\usepackage{bm}
\usepackage[most]{tcolorbox}
\usepackage{tabularx}
\usepackage{pifont} % optional for symbols
% --- Flowcharts (TikZ) ---
\usepackage{tikz}
\usetikzlibrary{arrows.meta,positioning,shapes.geometric,fit,calc}
\usepackage{xcolor}
% Algorithms (compact)
\usepackage{algorithm}
\usepackage{algpseudocode}

% Optional: slightly tighter algorithm typography
\algrenewcommand\algorithmicrequire{\textbf{Input:}}
\algrenewcommand\algorithmicensure{\textbf{Output:}}
\algrenewcommand\algorithmiccomment[1]{\hfill{\footnotesize$\triangleright$~#1}}

\usepackage{hyperref}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{booktabs,longtable,array,makecell}
\setlength{\LTleft}{0pt}   % no left indent for longtable
\setlength{\LTright}{0pt}  % no right indent for longtable
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}p{#1}}


% --- Theorems, lemmas, claims, proofs ---
\usepackage{amsthm}

% Number theorems within sections; share a counter across theorem-like envs.
\theoremstyle{plain} % bold headings, italic body
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{claim}[theorem]{Claim}

\theoremstyle{definition} % bold heading, upright body
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark} % italic heading, upright body
\newtheorem{remark}[theorem]{Remark}
\newtheorem{note}[theorem]{Note}

% Optional: unnumbered versions
\newtheorem*{theorem*}{Theorem}
\newtheorem*{lemma*}{Lemma}
\newtheorem*{claim*}{Claim}

% Proof environment is provided by amsthm:
% \begin{proof} ... \end{proof}
% Customize QED symbol if you like:
\renewcommand{\qedsymbol}{$\blacksquare$}

\newcommand{\code}[1]{\texttt{#1}}

\definecolor{prefillcolor}{RGB}{173,216,230}  % Light blue
\definecolor{decodecolor}{RGB}{144,238,144}   % Light green
\definecolor{gridcolor}{RGB}{200,200,200}     % Light gray
\definecolor{highlightcolor}{RGB}{255,182,193} % Light pink for integration region


% (Optional) If you want equations numbered by section (since you already use amsmath):
\numberwithin{equation}{section}



\title{BLIS: Blackbox Inference Performance Estimator}
\author{AI Platform Optimization Team}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This paper introduces BLIS, a system for blackbox inference performance estimation 
designed to model inference request flows and latency.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

\section{Related Work}
\label{sec:related-work}

\section{Design overview}
\label{sec:design-overview}

BLIS is primarily geared towards vLLM. We will describe extensions for other
inference platforms like sglang in Section \ref{sec:future-work}.

\subsection{vLLM overview}
\label{subsec:vllm-overview}
We will focus on handling of an inference request by vLLM. vLLM
has two main components: the API server and the engine core; these operate as separate threads and
communicate through a message queue.

\subsubsection{API server thread}
The API server is implemented using FastAPI. Its role is to handle client-facing tasks without blocking on actual model execution.
The API thread is responsible for:
\begin{enumerate}
    \item Receiving and tokenizing incoming requests from clients.
    \item Enqueuing tokenized requests into the message queue.
    \item Streaming back partial responses as they become available from the engine.
    \item Detokenizing completed sequences and packaging them into response
          objects.
    \item Emitting the final response back to the client.
\end{enumerate}

\subsubsection{Engine core thread}
The engine core runs the central \emph{inference loop}. This loop is driven by a
scheduler that repeatedly:
\begin{enumerate}
    \item Collects pending requests from the queue, forming a dynamic batch.
    \item Determines whether a request is in a \emph{prefill} (first token)
          or \emph{decode} (subsequent tokens) phase.
    \item Executes the forward pass on the GPU for that batch.
    \item Updates KV cache blocks, manages allocation and eviction, and applies
          prefix caching optimizations when possible.
    \item Returns generated tokens back to the API layer, either for streaming
          or final response.
\end{enumerate}
We will refer to this engine loop as the \emph{busy loop} since it continuously
steps through request-batches at a fine-grained timescale.

\subsection{Design of BLIS}
\label{subsec:blis-design}

\section{Latency model}
\label{sec:latency-model}

\subsection{Request types}
\label{subsubsec:request-types}

Requests can be of two types: \textit{prefill-only} or \textit{decode}. 
Prefill-only requests have their maximum output length set to $1$, and \textit{decode}
requests have this set to a number greater than $1$.

\subsection{Request life-cycle}
\label{subsubsec:request-life-cycle}

\begin{figure}[!ht]
  \centering
  \includegraphics[height=0.8\textwidth]{figs/reqlifecycle.png}
  \caption{Request Lifecycle}
  \label{fig:req-lifecycle}
\end{figure}

The life of an inference request in the system passes through multiple stages 
as  illustrated in Figure \ref{fig:req-lifecycle} and described below.

\paragraph*{1. Ingress.}
The client emits an inference request $i$ to the server. This contributes 
a network latency that depends linearly on the number of input tokens 
$\ell_i^{\text{in}}$.

\paragraph*{2. Pre-process.}
Upon arrival, the API server tokenizes the prompt, performs light
validation, and enqueues the request into the message queue. This contributes 
a latency that depends linearly on $\ell_i^{\text{in}}$.

\paragraph*{3. Scheduling.} The request waits in the queue until there is 
sufficient GPU capacity. During this waiting period, other requests get to participate
in the busy loop iterations.

\paragraph*{4. Chunked prefill.} The uncached portion of this request is 
prefilled one chunk at a time. The latency of this stage equals the sum of the 
duration of the busy loop iterations in which this request prefills.

\paragraph*{5. Decode.} The latency of this stage equals the sum of the duration
of the busy loop iterations in which this request is in its decode phase. 
The first output token is generated during the prefill phase, and subsequent 
tokens are generated in the decode iterations.

\paragraph*{6. Post-process.} Once all output tokens are produced, 
the API server detokenizes them and
packages the final response. The latency of this stage depends linearly on the 
number of output tokens $\ell_i^{\text{out}}$.

\paragraph*{7. Egress.}
The server emits response $i$ to the
client. This contributes a network latency that depends linearly 
on the number of output tokens $\ell_i^{\text{out}}$.


\section{Trace-Only Estimation of Step-Level Execution Coefficients}
\label{sec:step_beta_estimation}

This section presents a trace-only methodology for estimating
\emph{step-level} execution coefficients of a vLLM-style inference engine.
We proceed in three stages.
First, we introduce a simple but principled baseline estimator that fits
phase durations using time-integrated workload signals.
Second, we refine this estimator using an iterative, step-density--reweighted
procedure inspired by majorization--minimization (MM).
Finally, we discuss robustness, workload design for calibration, and practical
limitations.

Throughout, we emphasize trace-only operation: no step boundaries, 
per-step timings, or engine instrumentation are assumed.

\paragraph{Notation and units (summary).}
Table~\ref{tab:notation_summary} summarizes the principal symbols used in
this section and their units.

\begin{table}[h]
\centering
\small
\begin{tabular}{lll}
\toprule
\textbf{Symbol} & \textbf{Units} & \textbf{Meaning} \\
\midrule
$k$ & step index & busy-loop iteration (latent execution step) \\
$r$ & request index & inference request; each has exactly one prefill phase and one decode phase \\
$i$ & phase index & a specific phase (prefill or decode) of a specific request \\[0.5ex]

$t_{i,s},\,t_{i,e}$ & seconds & start and end times of phase $i$ \\
$\ell_i$ & seconds & observed duration of phase $i$ \\
$N_i$ & steps & step count for phase $i$ (fixed using traces during estimation) \\
$\tau_i$ & categorical (unitless) & phase type of instance $i$ (\textsf{prefill} or \textsf{decode}) \\
$T^{\mathrm{pf}}_k,\,T^{\mathrm{dec}}_k$ & tokens & prefill/decode token totals processed in step $k$ \\

$C$ & tokens/step & prefill chunk size \\
$P_r$ & tokens & total prefill tokens for request $r$ \\
$D_r$ & tokens & total decode tokens for request $r$ \\
$\mu_r$ & tokens & missing token mass of final prefill chunk \\[0.5ex]

$p^{\mathrm{pf}}(t)$ & tokens/step & instantaneous prefill pressure \\
$p^{\mathrm{dec}}(t)$ & tokens/step & instantaneous decode pressure \\
$\tilde p^{\mathrm{pf}}(t)$ & tokens/step & instantaneous effective prefill pressure (baseline) \\
$\tilde p^{\mathrm{pf}}(t;\beta)$ & tokens/step & instantaneous effective prefill pressure (iterative) \\[0.5ex]

$\widehat{\lambda}_i$ & steps/sec & phase-local step density proxy (baseline) \\
$\lambda(t;\beta)$ & steps/sec & instantaneous step density (iterative) \\[0.5ex]

$q_i(t;\beta)$ & 1/sec & normalized step-time attribution within phase $i$ \\
$q_r(t;\beta)$ & 1/sec & normalized step-time attribution for request $r$'s prefill \\[0.5ex]

$A_i^{\mathrm{pf}}$ & tokens & baseline prefill exposure \\
$A_i^{\mathrm{dec}}$ & tokens & baseline decode exposure \\
$\bar p_i^{\mathrm{pf}}(\beta)$ & tokens/step & step-averaged prefill pressure (yields step-weighted exposure via $N_i\bar p$) \\
$\bar p_i^{\mathrm{dec}}(\beta)$ & tokens/step & step-averaged decode pressure (yields step-weighted exposure via $N_i\bar p$) \\[0.5ex]

$\beta_0$ & sec/step & fixed per-step overhead \\
$\beta_1,\beta_2$ & sec/token & per-token execution costs \\
\bottomrule
\end{tabular}
\caption{Notation and units used in trace-only step-level coefficient estimation.}
\label{tab:notation_summary}
\end{table}


% ============================================================
\subsection{Problem Setup and Step-Level Model}
\label{sec:problem_setup}

We consider a vLLM-style inference engine that advances execution through
a single logical busy-loop.
Each busy-loop iteration (a \emph{step}) executes one forward-pass cycle
that may process prefill tokens, decode tokens, or both, for a set of
concurrently active requests.
Steps are assumed to execute sequentially.

Let $k$ index busy-loop steps.
We model the duration of step $k$ as
\begin{equation}
\Delta t_k
=
\beta_0
+
\beta_1\,T^{\mathrm{pf}}_k
+
\beta_2\,T^{\mathrm{dec}}_k,
\label{eq:step_model}
\end{equation}
where:
\begin{itemize}
  \item $\beta_0 \ge 0$ is a fixed per-step overhead (seconds/step),
  \item $\beta_1,\beta_2 \ge 0$ are per-token costs (seconds/token),
  \item $T^{\mathrm{pf}}_k$ and $T^{\mathrm{dec}}_k$ are the total numbers of
        prefill and decode tokens processed in step $k$ across all requests.
\end{itemize}

This affine step-time model is the fundamental abstraction used by our
simulator. Accurate simulation therefore requires identifying the
coefficient vector $\beta=(\beta_0,\beta_1,\beta_2)$.

\paragraph{What traces provide.}
Production inference traces do not expose step boundaries or per-step execution
timings. Instead, they provide a sequence of \emph{phase instances}, indexed by
$i$, each corresponding to either the prefill phase or the decode phase of a
single request.

For each phase instance $i$, the trace provides:
\begin{itemize}
  \item the request id $r_i$
  \item the phase type $\tau_i$ (\textit{prefill} or \textit{decode}),
  \item a start time $t_{i,s}$ and end time $t_{i,e}$,
  \item an observed phase duration $\ell_i = t_{i,e} - t_{i,s}$,
  \item aggregate token counts associated with the phase, including the number
        of \emph{prefill tokens} or \emph{decode tokens}.
\end{itemize}

For a phase instance $i$ associated with request $r_i$, we denote by $P_{r_i}$
the total number of prefill tokens for that request and by $D_{r_i}$ the total
number of decode tokens for its decode phase. Since each phase 
instance uniquely identifies its request, we write $P_r$ 
(prefill tokens of request $r$) and $D_r$ (decode tokens of request $r$) 
without ambiguity. From this 
information, we derive a step count $N_i \in \mathbb{N}$.
For a decode phase, execution proceeds one token per step, and hence
$N_i$ equals the number of decode tokens.
For a prefill phase, execution is chunked: under a prefill chunk size $C$,
a standard trace-inferred choice is
$N_i = \lceil P_r / C \rceil$.\footnote{
Prefix-cached tokens do not contribute to execution steps and are therefore
excluded from $P_r$. This treatment is consistent with the execution semantics
of vLLM-style inference engines~\cite{vllm}.
}

% ============================================================
\subsection{Trace-Derived Token Pressures}
\label{sec:pressures}

\paragraph{Intuition.}
Inference executes as a sequence of discrete steps, but production traces expose
only coarse, phase-level summaries.
For each prefill or decode phase of a request, the trace records when the phase
starts and ends and how many tokens are processed, but not how those tokens are
distributed across individual steps.
The goal is not to reconstruct the hidden step schedule, but to recover
step-level execution coefficients that describe how different types of work
contribute to elapsed time when execution is aggregated over many steps.

The baseline estimator bridges this gap by compressing step-level behavior into
phase-level quantities that are consistent with the trace.
It treats each phase as a superposition of many unobserved steps and represents
the effective step-level workload of each type using token pressures.
These pressures describe, as a function of time, how much prefill and decode
work is being processed per execution step across all concurrently active
requests, without requiring access to step boundaries or per-step timings.
This intuition is illustrated in Figure \ref{fig:overlap_phases}.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=\linewidth]{figs/overlap_phases.pdf}
  \caption{\textbf{Phase overlap and inferred workload over wall-clock time.}
Each horizontal bar represents a trace-visible prefill or decode phase 
of a request, while dashed vertical lines indicate wall-clock time 
instants at which an execution step boundary may occur. 
The dashed line at time $t = 8.0$ falls at the end of a step during 
which all requests are concurrently active, so both prefill and decode 
pressures are at their maximum for that step. Although traces 
expose only phase start and end times—not step boundaries—the 
degree of phase overlap determines how much work is executed per 
step when time is aggregated. The baseline estimator leverages this 
intuition by converting overlap in wall-clock time into integrated 
token exposures that are consistent with the underlying step-level 
execution model.}
\label{fig:overlap_phases}
\end{figure}

Given these pressures and the trace-inferred number of steps per phase, the
baseline constructs simple phase-level summaries—used as regression
features—that quantify how much prefill and decode work each phase “exposes” to
the underlying step model.
Each phase duration can then be written as a linear function of these summaries
and the unknown step-level coefficients, turning coefficient recovery into an
ordinary regression problem.
Solving this regression produces a single, globally consistent set of execution
coefficients from trace-only data and provides the starting point for the
step-density–reweighted refinements introduced next.

\paragraph{Trace-only attribution (important).}
All densities introduced below (e.g., $q_i$ and $q_r$) are \emph{attribution models} that allocate
\emph{latent step exposure} over wall-clock time using only trace-visible phase boundaries.
They are not claims about true step boundaries or per-step timings; the estimator uses them only through
time integrals on the trace-induced grid.


\paragraph{Terminology: instantaneous quantities and exposures.}
We distinguish between \emph{instantaneous} quantities, which are defined as
functions of wall-clock time $t$ on the trace-induced grid (e.g., instantaneous
pressures in tokens/step and instantaneous step density in steps/sec), and
\emph{exposures}, which are obtained by integrating instantaneous quantities
over a phase window (optionally with step-density weighting) and serve as
regression features. We reserve the term \emph{rate} for quantities defined per
unit time (e.g., step density), and use \emph{pressure} for quantities defined
per step (tokens/step).

\paragraph{Instantaneous token pressures.}
Let $p^{\mathrm{pf}}(t)$ and $p^{\mathrm{dec}}(t)$ denote the 
\emph{instantaneous} prefill and decode pressures (tokens/step) 
induced by the set of requests whose trace-visible phases overlap time $t$. 
Equivalently, they are the token totals that would be attributed to a 
representative busy-loop step executed during the wall-clock instant $t$.
These signals are reconstructed deterministically from phase overlap and 
scheduling semantics on the trace-induced time grid.

\subsubsection{Prefill Chunking and Partial-Chunk Correction}
\label{sec:prefill_correction}

Decode contributes exactly one token per active request per step.
Prefill, however, proceeds in chunks of size $C$ tokens per step, except for
the final prefill step of a request, which may process fewer than $C$ tokens.
Naively treating all prefill steps as full chunks introduces systematic bias.

For a request $r$ with $P_r$ prefill tokens and $N_r$ prefill steps, the final
prefill step processes
\begin{equation}
\rho_r = P_r - C\,(N_r-1),
\qquad \rho_r \in (0,C].
\label{eq:rho_def}
\end{equation}
The missing token mass relative to a full chunk is
\[
\mu_r = C - \rho_r \in [0,C).
\]

\paragraph{Naive full-chunk pressure.}
Let $a_r(t)\in\{0,1\}$ indicate whether request $r$ is in prefill at time $t$,
as determined from trace-visible phase boundaries.
The naive full-chunk prefill pressure is
\begin{equation}
p^{\mathrm{pf}}_{\mathrm{full}}(t)
=
C \sum_r a_r(t).
\end{equation}

\paragraph{No partial-chunk correction (baseline ablation).}
The simplest baseline ignores partial chunks entirely and uses
$p^{\mathrm{pf}}_{\mathrm{full}}(t)$ as the effective prefill pressure.
Under this choice, the estimator treats every prefill step as a full chunk,
and the missing mass $\mu_r$ is not explicitly accounted for.

This option serves as a natural ablation: it is fully trace-only, requires no
additional assumptions about where the final prefill step occurs in time, and
provides a lower bound on estimator fidelity when chunking effects are mild.

\paragraph{Uniform redistribution correction.}
A more refined baseline accounts for the missing mass $\mu_r$ by redistributing
it uniformly over the prefill interval of request $r$.
Let $[t_{r,s}, t_{r,e})$ denote the trace-visible prefill interval of request $r$.
Define a correction weight
\[
w_r(t)
=
\begin{cases}
\frac{1}{t_{r,e}-t_{r,s}}, & t\in[t_{r,s},t_{r,e}),\\
0, & \text{otherwise},
\end{cases}
\]
which satisfies $\int w_r(t)\,dt = 1$.

The effective prefill pressure is then
\begin{equation}
\tilde p^{\mathrm{pf}}(t)
=
p^{\mathrm{pf}}_{\mathrm{full}}(t)
-
\sum_r \mu_r\, w_r(t).
\label{eq:uniform_prefill_pressure}
\end{equation}
This construction preserves total prefill token mass exactly while avoiding any
assumption about the precise temporal location of the final prefill step.
Uniform redistribution is therefore a conservative, smoothing correction that
depends only on trace-visible phase boundaries.

\paragraph{End-localized partial-chunk correction.}
Uniform redistribution implicitly assumes that the final prefill step is equally
likely to occur anywhere within the prefill interval.
An alternative trace-only assumption is that the final prefill step completes
near the trace-visible end of the prefill phase.

Let $t_{r,e}$ denote the end timestamp of request $r$’s prefill phase, and let
$\mathcal{I}(t_{r,e})$ denote the smallest time-grid interval (induced by trace
phase boundaries) that contains $t_{r,e}$.
The end-localized correction assigns the entire missing mass $\mu_r$ to that
interval.

Equivalently, this corresponds to choosing a correction weight $w_r(t)$ that is
supported on $\mathcal{I}(t_{r,e})$ and normalized such that
$\int w_r(t)\,dt = 1$.
The resulting effective prefill pressure again takes the form
\begin{equation}
\tilde p^{\mathrm{pf}}(t)
=
p^{\mathrm{pf}}_{\mathrm{full}}(t)
-
\sum_r \mu_r\, w_r(t),
\end{equation}
but with $w_r(t)$ concentrated near the end of prefill rather than spread
uniformly.

This end-localized correction reflects the intuition that the final prefill
iteration must occur immediately prior to the recorded prefill completion time,
while remaining fully trace-only and avoiding any reconstruction of global step
boundaries.

\paragraph{Implementation note (no pointwise reconstruction required).}
Although we define token pressures and correction weights as functions of
wall-clock time, the baseline estimator never requires evaluating them
instantaneously.
All quantities enter the estimator only through time integrals over phase
intervals, such as
$\int_{t_{i,s}}^{t_{i,e}} \tilde p^{\mathrm{pf}}(t)\,dt$.
These integrals can be computed exactly from trace-visible phase boundaries by
sweeping over a piecewise-constant time grid and applying per-request correction
masses whose placement within a grid interval does not affect correctness.



\subsection{Baseline: Time-Integrated NNLS Estimation}
\label{sec:baseline}

We first present a simple, trace-only baseline estimator that fits phase
durations using time-integrated pressures.
Because pressures are expressed in \emph{tokens per step}, a time integral
$\int p(t)\,dt$ has units $(\text{tokens}/\text{step})\cdot \text{seconds}$.
To form regression features with units of \emph{tokens}---so that
$\beta_1,\beta_2$ retain the interpretation ``seconds per token''---we
convert time integrals into approximate \emph{step integrals} using a
phase-local constant step density.

\begin{algorithm}[t]
\caption{Baseline trace-only estimation}
\label{alg:baseline_intuition}
\begin{algorithmic}[1]
\Require Phase traces $\{(r_i,\tau_i,t_{i,s},t_{i,e},P_{r_i},D_i)\}$, chunk size $C$
\Ensure Estimated execution coefficients $\hat\beta$

\For{each phase instance $i$}
  \State infer step count $N_i$ from trace-visible tokens
  \Comment{definition of $N_i$; §\ref{sec:problem_setup}}
\EndFor

\State construct global time grid from all phase boundaries
\Comment{trace-induced grid for piecewise-constant instantaneous pressures; §\ref{sec:pressures}}

\For{each time interval in the grid}
  \State measure concurrent prefill and decode activity
  \Comment{overlap $\Rightarrow$ \emph{instantaneous} pressures $p^{\mathrm{pf}}(t),p^{\mathrm{dec}}(t)$ on the grid; §\ref{sec:pressures}}
\EndFor

\For{each prefill request $r$}
  \State apply trace-only partial-chunk correction (optional)
  \Comment{missing mass $\mu_r$ and correction weights for \emph{instantaneous effective prefill pressure} $\tilde p^{\mathrm{pf}}(t)$; §\ref{sec:prefill_correction}, Eq.~\eqref{eq:rho_def}--Eq.~\eqref{eq:uniform_prefill_pressure}}
\EndFor

\For{each phase instance $i$}
  \State compute phase-level prefill and decode exposures
  \Comment{baseline exposures $A_i^{\mathrm{pf}},A_i^{\mathrm{dec}}$ from instantaneous pressures and phase-local step density; §\ref{sec:baseline}, Eq.~\eqref{eq:baseline_lambda_hat}, Eq.~\eqref{eq:baseline_integrated_exposures}}
\EndFor

\State fit execution coefficients via NNLS
\Comment{baseline predictor and NNLS objective; §\ref{sec:baseline}, Eq.~\eqref{eq:baseline_predictor}, Eq.~\eqref{eq:baseline_nnls}}

\State \Return $\hat\beta$
\end{algorithmic}
\end{algorithm}



\paragraph{Phase-local step density.}
For phase instance $i$, let $\ell_i=t_{i,e}-t_{i,s}$ be its observed duration and
let $N_i\in\mathbb{N}$ be the trace-inferred step count.
We define the (phase-local) step density proxy
\begin{equation}
\widehat\lambda_i \;=\; \frac{N_i}{\ell_i},
\qquad
\text{units: steps/second}.
\label{eq:baseline_lambda_hat}
\end{equation}
This assumes steps are spread approximately uniformly over the phase window,
and is used \emph{only} to convert time integrals into step-aggregated token
totals in the baseline.\footnote{Implementation detail: to avoid extreme $\widehat{\lambda}_i$ when $\ell_i$
is very small, we may compute $\widehat{\lambda}_i = N_i/\max\{\ell_i, \ell_{\min}\}$ for a
small $\ell_{\min}\ge 0$. The default $\ell_{\min}=0$ recovers Eq.~\eqref{eq:baseline_lambda_hat}.} 
We emphasize that $\widehat{\lambda}_i$ is a surrogate step-density used only
to convert time-integrated pressures into step-aggregated token exposures.



\paragraph{Phase-level exposures (baseline regression features).}
We define baseline phase-level exposures
\begin{align}
A^{\mathrm{pf}}_i
&=
\int_{t_{i,s}}^{t_{i,e}}
\tilde p^{\mathrm{pf}}(t)\,\widehat\lambda_i\,dt,
&
A^{\mathrm{dec}}_i
&=
\int_{t_{i,s}}^{t_{i,e}}
p^{\mathrm{dec}}(t)\,\widehat\lambda_i\,dt,
\label{eq:baseline_integrated_exposures}
\end{align}
which have units of tokens because $\tilde p^{\mathrm{pf}}(t)$ and
$p^{\mathrm{dec}}(t)$ are in tokens/step and $\widehat\lambda_i$ is in
steps/second.

\paragraph{Baseline predictor.}
The baseline predicts phase duration as
\begin{equation}
\widehat \ell_i(\beta)
=
\beta_0\,N_i
+
\beta_1\,A^{\mathrm{pf}}_i
+
\beta_2\,A^{\mathrm{dec}}_i,
\label{eq:baseline_predictor}
\end{equation}
where $\beta_0$ has units seconds/step and $\beta_1,\beta_2$ have units
seconds/token, consistent with the step-level model in
Eq.~\eqref{eq:step_model}.

\paragraph{NNLS estimation.}
Given a set of phase instances $\mathcal{I}$, we estimate $\beta$ via
non-negative least squares:
\begin{equation}
\min_{\beta \in \mathbb{R}^3_{\ge 0}}
\sum_{i\in\mathcal{I}}
\left(
\widehat \ell_i(\beta) - \ell_i
\right)^2 .
\label{eq:baseline_nnls}
\end{equation}

We emphasize that the simulator generates its own step-level token counts 
and KV-cache dynamics; the only information learned from traces is the 
estimated $\beta$, which parameterizes the simulator but does not 
encode any trace-specific timing information.

\paragraph{Why reweighting helps.}
This estimator remains wall-clock--weighted because the baseline exposures 
$A_i^{\mathrm{pf}}$ and $A_i^{\mathrm{dec}}$ are formed
by integrating instantaneous pressures over wall-clock time 
within each phase,
using the phase-average step density proxy $\widehat\lambda_i$ rather than a
time-varying step density.
As a result, phases that coincide with long-step periods can exert
disproportionate influence, motivating the step-density--reweighted estimator
in Section~\ref{sec:step_density}.


% ============================================================
\subsection{Step-Density Reweighted Estimation}
\label{sec:step_density}

We now introduce an iterative estimator that shifts fitting from wall-clock
weighting toward step-level weighting. The step-density reweighted estimator can be combined with any of the
trace-only prefill correction schemes from
Section~\ref{sec:prefill_correction}.
In addition, it enables a $\beta$-informed refinement that localizes the
prefill partial chunk using the inferred step-density model.

\begin{algorithm}[t]
\caption{Iterative step-density reweighted estimation}
\label{alg:iterative_intuition}
\begin{algorithmic}[1]
\Require Phase traces, chunk size $C$, initial coefficients $\beta^{(0)}$, tolerance $\epsilon$
\Ensure Refined execution coefficients $\beta^\star$

\State initialize $\beta \gets \beta^{(0)}$
\Comment{baseline NNLS initialization; §\ref{sec:baseline}, Eq.~\eqref{eq:baseline_nnls}}

\Repeat
  \State compute step density across time intervals from $\beta$
  \Comment{\emph{instantaneous} step-time and step-density on the trace grid; §\ref{sec:step_density}, Eq.~\eqref{eq:local_step_time}, Eq.~\eqref{eq:step_density}}

  \State assign step exposure across time intervals
  \Comment{trace-grid integration of instantaneous step density; §\ref{sec:step_density}}

  \For{each prefill request $r$}
    \State localize final partial chunk using step density
    \Comment{$\beta$-informed placement of missing prefill tokens within the prefill window via request-level attribution and last-step window; §\ref{sec:beta_informed_prefill_correction}, Eq.~\eqref{eq:qr_def}--Eq.~\eqref{eq:last_step_window}}
  \EndFor

  \For{each phase instance $i$}
    \State compute step-averaged prefill and decode pressures
    \Comment{step-averaged (i.e., step-weighted) pressures from \emph{instantaneous} pressures and phase-level attribution; §\ref{sec:step_averaged_pressures}, Eq.~\eqref{eq:step_averaged_pressures}}
  \EndFor

  \State refit $\beta$ using frozen features
  \Comment{NNLS on step-weighted exposures $N_i\bar p_i^{\mathrm{pf}}$ and $N_i\bar p_i^{\mathrm{dec}}$ (tokens), holding attribution fixed; §\ref{sec:step_density}, Eq.~\eqref{eq:sr_nnls}}

\Until{change in $\beta$ is below $\epsilon$}

\State \Return $\beta^\star$
\end{algorithmic}
\end{algorithm}



\subsubsection{Local Step-Time Model and Step Density}

Given instantaneous pressures, we model the (latent) step duration at time $t$ as
\begin{equation}
\Delta(t;\beta)
=
\beta_0
+
\beta_1\,\tilde p^{\mathrm{pf}}(t;\beta)
+
\beta_2\,p^{\mathrm{dec}}(t),
\qquad \Delta(t;\beta) > 0.
\label{eq:local_step_time}
\end{equation}
In the iterative estimator, the effective prefill pressure
$\tilde p^{\mathrm{pf}}(t;\beta)$ depends on $\beta$ through the
$\beta$-informed partial-chunk correction; in the baseline,
$\tilde p^{\mathrm{pf}}(t)$ is fixed. Thus, $\Delta(t;\beta)$ 
represents the instantaneous duration of a busy-loop
step that would be executed at time $t$ given the token pressures observed 
at that moment and execution coefficients $\beta$.

The corresponding step density is
\begin{equation}
\lambda(t;\beta) = \frac{1}{\Delta(t;\beta)}.
\label{eq:step_density}
\end{equation}

The same step-density model will also be used to localize prefill
partial-chunk mass within each prefill window in a trace-only manner.


% ============================================================
\subsubsection{$\beta$-Informed Partial-Chunk Correction}
\label{sec:beta_informed_prefill_correction}

Uniform and end-localized corrections treat the temporal location of the final
(prefill) partial chunk as exogenous. In contrast, our iterative estimator can
\emph{infer a trace-only proxy for step timing} from the current coefficient
iterate $\beta^{(m)}$ and use it to place the missing prefill token mass
$\mu_r$ near the \emph{latent final prefill step}. This coupling is the key
differentiator of the step-density reweighted procedure.

\paragraph{Step-time attribution density.}
Using $\lambda(t;\beta)$ from Eq.~\eqref{eq:step_density}, define the normalized
density over request $r$'s prefill window as follows.
\begin{equation}
q_r(t;\beta)
=
\frac{\lambda(t;\beta)}{\int_{t_{r,s}}^{t_{r,e}} \lambda(u;\beta)\,du},
\qquad
\int_{t_{r,s}}^{t_{r,e}} q_r(t;\beta)\,dt = 1.
\label{eq:qr_def}
\end{equation}
We emphasize that at iteration $m$, the step-density $\lambda(t;\beta^{(m)})$ is held fixed while 
forming $q_r(\cdot;\beta^{(m)})$; we interpret $q_r(\cdot;\beta^{(m)})$
as a \emph{trace-only step-time attribution model} within the prefill window.


\paragraph{Last-step window under $q_r$.}
Let $N_r=\lceil P_r/C\rceil$ be the inferred number of prefill steps and
$\mu_r = C-\rho_r \in [0,C)$ be the missing token mass of the final partial
chunk (Eq.~\eqref{eq:rho_def}).
Define the $q_r$-CDF
\begin{equation}
F_r(t;\beta)=\int_{t_{r,s}}^{t} q_r(u;\beta)\,du,
\label{eq:Fr_def}
\end{equation}
and the \emph{last-step window} as the final $1/N_r$ fraction of step exposure:
\begin{equation}
\mathcal{W}_r(\beta)
=
\Bigl\{
t\in[t_{r,s},t_{r,e})
:
F_r(t;\beta)\ge 1-\tfrac{1}{N_r}
\Bigr\}.
\label{eq:last_step_window}
\end{equation}
Intuitively, $\mathcal{W}_r(\beta)$ is the region in wall-clock time where the
final prefill step is most plausible given the current step-density model.

\paragraph{$\beta$-informed correction weight.}
We concentrate the missing mass $\mu_r$ on $\mathcal{W}_r(\beta)$, proportional
to the step-time attribution:
\begin{equation}
w_r(t;\beta)
=
\frac{q_r(t;\beta)\,\mathbf{1}\{t\in\mathcal{W}_r(\beta)\}}
{\int_{\mathcal{W}_r(\beta)} q_r(u;\beta)\,du},
\qquad
\int w_r(t;\beta)\,dt = 1.
\label{eq:wr_beta_informed}
\end{equation}
The effective prefill pressure used by the iterative estimator is then
\begin{equation}
\tilde p^{\mathrm{pf}}(t;\beta)
=
p^{\mathrm{pf}}_{\mathrm{full}}(t)
-
\sum_r \mu_r\, w_r(t;\beta).
\label{eq:prefill_pressure_beta_informed}
\end{equation}
This preserves total prefill token mass exactly and uses $\beta$ only to place
the missing-token mass \emph{within} each request's trace-visible prefill interval.

\paragraph{Trace-only implementation (integrals over a grid).}
As in the baseline, all quantities above enter only through integrals over the trace-induced grid
$\{[g_j,g_{j+1})\}$, on which $p^{\mathrm{pf}}_{\mathrm{full}}(t)$ and $p^{\mathrm{dec}}(t)$ are piecewise-constant
(and hence $\lambda(t;\beta)$ is also piecewise-constant).


\paragraph{Connection to the iterative NNLS update.}
At iteration $m$, we use $\beta^{(m)}$ to form $\tilde p^{\mathrm{pf}}(t;\beta^{(m)})$
via Eq.~\eqref{eq:prefill_pressure_beta_informed}, then compute step-averaged
pressures $\bar p^{\mathrm{pf}}_i(\beta^{(m)})$ and
$\bar p^{\mathrm{dec}}_i(\beta^{(m)})$ (Eq.~\eqref{eq:step_averaged_pressures}),
and finally solve the frozen-feature NNLS subproblem (Eq.~\eqref{eq:sr_nnls}).
Thus, $\beta$ is refined jointly with a $\beta$-informed, trace-only placement
of prefill partial-chunk mass, yielding a substantially more step-faithful
calibration than any fixed (non-iterative) correction.


\subsubsection{Normalized Step Density within a Phase}

For phase instance $i$, define
\[
\Lambda_i(\beta)
=
\int_{t_{i,s}}^{t_{i,e}} \lambda(t;\beta)\,dt,
\qquad
q_i(t;\beta)
=
\frac{\lambda(t;\beta)}{\Lambda_i(\beta)}.
\]
By construction, $\int q_i(t;\beta)\,dt = 1$.
We treat $N_i$ as fixed and use $q_i(t;\beta)$ as a trace-only 
step-time attribution model within the phase window.

\subsubsection{Step-Weighted Exposures}
\label{sec:step_averaged_pressures}

Decode phases require no partial-chunk correction: decode executes exactly one
token per step, so the trace-inferred step count equals the token count and all
decode work is fully accounted for by $p^{\mathrm{dec}}(t)$. For a prefill phase, 
define
\begin{equation}
\bar p^{\mathrm{pf}}_i(\beta)
=
\int_{t_{i,s}}^{t_{i,e}} \tilde p^{\mathrm{pf}}(t;\beta)\, q_i(t;\beta)\,dt,
\qquad
\bar p^{\mathrm{dec}}_i(\beta)
=
\int_{t_{i,s}}^{t_{i,e}} p^{\mathrm{dec}}(t)\, q_i(t;\beta)\,dt.
\label{eq:step_averaged_pressures}
\end{equation}

\paragraph{From step-averaged pressure to exposure.}
The quantities $\bar p_i^{\mathrm{pf}}(\beta)$ and $\bar p_i^{\mathrm{dec}}(\beta)$
remain \emph{step-averaged pressures} (tokens/step). The corresponding
\emph{step-weighted exposures} used by the regression are obtained by multiplying
by the trace-inferred step count: $N_i\bar p_i^{\mathrm{pf}}(\beta)$ and
$N_i\bar p_i^{\mathrm{dec}}(\beta)$ (tokens).


Note that $\bar p_i^{\mathrm{pf}}(\beta)$ retains units of tokens/step, so
$N_i\bar p_i^{\mathrm{pf}}(\beta)$ is an \emph{effective step-aggregated prefill exposure}
for phase $i$ under the step-timing model $\lambda(t;\beta)$.
In the special case of approximately uniform step density within the phase,
this reduces to the baseline exposure $A_i^{\mathrm{pf}}$ (up to numerical integration error).


\subsubsection{MM-Style Iterative NNLS}

At iteration $m$, we use $\beta^{(m)}$ to form both the step-averaged
pressures and the $\beta$-informed prefill correction, freeze these
quantities, and solve
\begin{equation}
\beta^{(m+1)}
=
\arg\min_{\beta\ge 0}
\sum_{i\in\mathcal{I}}
\left(
N_i\beta_0
+
N_i\bar p^{\mathrm{pf}}_i(\beta^{(m)})\beta_1
+
N_i\bar p^{\mathrm{dec}}_i(\beta^{(m)})\beta_2
-
\ell_i
\right)^2 .
\label{eq:sr_nnls}
\end{equation}
Optional damping
$\beta^{(m+1)} \leftarrow (1-\eta)\beta^{(m)} + \eta\beta^{(m+1)}$
improves numerical stability. 
We iterate until $\|\beta^{(m+1)} - \beta^{(m)}\|_2 < \epsilon$ or a maximum
iteration count is reached.

% ============================================================
\subsection{Robust Estimation and Fixed-Point View}
\label{sec:robust_fixed_point}

To mitigate outliers, we optionally replace the squared loss with a Huber loss
$\rho_\delta$.
The resulting mapping defines a fixed-point iteration
$\beta^{(m+1)}=\mathcal{F}(\beta^{(m)})$, terminated by relative change or a
fixed iteration budget.

% ============================================================
\subsection{Limitations and Scope}
\label{sec:limitations}

This approach assumes a single sequential busy-loop and accurate knowledge of
engine scheduling rules.
Partial-chunk corrections remain approximate due to unobserved step
boundaries.
Identifiability requires sufficient variation in token pressures.
Despite these limitations, the method provides a practical and non-intrusive
path to step-level calibration using only production traces.

\paragraph{Simulator linkage.}
The estimated coefficients $\beta^\star$ are used directly in the step-time
model (Eq.~\eqref{eq:step_model}) of the discrete-event simulator.


% ============================================================
\subsection{Design of Inference Workloads for Calibration}
\label{sec:workload_design}

Identifiability of step-level execution coefficients requires sufficient
variation in instantaneous token pressures.
Highly homogeneous workloads may yield nearly collinear features, limiting
estimation precision even with large trace volumes.
We therefore explicitly design inference workloads that \emph{excite}
distinct execution regimes during calibration windows.

\subsubsection{Calibration Probes}

We construct a small set of \emph{calibration probes}, each defined by a
controlled combination of:
\begin{itemize}
  \item request arrival rate and burstiness,
  \item input length distribution,
  \item prefix hit distribution,
  \item decode length distribution,
  \item maximum concurrency or batch limits.
\end{itemize}

These probes are executed using standard benchmarking or workload-generation
frameworks and produce ordinary production traces. No engine instrumentation,
step timing, or internal counters are used.

\subsubsection{Feature Diversity Objective}

Let $\phi_i(\beta)$ denote the step-resolved feature vector for phase $i$:
\[
\phi_i(\beta)
=
\left(
N_i,\;
N_i\bar p^{\mathrm{pf}}_i(\beta),\;
N_i\bar p^{\mathrm{dec}}_i(\beta)
\right).
\]

Calibration probes are selected to maximize diversity among these features
across phases, reducing collinearity between prefill, decode, and step-count
terms. In practice, this can be achieved by:
\begin{itemize}
  \item mixing prefill-heavy and decode-heavy workloads,
  \item varying concurrency independently of token lengths,
  \item introducing controlled arrival-rate ramps or bursts.
\end{itemize}

\subsubsection{Design-of-Experiments Perspective}

From a design-of-experiments viewpoint, probe selection aims to improve the
conditioning of the regression problem.
Simple heuristics include:
\begin{itemize}
  \item maximizing the determinant or minimizing the condition number of the
        empirical feature covariance matrix,
  \item ensuring coverage of distinct operating regimes
        (e.g., low-load vs.\ saturation).
\end{itemize}

These criteria are evaluated using trace-derived features only and do not
require knowledge of the true execution coefficients. These criteria 
depend only on aggregate variation in step-level feature magnitudes and 
are insensitive to the specific treatment of partial prefill
chunks, which affects only localized exposure attribution within phases.

\paragraph{Separation of Concerns.}
Workload generators and benchmarking frameworks are used solely to induce
pressure variability during calibration. Estimation of step-level execution
coefficients relies exclusively on trace-visible phase timing and token
counts, preserving the trace-only and non-intrusive nature of the method.

% ============================================================
\subsection{Justification and Scope}
\label{sec:extended_justification}

Robust loss functions address unavoidable noise and outliers in production
traces without altering the underlying execution model.
The fixed-point formulation clarifies how step-density reweighting reconciles
time-based observations with step-based costs.
Finally, explicit workload design ensures that the estimation problem is
well-conditioned by construction, rather than relying on incidental workload
variation.

Together, these extensions strengthen the estimator’s practical reliability
and interpretability while preserving its central property: step-level
execution coefficients are inferred using only trace-visible information,
without engine instrumentation or per-step timing.


\end{document}
