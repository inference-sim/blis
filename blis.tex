\documentclass[11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{times}
\usepackage{graphicx}
\usepackage{amsmath, amssymb}
\usepackage{hyperref}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{booktabs,longtable,array,makecell}
\setlength{\LTleft}{0pt}   % no left indent for longtable
\setlength{\LTright}{0pt}  % no right indent for longtable
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}p{#1}}

% --- Theorems, lemmas, claims, proofs ---
\usepackage{amsthm}

% Number theorems within sections; share a counter across theorem-like envs.
\theoremstyle{plain} % bold headings, italic body
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{claim}[theorem]{Claim}

\theoremstyle{definition} % bold heading, upright body
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark} % italic heading, upright body
\newtheorem{remark}[theorem]{Remark}
\newtheorem{note}[theorem]{Note}

% Optional: unnumbered versions
\newtheorem*{theorem*}{Theorem}
\newtheorem*{lemma*}{Lemma}
\newtheorem*{claim*}{Claim}

% Proof environment is provided by amsthm:
% \begin{proof} ... \end{proof}
% Customize QED symbol if you like:
\renewcommand{\qedsymbol}{$\blacksquare$}

% (Optional) If you want equations numbered by section (since you already use amsmath):
\numberwithin{equation}{section}



\title{BLIS: Blackbox Inference Performance Estimator}
\author{AI Platform Optimization Team}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This paper introduces BLIS, a system for blackbox inference performance estimation 
designed to model inference request flows and latency.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

\section{Related Work}
\label{sec:related-work}

\section{Design overview}
\label{sec:design-overview}

BLIS is primarily geared towards vLLM. We will describe extensions for other
inference platforms like sglang in Section \ref{sec:future-work}.

\subsection{vLLM overview}
\label{subsec:vllm-overview}
We will focus on handling of an inference request by vLLM. vLLM
has two main components: the API server and the engine core; these operate as separate threads and
communicate through a message queue.

\paragraph{API server thread.}
The API server is implemented using FastAPI. Its role is to handle client-facing tasks without blocking on actual model execution.
The API thread is responsible for:
\begin{enumerate}
    \item Receiving and tokenizing incoming requests from clients.
    \item Enqueuing tokenized requests into the message queue.
    \item Streaming back partial responses as they become available from the engine.
    \item Detokenizing completed sequences and packaging them into response
          objects.
    \item Emitting the final response back to the client.
\end{enumerate}

\paragraph{Engine core thread.}
The engine core runs the central \emph{inference loop}. This loop is driven by a
scheduler that repeatedly:
\begin{enumerate}
    \item Collects pending requests from the queue, forming a dynamic batch.
    \item Determines whether each request is in a \emph{prefill} (first token)
          or \emph{decode} (subsequent tokens) stage.
    \item Executes the forward pass on the GPU for that batch.
    \item Updates KV cache blocks, manages allocation and eviction, and applies
          prefix caching optimizations when possible.
    \item Returns generated tokens back to the API layer, either for streaming
          or final response.
\end{enumerate}
We will refer to this engine loop as the \emph{busy loop} since it continuously
steps through request-batches at a fine-grained timescale.

\subsection{Design of BLIS}
\label{subsec:blis-design}

\section{Latency model}
\label{sec:latency-model}

\subsubsection{Request life-cycle}
\label{subsubsec:request-life-cycle}

We formalize the progression of a single request $i$ through the inference system.
Each stage is associated with latencies and variables in
Table~\ref{tab:notation} that are estimated, and collectively, they determine the end-to-end delay
$e_i$.

\paragraph{1. Ingress.}
The client emits request $i$, which traverses the network before reaching the
server. This contributes a network ingress delay $n_i^{\text{in}}$. From a 
modeling perspective, this has a linear dependency on the number of input tokens in the request, $t_i^{in}$.

\paragraph{2. Pre-process.}
Upon arrival, the API server thread tokenizes the prompt, performs light
validation, and enqueues the request into the message queue contributing a latency of $u_i$. This has a linear dependence 
on the number of input tokens, $t_i^{in}$.

\paragraph{3. Scheduling.} The request waits in the queue until there is sufficient GPU capacity; here, sufficiency
is determined by the batching policy, token budget, and other vLLM params. The duration of this waiting period equals the
sum of the durations of the busy loop iterations within this period.

\paragraph{4. Chunked prefill.} The duration of prefill equals the
sum of the durations of the busy loop iterations wherein the request participated in its prefill phase.

\paragraph{5. Decode.} The duration of decode equals the
sum of the durations of the busy loop iterations wherein the request participated in its decode phase.

\paragraph{6. Post-process.} Once all output tokens are produced, the API server detokenizes them and
packages the final response. This incurs a post-process latency $c_i$, with a linear dependence on 
the number of output tokens $t_i^{out}$.

\paragraph{7. Egress.}
The server emits response $i$, which traverses the network before reaching the
client. This contributes a network egress delay $n_i^{\text{out}}$, with a linear
dependence on the number of output tokens $t_i^{out}$.

\subsection{End-to-End request latency model}
\label{subsec:end-to-end-latency-model}

\paragraph{Basic decomposition.}
\begin{align}
e_i = n_i + u_i + s_i + p_i + d_i + c_i
\label{eq:end-to-end}
\end{align}

\paragraph{Suitable for estimation.}
\begin{align}
e_i \approx \alpha_0
 + \alpha_1\, t_i^{\text{in}}
 + \alpha_2\, t_i^{\text{out}}
 + \sum_{j \in \mathcal{S}_i} \lambda_j
 + \sum_{j \in \mathcal{P}_i} \lambda_j
 + \sum_{j \in \mathcal{D}_i} \lambda_j
\label{eq:e2e-param}
\end{align}

\textbf{Note:} Regression equations such as \eqref{eq:e2e-param} and \eqref{eq:busy-loop-expanded}
are understood to hold up to an additive residual noise term. When we write $a \approx b$, it is shorthand for $a=b+\varepsilon$ 
where $\varepsilon$ is the noise term. 

\subsubsection{Busy-loop latency model}
\label{subsubsec:busy-loop-latency-model}
We first define the quadratic term that captures the quadratic size of the the KV tensors
that are computed in the presence of cache misses and cache hits for requests during iteration $j$:
\begin{align}
\psi_j
  = \sum_{i \in \mathcal{B}_j}
    \Big(
      \chi_{j,i}^{\text{miss}} \cdot \chi_{j,i}^{\text{hit}}
      + \big(\chi_{j,i}^{\text{miss}}\big)^2
    \Big)
\label{eq:quad}
\end{align}

The duration of iteration $j$ is now modeled as a quadratic function of features derived from the contents of the queue, batch, 
and cache as follows:
\begin{align}
\lambda_j & \approx \beta_0 && \text{(intercept)} \nonumber \\
 & + \beta_1 \rho_j
 + \beta_2 \tau_j && \text{(scheduling)} \nonumber \\
 & + \beta_3 \pi_j
 + \beta_4 \sum_{i \in \mathcal{B}_j} \chi_{j,i}^{\text{hit,new}}
 + \beta_5 \sum_{i \in \mathcal{B}_j} \chi_{j,i}^{\text{miss}} 
 + \beta_6 \psi_j && \text{(prefill; last term is quadratic)} \nonumber \\
 & + \beta_7 \delta_j 
 + \beta_8 \tau_j^{\text{dec}} && \text{(decode)} \nonumber \\
 & + \beta_9 \rho_j^{\text{pre-fin}}
 + \beta_{10} \rho_j^{\text{dec-fin}}
 + \beta_{11} \tau_j^{\text{fin}} && \text{(post-process)}
\label{eq:busy-loop-expanded}
\end{align}

\section{Estimation}
\label{sec:estimation}

\subsection{Base Estimation with Linear Regression}
\label{subsec:base-estimation-linear-regression}

We estimate the model coefficients through a set of scenarios as follows. 
Each scenario involves the client sending an inference request workload 
to the server, and defines a set of conditions that the request 
workload needs to satisfy.

\subsubsection{Scenario 2}
\label{subsubsec:scenario2}

\paragraph{Setup.}
Fix chunk size $C \in \{256,512,1024,2048\}$, block size $b=16$, and 
maximum length $M=2048$. Choose a step $J>0$ for gridding sequence lengths 
(e.g., $J{=}20$ train, $J{=}25$ test). Each $C$ is associated with multiple 
request pairs; a pair of these requests $(i,1)$ and $(i,2)$ is designed as follows.

\begin{align*}
t_{i,1} & = m_iC & & \forall m_i \in \{0, 1, 2, \ldots, \lfloor M/C \rfloor \} \\
t_{i,2} & = t_{i,1}+\Delta_i & & \forall \Delta_i \in \{0, J, 2J, 3J, \ldots \lfloor C/J \rfloor \} 
\end{align*}

Every request is a prefill request; hence $t_{i, k}^{out}$ is always $1$. Each request
consists of a single prompt and is sent to the \textit{/v1/completions} API endpoint of 
the vllm server.


\paragraph{Note:} Certain members in these pairs are omitted (not sent) by the client. 
When $m_i = 0$, the first member of the pair will have no tokens. 
When $m_i = 0$ and $\Delta_i = 0$, the second member of the pair will have no tokens.
If any setting of $m_i$ and $\Delta_i$ causes the sequence
length of the second member to exceed $M$, the request is omitted.

The token sequence of $(i,1)$ is a strict prefix of $(i,2)$, and prefixes do not overlap across pairs.
Requests are issued strictly sequentially $(i,1) \to (i,2) \to (i + 1,1) \to (i+1,2)$.

\paragraph{Model.} 
Define the coefficient sums $\theta_0, \theta_1, \text{ and } \beta_0^\star$ as follows.
\begin{align}
\theta_0 & \doteq & \alpha_0 + \beta_1 + \beta_9 + \beta_{11} + \alpha_2 \label{eq:theta0} \\
\theta_1 & \doteq &  \beta_0 + \beta_3 \label{eq:theta1} \\
\theta_2 & \doteq & \alpha_1 + \beta_2 \label{eq:theta2}
\end{align}

\begin{theorem}
\label{theo:scenario2}
In Scenario 2, the latency equation \eqref{eq:e2e-param} reduces to the following form.

\begin{align}
e_{i,1}
&\approx \theta_0
  + \theta_1 m_i 
  + \theta_2 m_i C
  + \beta_5 \frac{m_iC}{b}
  + \beta_6 \frac{C^2}{2b^2} m_i(m_i + 1) \label{eq:sc2mem1}\\
e_{i,2}
&\approx \theta_0
  + \theta_1 (m_i + 1) 
  + \theta_2 (m_i C + \Delta_i)
  + \beta_5 \frac{\Delta_i}{b}
  + \beta_6 \frac{\Delta_i (m_i C + \Delta_i)}{b^2} \label{eq:sc2mem2}
\end{align}
\end{theorem}

\begin{proof}
Scenario~2 is prefill-only and strictly sequential, so there is no decode
and no queue wait beyond admission: $\mathcal{D}_{i,k}=\varnothing$ and
$\mathcal{S}_{i,k}=\varnothing$ for $k\in\{1,2\}$. Thus:

\begin{align}
e_{i,1}
&\approx \alpha_0
  + \alpha_1 (m_iC)
  + \alpha_2
  + \sum_{j \in \mathcal{P}_{(i, 1)}} \lambda_j \label{eq:sc2step1mem1} \\
e_{i,2}
&\approx \alpha_0
  + \alpha_1 (m_iC + \Delta_i)
  + \alpha_2
  + \sum_{j \in \mathcal{P}_{(i, 2)}} \lambda_j \label{eq:sc2step1mem2}
\end{align}

Request $(i,1)$ has $m_i$ busy loop iterations, and request $(i, 2)$ has $m_i + 1$. 
By design, each iteration for request $(i,k)$ has
$\pi_j=1$ (single active request), and the first such iteration involves
scheduling of the request, so $(\rho_j,\tau_j)=(1,\,t_{i,k})$ only in that first
iteration and $(0,0)$ thereafter; similarly, $(\rho_j^{pre-fin},\tau_j^{fin}) = (1, 1)$ 
only in the last iteration and $(0,0)$ until then. There are no decode terms. 


Consider the first member of pair $i$.
At iteration $j \in \{1, \ldots, m_i\}$, there are $(j-1)C/b$ total cache hit blocks, 
zero new cache hits, and $C/b$ misses. Hence, summing \eqref{eq:busy-loop-expanded} 
over $j\in\mathcal{P}_{i,1}$ yields:

\begin{align}
\sum_{j \in \mathcal{P}_{(i, 1)}} \lambda_j
& = \beta_0 m_i + \beta_1 + \beta_2 m_i C  + \beta_3 m_i + 
\beta_4 \left( \sum_{j \in \mathcal{P}_{(i, 1)}} \chi_{j,i}^{\text{hit,new}} \right) + 
\beta_5 \left( \sum_{j \in \mathcal{P}_{(i, 1)}} \chi_{j,i}^{\text{miss}} \right) \nonumber \\
& + \beta_6 \left( \sum_{j \in \mathcal{P}_{(i, 1)}} \psi_j \right) + 
\beta_9 + \beta_{11} \nonumber \\ 
& = (\beta_1 + \beta_9 + \beta_{11}) + 
(\beta_0 + \beta_3) m_i + 
\beta_2 m_i C  +
\sum_{j \in \mathcal{P}_{(i, 1)}} \left( \beta_4 \chi_{j,i}^{\text{hit,new}} + 
\beta_5 \chi_{j,i}^{\text{miss}} + 
\beta_6 \psi_j \right) \nonumber \\
& = (\beta_1 + \beta_9 + \beta_{11}) + 
(\beta_0 + \beta_3) m_i + 
\beta_2 m_i C  +
\beta_5 (m_i C/b) + 
\sum_{j \in \mathcal{P}_{(i, 1)}} \beta_6 \psi_j \nonumber \\
& = (\beta_1 + \beta_9 + \beta_{11}) + 
(\beta_0 + \beta_3) m_i + 
\beta_2 m_i C + \beta_5 (m_i C/b) + 
\beta_6 (C / b)^2 \sum_{j = 1}^{m_i} ((j - 1) + 1) \nonumber \\
& = (\beta_1 + \beta_9 + \beta_{11}) + 
(\beta_0 + \beta_3) m_i + 
\beta_2 m_i C + \beta_5 (m_i C/b) + 
\beta_6 (C / b)^2 m_i(m_i + 1)/2 \label{eq:sc2step2mem1}
\end{align}


Consider the second member of pair $i$.
The entire sequence of the first member is a prefix; hence, at iteration 
$j \in \{1, \ldots, m_i\}$, there are $(j-1)C/b$ total cache hit blocks, 
$C/b$ new cache hits, and zero cache misses; at iteration $m_i + 1$,
there are $m_iC/b$ total cache hit blocks, zero new cache hits, 
and $\Delta_i/b$ cache misses. Hence, summing \eqref{eq:busy-loop-expanded} 
over $j\in\mathcal{P}_{i,2}$ yields:

\begin{align}
\sum_{j \in \mathcal{P}_{(i, 2)}} \lambda_j
& = (\beta_1 + \beta_9 + \beta_{11}) + 
(\beta_0 + \beta_3) (m_i + 1) + 
\beta_2 (m_i C + \Delta_i) + \beta_5 (\Delta_i/b) + 
\beta_6 m_i C \Delta_i / b^2 \label{eq:sc2step2mem2}
\end{align}

Combining Definitions \eqref{eq:theta0}, \eqref{eq:theta1}, \eqref{eq:theta2} and 
Equations \eqref{eq:sc2step1mem1} and \eqref{eq:sc2step2mem1} yields \eqref{eq:sc2mem1}. 
Combining Definitions \eqref{eq:theta0}, \eqref{eq:theta1}, \eqref{eq:theta2} and 
Equations \eqref{eq:sc2step1mem2} and \eqref{eq:sc2step2mem2} yields \eqref{eq:sc2mem2}.

Hence, Theorem \ref{theo:scenario2} holds.
\end{proof}

We fit \eqref{eq:sc2mem1} and \eqref{eq:sc2mem2} jointly by 
Non-negative Least Squares and RANSAC, over all $(m_i,\Delta_i)$ and all $C$.
This enables us to estimate $\theta_0$, $\theta_1$, $\theta_2$, 
$\beta_5$, and $\beta_6$ directly. 

\setlength{\tabcolsep}{6pt}

{
\setlength{\emergencystretch}{2em}
\begin{longtable}{@{} C{0.22\textwidth} L{0.74\textwidth} @{}}
\caption{Notation used in this paper.}
\label{tab:notation}\\

\toprule
\textbf{Symbol} & \textbf{Meaning} \\
\midrule
\endfirsthead

\multicolumn{2}{l}{\textit{Table \ref{tab:notation} (continued)}}\\
\toprule
\textbf{Symbol} & \textbf{Meaning} \\
\midrule
\endhead

\midrule
\multicolumn{2}{r}{\textit{Continued on next page}}\\
\bottomrule
\endfoot

\bottomrule
\endlastfoot


\multicolumn{2}{l}{\textit{Indices and sets}}\\
\midrule
$i$ & Request (or request-pair) index. \\
$k \in \{1,2\}$ & Member of a Scenario~2 prefix pair: $(i,1)$ is the prefix; $(i,2)$ extends it. \\
$j$ & Busy-loop iteration index. \\
$\mathcal{B}_j$ & Requests included in the batch at iteration $j$. \\
$\mathcal{S}_i$ & Iterations where request $i$ is waiting in the message queue. \\
$\mathcal{P}_i$ & Iterations where request $i$ participates in prefill. \\
$\mathcal{D}_i$ & Iterations where request $i$ participates in decode. \\[2pt]

\midrule
\multicolumn{2}{l}{\textit{Per-request latencies and token counts}}\\
\midrule
$e_i$ & End-to-end latency of request $i$. \\
$n_i^{\text{in}},\, n_i^{\text{out}}$ & Network ingress / egress latency for request $i$. \\
$n_i$ & Total network latency; $n_i = n_i^{\text{in}} + n_i^{\text{out}}$. \\
$u_i$ & Pre-processing latency (API $\rightarrow$ queue) for request $i$. \\
$s_i$ & Scheduling/wait time in the queue for request $i$. \\
$p_i$ & Prefill latency (scheduled $\rightarrow$ first token) for request $i$. \\
$d_i$ & Decode latency (first $\rightarrow$ last token) for request $i$. \\
$c_i$ & Post-processing latency for request $i$. \\
$t_i^{\text{in}},\, t_i^{\text{out}}$ & Input / output tokens of request $i$. \\
$t_{i,1},\, t_{i,2}$ & Scenario~2: input tokens of the pair members $(i,1)$ and $(i,2)$. \\
$\Delta_i$ & Scenario~2: incremental tokens; $\Delta_i \doteq t_{i,2} - t_{i,1}$ (chosen $<C$). \\[2pt]

\midrule
\multicolumn{2}{l}{\textit{Busy-loop features (iteration $j$)}}\\
\midrule
$\lambda_j$ & Duration (length) of busy-loop iteration $j$. \\
$\rho_j$ & Requests in the message queue at the start of iteration $j$. \\
$\tau_j$ & Total \emph{input} tokens across all queued requests at $j$. \\
$\pi_j$ & Prefill multiplicity in batch at $j$ (number of prefill requests active). \\
$\delta_j$ & Number of decode requests active in batch at $j$. \\
$\chi_{j,i}^{\text{hit,new}}$ & Cache hits (in blocks) among \emph{newly touched} prefill blocks for request $i$ in $j$. \\
$\chi_{j,i}^{\text{miss}}$ & Cache misses (in fractional blocks) on new prefill tokens for $i$ in $j$; equals new-tokens$/b$. \\
$\chi_{j,i}^{\text{hit}}$ & Total cache hits (in blocks) for $i$ in $j$. \\
$\tau_j^{\text{dec}}$ & Total decode tokens across all decode requests in batch at $j$. \\
$\psi_j$ & Quadratic cache penalty at $j$: $\sum_{r \in \mathcal{B}_j}\big(\chi_{j,r}^{\text{miss}}\chi_{j,r}^{\text{hit}} + (\chi_{j,r}^{\text{miss}})^2\big)$. \\
$\rho_j^{\text{pre-fin}}$ & Prefill requests that finished in iteration $j$. \\
$\rho_j^{\text{dec-fin}}$ & Decode requests that finished in iteration $j$. \\
$\tau_j^{\text{fin}}$ & Tokens across all requests that finished in iteration $j$. \\[2pt]

\midrule
\multicolumn{2}{l}{\textit{Scenario~2 (chunk-aligned prefix pairs): derived counts and features}}\\
\midrule
$C$ & Chunk size (tokens per prefill iteration). \\
$b$ & KV-cache block size (tokens). \\
$M$ & Maximum sequence length (tokens). \\
$J$ & Step used to grid lengths (Scenario~2). \\
$t_{i,1}$ & First member length, constrained to multiples of $C$: $t_{i,1}=mC$, $m\in\mathbb{Z}_{\ge 0}$. \\
$\Delta_i$ & Increment for the second member, $0<\Delta_i<C$; $t_{i,2}=t_{i,1}+\Delta_i$. \\
$\eta(t)$ & Chunk count: $\eta(t)=\lceil t/C\rceil$; hence $\eta(t_{i,1})=m$, $\eta(t_{i,2})=m{+}1$. \\
$m^{(1)}_i,\,m^{(2)}_i$ & Miss blocks: $m^{(1)}_i=\frac{mC}{b}$,\; $m^{(2)}_i=\frac{\Delta_i}{b}$. \\
$q^{(1)}_i$ & Quadratic (first member): $\displaystyle q^{(1)}_i=\frac{C^2}{2b^2}\,m(m{+}1)=\frac{1}{2b^2}\,t_{i,1}(t_{i,1}{+}C)$. \\
$q^{(2)}_i$ & Quadratic (second member): $\displaystyle q^{(2)}_i=\Big(\frac{\Delta_i}{b}\Big)\Big(\frac{t_{i,2}}{b}\Big)=\frac{\Delta_i\,(mC+\Delta_i)}{b^2}$. \\
$\theta_0$ & Scenario~2 intercept: $\theta_0=\alpha_0+\beta_1+\beta_9$ (first-iteration queue admission + prefill-finish). \\
$\theta_1$ & Scenario~2 linear slope: $\theta_1=\alpha_1+\beta_2+\beta_{11}$ (tokenize + one-time queued-tokens scan + finish-tokens). \\
$\beta_0^\star$ & Per-iteration base (prefill): $\beta_0^\star=\beta_0+\beta_3$; contributes $\beta_0^\star\cdot \eta(t)$. \\

\midrule
\multicolumn{2}{l}{\textit{Coefficients and grouped parameters}}\\
\midrule
$\alpha_0, \alpha_1, \alpha_2, \beta_0, \ldots, \beta_{11}, \beta_0^\star, \theta_0, \theta_1, \gamma_0, \gamma_1$ \\

\midrule
\multicolumn{2}{l}{\textit{Units and conventions}}\\
\midrule
blocks & All $\chi^{\cdot}_{j,i}$, $m^{(\cdot)}_i$, $q^{(\cdot)}_i$ are expressed using block units; fractional misses $=$ tokens$/b$. \\
tokens & All $t$-quantities ($t_i^{\text{in}}$, $t_{i,k}$, $\Delta_i$, $C$, $M$) are in tokens. \\
time & $\lambda_j$, $e_i$, and stage latencies ($n_i^{\cdot}, m_i, s_i, p_i, d_i, c_i$) are in time units. \\

\end{longtable}
}



\subsection{Refinements using Blackbox Optimization}
\label{subsec:refinements-blackbox-optimization}

\section{Evaluation}
\label{sec:evaluation}

\section{Implementation}
\label{sec:implementation}

\section{Limitations of our Approach}
\label{sec:limitations}

Our current approach has several important limitations:

\begin{enumerate}
    \item \textbf{Hardware and model specificity.}
    BLIS is calibrated for a fixed combination of GPU type and model
    architecture. We do not attempt to generalize across different hardware or
    models. If either changes, BLIS must be retrained with new data.

    \item \textbf{No pre-emption.}
    We do not model pre-emption of running requests. In practice, triggering
    pre-emption during request handling is rare and avoided by the scheduler
    logic, but its absence in our model means we cannot accurately predict
    latency under pre-emptive scenarios.

    \item \textbf{No speculative decoding.}
    We currently omit speculative decoding mechanisms (such as those recently
    introduced in vLLM as an experimental feature).
\end{enumerate}

\section{Future Work}
\label{sec:future-work}

\section{Conclusion}
\label{sec:conclusion}

\bibliographystyle{plain}
\bibliography{blis}

\end{document}
