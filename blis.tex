\documentclass[11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{times}
\usepackage{graphicx}
\usepackage{amsmath, amssymb}
\usepackage{bm}
\usepackage[most]{tcolorbox}
\usepackage{tabularx}
\usepackage{pifont} % optional for symbols
\usepackage{tikz}
\usetikzlibrary{arrows.meta}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{hyperref}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{booktabs,longtable,array,makecell}
\setlength{\LTleft}{0pt}   % no left indent for longtable
\setlength{\LTright}{0pt}  % no right indent for longtable
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}p{#1}}


% --- Theorems, lemmas, claims, proofs ---
\usepackage{amsthm}

% Number theorems within sections; share a counter across theorem-like envs.
\theoremstyle{plain} % bold headings, italic body
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{claim}[theorem]{Claim}

\theoremstyle{definition} % bold heading, upright body
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark} % italic heading, upright body
\newtheorem{remark}[theorem]{Remark}
\newtheorem{note}[theorem]{Note}

% Optional: unnumbered versions
\newtheorem*{theorem*}{Theorem}
\newtheorem*{lemma*}{Lemma}
\newtheorem*{claim*}{Claim}

% Proof environment is provided by amsthm:
% \begin{proof} ... \end{proof}
% Customize QED symbol if you like:
\renewcommand{\qedsymbol}{$\blacksquare$}

\newcommand{\code}[1]{\texttt{#1}}

\definecolor{prefillcolor}{RGB}{173,216,230}  % Light blue
\definecolor{decodecolor}{RGB}{144,238,144}   % Light green
\definecolor{gridcolor}{RGB}{200,200,200}     % Light gray
\definecolor{highlightcolor}{RGB}{255,182,193} % Light pink for integration region


% (Optional) If you want equations numbered by section (since you already use amsmath):
\numberwithin{equation}{section}



\title{BLIS: Blackbox Inference Performance Estimator}
\author{AI Platform Optimization Team}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This paper introduces BLIS, a system for blackbox inference performance estimation 
designed to model inference request flows and latency.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

\section{Related Work}
\label{sec:related-work}

\section{Design overview}
\label{sec:design-overview}

BLIS is primarily geared towards vLLM. We will describe extensions for other
inference platforms like sglang in Section \ref{sec:future-work}.

\subsection{vLLM overview}
\label{subsec:vllm-overview}
We will focus on handling of an inference request by vLLM. vLLM
has two main components: the API server and the engine core; these operate as separate threads and
communicate through a message queue.

\subsubsection{API server thread}
The API server is implemented using FastAPI. Its role is to handle client-facing tasks without blocking on actual model execution.
The API thread is responsible for:
\begin{enumerate}
    \item Receiving and tokenizing incoming requests from clients.
    \item Enqueuing tokenized requests into the message queue.
    \item Streaming back partial responses as they become available from the engine.
    \item Detokenizing completed sequences and packaging them into response
          objects.
    \item Emitting the final response back to the client.
\end{enumerate}

\subsubsection{Engine core thread}
The engine core runs the central \emph{inference loop}. This loop is driven by a
scheduler that repeatedly:
\begin{enumerate}
    \item Collects pending requests from the queue, forming a dynamic batch.
    \item Determines whether a request is in a \emph{prefill} (first token)
          or \emph{decode} (subsequent tokens) phase.
    \item Executes the forward pass on the GPU for that batch.
    \item Updates KV cache blocks, manages allocation and eviction, and applies
          prefix caching optimizations when possible.
    \item Returns generated tokens back to the API layer, either for streaming
          or final response.
\end{enumerate}
We will refer to this engine loop as the \emph{busy loop} since it continuously
steps through request-batches at a fine-grained timescale.

\subsection{Design of BLIS}
\label{subsec:blis-design}

\section{Latency model}
\label{sec:latency-model}

\subsection{Request types}
\label{subsubsec:request-types}

Requests can be of two types: \textit{prefill-only} or \textit{decode}. 
Prefill-only requests have their maximum output length set to $1$, and \textit{decode}
requests have this set to a number greater than $1$.

\subsection{Request life-cycle}
\label{subsubsec:request-life-cycle}

\begin{figure}[!ht]
  \centering
  \includegraphics[height=0.8\textwidth]{figs/reqlifecycle.png}
  \caption{Request Lifecycle}
  \label{fig:req-lifecycle}
\end{figure}

The life of an inference request in the system passes through multiple stages 
as  illustrated in Figure \ref{fig:req-lifecycle} and described below.

\paragraph*{1. Ingress.}
The client emits an inference request $i$ to the server. This contributes 
a network latency that depends linearly on the number of input tokens 
$\ell_i^{\text{in}}$.

\paragraph*{2. Pre-process.}
Upon arrival, the API server tokenizes the prompt, performs light
validation, and enqueues the request into the message queue. This contributes 
a latency that depends linearly on $\ell_i^{\text{in}}$.

\paragraph*{3. Scheduling.} The request waits in the queue until there is 
sufficient GPU capacity. During this waiting period, other requests get to participate
in the busy loop iterations.

\paragraph*{4. Chunked prefill.} The uncached portion of this request is 
prefilled one chunk at a time. The latency of this stage equals the sum of the 
duration of the busy loop iterations in which this request prefills.

\paragraph*{5. Decode.} The latency of this stage equals the sum of the duration
of the busy loop iterations in which this request is in its decode phase. 
The first output token is generated during the prefill phase, and subsequent 
tokens are generated in the decode iterations.

\paragraph*{6. Post-process.} Once all output tokens are produced, 
the API server detokenizes them and
packages the final response. The latency of this stage depends linearly on the 
number of output tokens $\ell_i^{\text{out}}$.

\paragraph*{7. Egress.}
The server emits response $i$ to the
client. This contributes a network latency that depends linearly 
on the number of output tokens $\ell_i^{\text{out}}$.

\iffalse
\subsection{End-to-End request latency model}
\label{subsec:end-to-end-latency-model}

Let $\lambda_j$ denote the duration of busy loop iteration $j$. We start
with a high-level model for the end-to-end latency as follows.

\begin{align}
e_i \approx \alpha_0
 + \alpha_1 \ell_i^{\text{in}}
 + \alpha_2 \ell_i^{\text{out}}
 + \sum_{\text{i waits at j}} \lambda_j
 + \sum_{\text{i prefills at j}} \lambda_j
 + \sum_{\text{i decodes at j}} \lambda_j
\label{eq:e2e-param}
\end{align}

\textbf{Note:} When we write $a \approx b$, it is shorthand for $a=b+\varepsilon$ 
where $\varepsilon$ is a residual noise term. Regression equations such 
as \eqref{eq:e2e-param} are understood to hold up to residual noise term.

\subsubsection{Busy loop}
\label{subsubsec:busy-loop-latency-model}
Let $\chi_{j,i}^{\text{hit}}$ and $\chi_{j,i}^{\text{miss}}$ be the numbers of 
cached and uncached (new) tokens of request $i$ during iteration $j$.
Define the following request sets.
\begin{align*}
  \mathcal{P}_j & = & \{i:\text{$i$ is in prefill at $j$}\} \nonumber \\
  \mathcal{B}^{(1)}_j & = & \{i:\text{$i$ is in its first busy loop iteration at $j$}\} \nonumber \\
  \mathcal{D}_j & = & \{i:\text{$i$ is decoding at $j$}\} \nonumber \\
  \mathcal{F}_j & = & \{i:\text{$i$ finishes at $j$}\}
\end{align*}
Let $\ell_{j,i}$ denote the context length of request $i$ at the 
start of iteration $j$.

\paragraph*{Quadratic prefill term.} We define a quadratic term as follows.
\begin{align}
\psi_j
  = \sum_{i \in \mathcal{P}_j}
    \Big(
      \chi_{j,i}^{\text{miss}} \cdot \chi_{j,i}^{\text{hit}}
      + \big(\chi_{j,i}^{\text{miss}}\big)^2
    \Big)
\label{eq:quad}
\end{align}

\paragraph*{Latency model.}
Let $L^{\text{dec}}_j=\sum_{i\in\mathcal{D}_j}\ell_{j,i}$ be the total decode
context length in iteration $j$, and
$L^{\text{fin}}_j=\sum_{i\in\mathcal{F}_j}\ell_{j,i}$ the total context length of
requests that finish in $j$ (a proxy for KV cache tokens to free). 
We model the iteration time as
\begin{align}
\lambda_j 
  \approx \beta_0
  + \beta_1 \sum_{i \in \mathcal{B}^{(1)}_j} \chi_{j,i}^{\text{hit}}
  + \beta_2 \sum_{i \in \mathcal{P}_j}      \chi_{j,i}^{\text{miss}}
  + \beta_3 \psi_j 
  + \beta_4 |\mathcal{D}_j|
  + \beta_5 L^{\text{dec}}_j
  + \beta_6 |\mathcal{F}_j|
  + \beta_7 L^{\text{fin}}_j
\label{eq:busy-loop-expanded}
\end{align}

\paragraph*{Justification.} Each busy-loop iteration mixes four kinds of work.

\begin{enumerate}
  \item \emph{Fixed work.} An iteration-invariant cost (\(\beta_0\)).
  \item \emph{Prefill work.} The first prefill iteration of a request involves hashing
its input tokens to determine the number of cache hits (linear in hits; \(\beta_1\)).
Every new token pays a linear per-token cost (embeddings, MLP, KV writes; \(\beta_2\)).
Attention mechanism during prefill is quadratic: every new token 
(cache miss) must attend to all tokens already in the context (cache hit) 
plus the other new tokens from the same chunk (\(\psi_j\) in~\eqref{eq:quad}; 
\(\beta_3\)).
  \item \emph{Decode work.} Decode generates one token per active request:
there is a per-request overhead (\(\beta_4\)) and compute that scales with
the request’s current context length (\(\beta_5\)).
  \item \emph{Completion work.} When a request completes, we free KV blocks, update
refcounts, and serialize output; this scales linearly in the number of 
finished requests 
(\(\beta_6\)) and the total length of finished requests (\(\beta_7\)).
\end{enumerate}

The coefficients \(\{\beta_k\}\) absorb model/hardware constants
(layers, heads, width, precision, kernel efficiency).


\section{Estimation}
\label{sec:estimation}

\subsection{Base Estimation with Linear Regression}
\label{subsec:base-estimation-linear-regression}

Equation (\ref{eq:busy-loop-expanded}) is a quadratic function of  
$\boldsymbol{\chi^{\text{miss}}}$ and $\boldsymbol{\chi^{\text{hit}}}$; however,  
it is a linear function of $\boldsymbol{\alpha}$ 
and $\boldsymbol{\beta}$. This enables the estimation of $\boldsymbol{\alpha}$ 
and $\boldsymbol{\beta}$ through linear regression as described
in Scenario 3.

\subsubsection{Scenario 3}
\label{subsubsec:scenario3}

Scenario~3 involves generating request quadruples (``quads'').  

\paragraph*{Setup.}
Unless otherwise specified, all lengths are in tokens. 
Fix a chunk size $C\in\{256,512,1024,2048,4096\}$, 
a block size $b\in\{8,16,32\}$, and maximum sequence length 
$M = 8192$ blocks.

Each quad $i$ consists of four requests $(i,1)$, $(i,2)$, $(i,3)$, and $(i,4)$.
Requests within a quad may share prefixes, while requests across quads do not. 
Requests are issued strictly sequentially; a request 
is emitted by the client only after it receives the response to the previous request:
$\ldots \to (i-1, 4) \to (i,1) \to (i,2) \to (i,3) \to (i,4) \to (i + 1,1) \to \ldots$ 

The input length, output length, and the content of the requests are as follows.

\begin{enumerate}
  \item $(i,1)$ is a prefill-only request with input length 
        $4 + \Delta_{i,1}$. Its first four tokens are distinct from the 
        first four tokens of any request in quads $\{1, 2, \ldots, i-1 \}$. 
        This guarantees that request $(i, 1)$ does not share a prefix with any 
        previously generated requests. Its maximum output length is set to $1$.
  \item $(i,2)$ is a prefill-only request with two input segments, of total length 
        $5 + \Delta_{i,1} + \Delta_{i,2}$. 
        The first segment is identical to $(i,1)$; the second has length 
        $1 + \Delta_{i,2}$. Its maximum output length is set to $1$.
  \item The input sequence of $(i,3)$ is identical to that of $(i,1)$. 
        Its maximum output length is set to $1 + \Delta_{i,3}$.
  \item $(i,4)$ has two input segments. The first segment is identical to $(i,1)$.  
        The second segment has length $1 + \Delta_{i,2}$, but its first token  
        differs from the first token of the second segment in $(i,2)$. 
        This guarantees that request $(i, 4)$ shares a prefix of length exactly 
        $4 + \Delta_{i,1}$ with other requests in this quad. Its maximum output length 
        is set to $1 + \Delta_{i,3}$ blocks.
\end{enumerate}

The structure of the requests in the quad is summarized in the following table.

\begin{table}[h]
\centering
\caption{Structure of requests in quad in Scenario 3; ``='' indicates a segment 
that is identical to the $(i,1)$ input sequence.}
\label{tab:quad-structure}
\begin{tabular}{@{}c|c|c|c@{}}
\toprule
Request & Input Segments & Total Input & Output (max) \\ \midrule
$(i,1)$ & $[4 + \Delta_{i,1}]$ 
        & $4 + \Delta_{i,1}$ 
        & 1 (prefill only) \\
$(i,2)$ & $[= (i,1)], [1 + \Delta_{i,2}]$ 
        & $5 + \Delta_{i,1} + \Delta_{i,2}$ 
        & 1 (prefill only) \\
$(i,3)$ & $[= (i,1)]$ 
        & $4 + \Delta_{i,1}$ 
        & $1 + \Delta_{i,3}$ \\
$(i,4)$ & $[= (i,1)], [1 + \Delta_{i,2}]$
        & $5 + \Delta_{i,1} + \Delta_{i,2}$ 
        & $1 + \Delta_{i,3}$ \\ \bottomrule
\end{tabular}
\end{table}

\paragraph*{Sampling.}
Requests in a quad have a maximum length budget of $M$ that must be divided across input segments and output, 
with variability. We therefore sample the $\Delta$s using a Dirichlet–Multinomial: 
a Dirichlet distribution enforces the simplex constraint and provides a 
single concentration knob to control balanced vs. skewed splits, and the Multinomial yields integer lengths. 
This also induces realistic negative correlations between parts (e.g., longer Segment~1 leaves fewer tokens 
for Segment~2 or output). The sampler is given in Algorithm~\ref{algo:sc3sampler}.

\begin{algorithm}[th]
  \caption{Segment lengths sampler in Scenario 3 (Dirichlet–Multinomial)}
  \label{algo:sc3sampler}
  \begin{algorithmic}[1]
    \Require Budget $N = M - 6$; concentration $\boldsymbol{\eta}=(\eta_1, \eta_2, \eta_3)$
    \State Sample proportions $\mathbf{p} \sim \mathrm{Dirichlet}(\boldsymbol{\eta})$
    \State Sample counts $(\Delta_{i,1}, \Delta_{i,2}, \Delta_{i,3}) \sim \mathrm{Multinomial}\left(N, \mathbf{p}\right)$
    \State \textbf{return} $\Delta_{i,1}, \Delta_{i,2}, \Delta_{i,3}$
  \end{algorithmic}
\end{algorithm}

\begin{theorem}
\label{theo:scenario3}
Define the following quantities.
\begin{align*}
& q_1 = \left\lfloor \frac{4 + \Delta_{i,1}}{C} \right\rfloor  
& r_1 = \left\lceil \frac{4 + \Delta_{i,1}}{C} \right\rceil 
& s_1 = \frac{4 + \Delta_{i,1}}{C} - q_1 & & s_1 \in [0,1) \nonumber \\
& q_2 = \left\lfloor \frac{1 + \Delta_{i,2}}{C} \right\rfloor
& r_2 = \left\lceil \frac{1 + \Delta_{i,2}}{C} \right\rceil
& s_2 = \frac{1 + \Delta_{i,2}}{C} - q_2 & & s_2 \in [0,1) \nonumber \\
& q_4 = \left\lfloor \frac{1 + \Delta_{i,2}}{C} \right\rfloor
& r_4 = \left\lceil \frac{1 + \Delta_{i,2}}{C} \right\rceil
& s_4 = \frac{1 + \Delta_{i,2}}{C} - q_4 & & s_4 \in [0,1)
\end{align*}
Recall that $\ell_i^{\text{out}}$ is the realized output length. 
The end-to-end latencies in Scenario 3 can be expressed as follows.
\begin{align}
e_{i,1} & \approx \alpha_0
  + \alpha_1 (4 + \Delta_{i,1})
  + \alpha_2 \nonumber\\
& + \beta_0 r_1
  + \beta_2 (4 + \Delta_{i,1})
  + \beta_3 C^2 \left(\frac{q_1 (q_1 + 1)}{2} + q_1 s_1 + s_1^2\right)
  + \beta_6
  + \beta_7 (5 + \Delta_{i,1}) \label{eq:req1-latency} \\
e_{i,2} & \approx \alpha_0
  + \alpha_1 \big(5 + \Delta_{i,1} + \Delta_{i,2}\big)
  + \alpha_2 \nonumber\\
& + \beta_0 r_2
  + \beta_1 (4 + \Delta_{i,1})
  + \beta_2 (1 + \Delta_{i,2}) \nonumber\\
& + \beta_3 \Big(
      (4 + \Delta_{i,1}) (1 + \Delta_{i,2})
      + C^2 \big(\tfrac{q_2(q_2+1)}{2} + q_2 s_2 + s_2^2\big)
    \Big)
  + \beta_6
  + \beta_7 \big(6 + \Delta_{i,1} + \Delta_{i,2}\big)
\label{eq:req2-latency} \\
e_{i,3} & \approx \alpha_0
  + \alpha_1 \big(4 + \Delta_{i,1}\big)
  + \alpha_2 \ell_i^{\text{out}} \nonumber \\
& + \beta_0 \ell_i^{\text{out}}
  + \beta_1 \big(4 + \Delta_{i,1}\big) \nonumber \\
&  + \beta_4 \ell_i^{\text{out}}
  + \beta_5 \cdot \frac{\ell_i^{\text{out}} \big( 2(4 + \Delta_{i,1}) + \ell_i^{\text{out}} - 1 \big)}{2}
  + \beta_6
  + \beta_7 \big(4 + \Delta_{i,1} + \ell_i^{\text{out}}\big)
\label{eq:req3-latency} \\
e_{i,4} & \approx \alpha_0
  + \alpha_1 \big(5 + \Delta_{i,1} + \Delta_{i,2}\big)
  + \alpha_2 \ell_i^{\text{out}} \nonumber\\
& + \beta_0 (r_4 + \ell_i^{\text{out}} - 1)
  + \beta_1 \big(4 + \Delta_{i,1}\big)
  + \beta_2 \big(1 + \Delta_{i,2}\big) \nonumber\\
& + \beta_3 \Big(
      (4 + \Delta_{i,1}) (1 + \Delta_{i,2})
      + C^2 \big(\tfrac{q_4(q_4+1)}{2} + q_4 s_4 + s_4^2\big)
    \Big) \nonumber\\
& + \beta_4 (\ell_i^{\text{out}} - 1)
  + \beta_5 \cdot \frac{(\ell_i^{\text{out}} - 1)\big(2(5 + \Delta_{i,1} + \Delta_{i,2}) + (\ell_i^{\text{out}} - 1) - 1\big)}{2} \nonumber\\
& + \beta_6
  + \beta_7 \big(5 + \Delta_{i,1} + \Delta_{i,2} + \ell_i^{\text{out}}\big)
\label{eq:req4-latency}
\end{align}

\end{theorem}

\begin{proof}[\textbf{Proof of \eqref{eq:req1-latency}}.]
Start from the end-to-end model \eqref{eq:e2e-param}, where the busy-loop
contribution is $\sum_j \lambda_j$ over the iterations in which $(i,1)$ is active.

\emph{(i) Prefill-only structure.}
Request $(i,1)$ shares no prefix with earlier requests by construction, so all its
input tokens are uncached (misses). Since it is prefill-only, it participates in
exactly $r_1 = \lceil (4 + \Delta_{i,1})/C \rceil$ prefill iterations and 
no decode iterations.
Thus, the fixed per-iteration cost contributes $\beta_0 r_1$, while the first-prefill
hit term contributes $0$ (there are no hits in the first chunk).

\emph{(ii) Linear per-miss work.}
All $4 + \Delta_{i,1}$ input tokens are new, giving the linear term
$\beta_2 (4 + \Delta_{i,1})$.

\emph{(iii) Quadratic prefill work.}
Let the prefill be split into $q_1$ full chunks of size $C$ and one final partial
chunk of size $s_1 C$ (with $s_1 = 0$ meaning no partial chunk). 
In chunk $t=1, \dots, q_1$,
the request has $\chi^{\text{miss}} = C$ new tokens and $\chi^{\text{hit}} = (t-1)C$
cached tokens from earlier chunks of the \emph{same} request. Hence
\[
\chi^{\text{miss}}\chi^{\text{hit}} + (\chi^{\text{miss}})^2
= C \cdot (t-1)C + C^2 = C^2 t
\]
Summing over $t = 1$ to $q_1$ yields $C^2 \cdot \frac{q_1(q_1 + 1)}{2}$. 
If $s_1 > 0$, the last
partial chunk has $\chi^{\text{miss}} = s_1 C$ and $\chi^{\text{hit}} = q_1 C$, 
contributing $C^2(q_1 s_1 + s_1^2)$. Together these give the quadratic term
$\beta_3 C^2 \left( \frac{q_1(q_1 + 1)}{2} + q_1 s_1 + s_1^2 \right)$.

\emph{(iv) Decode and completion terms.}
There is no decode for a prefill-only request, so the $\beta_4$ and $\beta_5$
terms are absent. The request finishes at the end of its last prefill iteration,
so exactly one completion event occurs, contributing $\beta_6$ plus a cost that
scales with the finished request’s context length. Using \eqref{eq:busy-loop-expanded},
this adds $\beta_7 L^{\text{fin}}$, and for $(i,1)$, including the single output 
token at finish, $L^{\text{fin}} = 5 + \Delta_{i,1}$, giving $\beta_7 (5 + \Delta_{i,1})$.

\emph{(v) Ingress/egress and API costs.}
The linear ingress/egress/API terms contribute
$\alpha_0 + \alpha_1(4 + \Delta_{i,1}) + \alpha_2$ (the last because the maximum
output is $1$).

Collecting all nonzero contributions gives \eqref{eq:req1-latency}.
\end{proof}

\begin{proof}[\textbf{Proof of \eqref{eq:req2-latency}}.]
Start from \eqref{eq:e2e-param}. Since requests are issued sequentially, 
$(i,2)$ experiences no queueing from other requests; its busy-loop contribution 
is the sum of $\lambda_j$ over iterations in which $(i,2)$ is in prefill.

\emph{(i) Prefill structure and iteration count.}
Request $(i,2)$ is prefill-only with a two-segment input: 
a cached prefix of length $4 + \Delta_{i,1}$ (identical to $(i,1)$) and 
a new segment of length $1 + \Delta_{i,2}$. Only the \emph{new} tokens are chunked, 
so the number of prefill iterations is $r_2 = \lceil (1 + \Delta_{i,2})/C \rceil$, 
giving the fixed cost $\beta_0 r_2$. Because cached tokens are already present at the 
first prefill iteration, the first-chunk hit term contributes $\beta_1 (4 + \Delta_{i,1})$.

\emph{(ii) Linear per-miss work.}
Exactly $1 + \Delta_{i,2}$ tokens are new, yielding the 
linear term $\beta_2 (1 + \Delta_{i,2})$.

\emph{(iii) Quadratic prefill work.}
Split the new segment into $q_2$ full chunks of size $C$ and one 
final partial chunk of size $s_2C$ (with $s_2 = 0$ implying no partial chunk).
For a full chunk $t=1, \ldots, q_2$, the new tokens (misses) are $C$ and the 
cached tokens (hits) are $(4 + \Delta_{i,1}) + (t-1)C$ (the prefix plus prior new tokens). 
Hence the quadratic contribution per full chunk is
\[
\chi^{\text{miss}}\chi^{\text{hit}} + (\chi^{\text{miss}})^2
= C\big((4 + \Delta_{i,1}) + (t-1)C\big) + C^2
= C(4 + \Delta_{i,1}) + C^2 t.
\]
Summing over $t=1$ to $q_2$ gives $C(4 + \Delta_{i,1}) q_2 + C^2 \tfrac{q_2(q_2 + 1)}{2}$.
If $s_2 > 0$, the partial chunk contributes
$s_2 C \cdot ((4 + \Delta_{i,1}) + q_2C) + (s_2C)^2
= C (4 + \Delta_{i,1}) s_2 + C^2 (q_2 s_2 + s_2^2)$.
Combining full and partial chunks,
\[
\sum \big(\chi^{\text{miss}}\chi^{\text{hit}} + (\chi^{\text{miss}})^2\big)
= (4 + \Delta_{i,1})(1 + \Delta_{i,2}) + C^2 \Big(\tfrac{q_2 (q_2 + 1)}{2} + q_2 s_2 + s_2^2 \Big)
\]
which yields the $\beta_3$ term in \eqref{eq:req2-latency}.

\emph{(iv) Decode and completion terms.}
There is no decode, so $\beta_4, \beta_5$ do not appear. 
The request finishes at the end of its last prefill iteration, 
contributing one completion event ($\beta_6$) and a length-proportional cost 
$\beta_7 L^{\text{fin}}$. At finish, the context contains all input tokens 
plus the single output token generated at prefill completion, 
i.e., $L^{\text{fin}} = (5 + \Delta_{i,1} + \Delta_{i,2}) + 1 = 6 + \Delta_{i,1} + \Delta_{i,2}$.

\emph{(v) Ingress/egress and API costs.}
These contribute $\alpha_0 + \alpha_1 (5 + \Delta_{i,1} + \Delta_{i,2}) + \alpha_2$.

Collecting all contributions gives \eqref{eq:req2-latency}.
\end{proof}

\begin{proof}[\textbf{Proof of \eqref{eq:req3-latency}}.]
Start from \eqref{eq:e2e-param}. The input of $(i,3)$ equals $(i,1)$, so the entire
prefix of length $4 + \Delta_{i,1}$ is cached.

\emph{(i) First-iteration hit cost.}
The engine must account for cached tokens
once when the request first participates in the busy loop (a decode iteration here).
This yields the hit term $\beta_1 (4 + \Delta_{i,1})$.

\emph{(ii) No prefill-miss work.}
There are no prefill iterations for $(i,3)$, so the $\beta_2$ and $\beta_3$ terms
are absent.

\emph{(iii) Decode iterations.}
The request produces exactly $\ell_i^{\text{out}}$ tokens via decode, so there are
$\ell_i^{\text{out}}$ iterations, contributing $\beta_0 \ell_i^{\text{out}}$ from fixed cost
and $\beta_4 \ell_i^{\text{out}}$ from per-request overhead.

\emph{(iv) Decode context-length work.}
Decode iteration $t=0,\ldots,\ell_i^{\text{out}}-1$ sees context
$ (4 + \Delta_{i,1}) + t$. Hence
\[
\sum \ell_{j,i}
= \sum_{t=0}^{\ell_i^{\text{out}} - 1} \big(4 + \Delta_{i,1} + t\big)
= \frac{\ell_i^{\text{out}} \big( 2(4 + \Delta_{i,1}) + \ell_i^{\text{out}} - 1 \big)}{2},
\]
which yields the $\beta_5$ term in \eqref{eq:req3-latency}.

\emph{(v) Completion work.}
Exactly one completion occurs, contributing $\beta_6$ and a length-proportional cost
with $L^{\text{fin}} = (4 + \Delta_{i,1}) + \ell_i^{\text{out}}$, giving
$\beta_7 \big(4 + \Delta_{i,1} + \ell_i^{\text{out}}\big)$.

\emph{(vi) Ingress/egress and API costs.}
These contribute $\alpha_0 + \alpha_1 (4 + \Delta_{i,1}) + \alpha_2 \ell_i^{\text{out}}$.

Collecting terms gives \eqref{eq:req3-latency}.
\end{proof}

\begin{proof}[\textbf{Proof of \eqref{eq:req4-latency}}.]
Start from \eqref{eq:e2e-param}. Request $(i,4)$ has a cached prefix of length
$4+\Delta_{i,1}$ (identical to $(i,1)$), followed by a new segment of length
$1+\Delta_{i,2}$, and it generates $\ell_i^{\text{out}}$ output tokens.

\emph{(i) Prefill structure and iteration count.}
Only the \emph{new} segment is chunked, so the number of prefill iterations is
$r_4=\lceil(1+\Delta_{i,2})/C\rceil$, contributing the fixed cost $\beta_0 r_4$.
Because cached tokens are present from the first prefill iteration, the first-iteration
hit term contributes $\beta_1(4+\Delta_{i,1})$.

\emph{(ii) Linear per-miss work.}
Exactly $1+\Delta_{i,2}$ tokens are new, yielding $\beta_2(1+\Delta_{i,2})$.

\emph{(iii) Quadratic prefill work.}
Split the new segment into $q_4$ full chunks of size $C$ and a final partial of size
$s_4C$ (when $s_4=0$ there is no partial). For a full chunk $t=1,\ldots,q_4$,
$\chi^{\text{miss}}=C$ and $\chi^{\text{hit}}=(4+\Delta_{i,1})+(t-1)C$, so
\[
\chi^{\text{miss}}\chi^{\text{hit}} + (\chi^{\text{miss}})^2
= C(4+\Delta_{i,1}) + C^2 t.
\]
Summing full chunks gives $C(4+\Delta_{i,1})q_4 + C^2\frac{q_4(q_4+1)}{2}$.
If $s_4>0$, the partial contributes $C(4+\Delta_{i,1})s_4 + C^2(q_4 s_4 + s_4^2)$.
Combining, the quadratic work equals
\[
(4+\Delta_{i,1})(1+\Delta_{i,2})
+ C^2\!\left(\tfrac{q_4(q_4+1)}{2} + q_4 s_4 + s_4^2\right),
\]
yielding the $\beta_3$ term.

\emph{(iv) Decode iterations.}
Since the first output token is produced at the end of prefill, decode runs for
$\ell_i^{\text{out}}-1$ iterations. This contributes $\beta_0(\ell_i^{\text{out}}-1)$
from fixed cost and $\beta_4(\ell_i^{\text{out}}-1)$ from per-request overhead.

\emph{(v) Decode context-length work.}
Decode iteration $t=0,\ldots,\ell_i^{\text{out}}-2$ sees context
$ (5+\Delta_{i,1}+\Delta_{i,2}) + t$, so
\[
\sum \ell_{j,i}
= \sum_{t=0}^{\ell_i^{\text{out}}-2} \big(5+\Delta_{i,1}+\Delta_{i,2}+t\big)
= \frac{(\ell_i^{\text{out}}-1)\big(2(5+\Delta_{i,1}+\Delta_{i,2}) + (\ell_i^{\text{out}}-1) - 1\big)}{2},
\]
giving the $\beta_5$ term.

\emph{(vi) Completion work.}
Exactly one completion occurs, contributing $\beta_6$ and a length-proportional cost
with $L^{\text{fin}}=(5+\Delta_{i,1}+\Delta_{i,2})+\ell_i^{\text{out}}$, hence the
$\beta_7$ term shown.

\emph{(vii) Ingress/egress and API costs.}
These contribute $\alpha_0 + \alpha_1(5+\Delta_{i,1}+\Delta_{i,2})
+ \alpha_2 \ell_i^{\text{out}}$.

Collecting all terms yields \eqref{eq:req4-latency}.
\end{proof}


\subsection{Refinements using Blackbox Optimization}
\label{subsec:refinements-blackbox-optimization}

\subsubsection{Scenario 4: Stress Workloads for Identifiability}
\label{subsubsec:scenario4}

\paragraph*{Why Scenario~4?}
In Scenario~3 we issued requests strictly one after another. 
This was useful for controlled analysis, but it makes some coefficients in 
(\ref{eq:busy-loop-expanded}) hard to tell apart. 
For example, the fixed per-iteration cost $\beta_0$ and the 
per-finish cost $\beta_6$ always appear together, so 
their effects ``bunch up.'' 
Scenario~4 is designed to break such correlations by mixing 
different kinds of requests \emph{concurrently}.

\paragraph*{Key idea.}
We want situations where:
\begin{enumerate}
  \item Busy loop iterations run for a long time \emph{without any finishes}
        (so we can clearly see $\beta_0$, $\beta_4$, $\beta_5$).
  \item Bursts of finishes happen with very little else going on
        (so we can clearly see $\beta_6$, $\beta_7$).
  \item Large uncached segments appear with different chunk sizes
        (so we can measure quadratic prefill effects $\beta_2$, $\beta_3$).
\end{enumerate}

\paragraph*{How to craft the workload.}
We design ``episodes'' that deliberately excite different parts of the latency model:
\begin{itemize}
  \item \textbf{Episode A (Decode plateaus):} Launch several long-output decode requests with cached prefixes. 
  This produces many iterations with large decode load but no completions --- a clean signal for $\beta_0$, $\beta_4$, and $\beta_5$.
  \item \textbf{Episode B (Finish spikes):} Launch many short-output requests close together. 
  Their completions cluster, producing spikes that isolate $\beta_6$ and $\beta_7$.
  \item \textbf{Episode C (Quadratic prefills):} Launch requests with long uncached segments under varying chunk sizes. 
  This creates strong quadratic work that identifies $\beta_2$ and $\beta_3$.
  \item \textbf{Episode D (Controls):} Run one request at a time. 
  These anchor the fixed overhead terms $\alpha_0$ and $\beta_0$.
\end{itemize}

By mixing these episodes and varying chunk sizes $C$, block sizes $b$, and output lengths,
we obtain diverse situations where the different cost terms can be estimated separately.

\vspace{0.75em}
\noindent\textbf{Takeaway.}  
Unlike Scenario~3, requests in Scenario~4 should \emph{not} be strictly sequential.  
They should overlap and stress the system in different ways to tease apart 
the contributions of each coefficient.

\subsection{Blackbox Optimization after Regression}
\label{subsec:blackbox-after-ols}

\paragraph*{Step 1: Linear regression as a baseline.}
We first fit coefficients $(\alpha, \beta)$ using regression, 
because (\ref{eq:e2e-param}) and (\ref{eq:busy-loop-expanded}) are linear in the coefficients. 
This gives us good initial values quickly.

\paragraph*{Step 2: Refine with blackbox optimization.}
In reality, our predictor never matches the true system exactly:
queueing noise, kernel implementations, and clock jitter 
all introduce extra effects. 
So we refine coefficients by treating the simulator + hardware as a 
\emph{blackbox function}: input = coefficients, output = prediction error. 

We want provably good algorithms that work even when we only observe noisy end-to-end latencies.

\paragraph*{Provably convergent choice: SPSA.}
A simple but powerful option is \emph{Simultaneous Perturbation Stochastic Approximation (SPSA)}:
\begin{itemize}
  \item At each step, we perturb all coefficients in random directions (e.g., $\pm 1$ signs).
  \item We measure the loss (prediction error) at two points: ``plus perturbation'' and ``minus perturbation.''
  \item From these two measurements we estimate a gradient and update all coefficients at once.
\end{itemize}
SPSA needs only two measurements per step \emph{regardless of how many coefficients we have}, 
and it comes with convergence guarantees under standard step-size rules. 
This makes it practical and theoretically sound.

\paragraph*{Other practical options.}
In addition to SPSA:
\begin{itemize}
  \item \textbf{Zeroth-order SGD:} Similar to SPSA but uses Gaussian perturbations; often smoother.
  \item \textbf{BOBYQA/Cobyla:} Derivative-free optimizers that work well with box constraints.
  \item \textbf{Bayesian optimization:} Useful if we reparameterize to a low dimension.
  \item \textbf{CMA-ES / NES:} Robust global search strategies when the loss landscape has many local minima.
\end{itemize}

\paragraph*{Practical details.}
\begin{itemize}
  \item Use regression estimates as the starting point.
  \item Impose box constraints (e.g., $\beta_2,\beta_3,\beta_5,\beta_6,\beta_7 \ge 0$).
  \item Average losses over repeated runs of Scenario~4 episodes to reduce noise.
  \item Early stop when validation episodes stop improving.
\end{itemize}

\paragraph*{Summary.}
The strategy is:
\begin{enumerate}
  \item Design workloads (Scenario~4) that disentangle the cost terms.
  \item Run linear regression for a fast baseline estimate.
  \item Apply blackbox optimization (e.g., SPSA) for refinement, 
  which provably converges using only end-to-end latency observations.
\end{enumerate}
This combination balances practicality (OLS initialization), 
theory (provable blackbox convergence), and engineering usability.

\fi

\section{Trace-Only Estimation of Step-Level Execution Coefficients}
\label{sec:step_beta_estimation}

This section presents a trace-only methodology for estimating
\emph{step-level} execution coefficients of a vLLM-style inference engine.
We proceed in three stages.
First, we introduce a simple but principled baseline estimator that fits
phase durations using time-integrated workload signals.
Second, we refine this estimator using an iterative, step-density--reweighted
procedure inspired by majorization--minimization (MM).
Finally, we discuss robustness, calibration workload design, and practical
limitations.

Throughout, we emphasize trace-only operation: no step boundaries,
per-step timings, or engine instrumentation are assumed.

% ============================================================
\subsection{Problem Setup and Step-Level Model}
\label{sec:problem_setup}

We consider a vLLM-style inference engine that advances execution through
a single logical busy-loop.
Each busy-loop iteration (a \emph{step}) executes one forward-pass cycle
that may process prefill tokens, decode tokens, or both, for a set of
concurrently active requests.
Steps are assumed to execute sequentially.

Let $k$ index busy-loop steps.
We model the wall-clock duration of step $k$ as
\begin{equation}
\Delta t_k
=
\beta_0
+
\beta_1\,T^{\mathrm{pf}}_k
+
\beta_2\,T^{\mathrm{dec}}_k,
\label{eq:step_model}
\end{equation}
where:
\begin{itemize}
  \item $\beta_0 \ge 0$ is a fixed per-step overhead (seconds/step),
  \item $\beta_1,\beta_2 \ge 0$ are per-token costs (seconds/token),
  \item $T^{\mathrm{pf}}_k$ and $T^{\mathrm{dec}}_k$ are the total numbers of
        prefill and decode tokens processed in step $k$ across all requests.
\end{itemize}

This affine step-time model is the fundamental abstraction used by our
simulator. Accurate simulation therefore requires identifying the
coefficient vector $\beta=(\beta_0,\beta_1,\beta_2)$.

\paragraph{What traces provide.}
Production traces do not expose step boundaries or per-step token counts.
Instead, they provide \emph{phase instances} indexed by $i$, each
corresponding to either a prefill phase or a decode phase of a single request.
For each phase instance $i$, the trace provides:
\begin{itemize}
  \item start time $t_{i,s}$ and end time $t_{i,e}$,
  \item observed duration $T_i = t_{i,e}-t_{i,s}$,
  \item a trace-inferred step count $N_i \in \mathbb{N}$, deterministically
        derived from trace-visible token counts and known engine scheduling rules.
        For a decode phase, $N_i$ equals the number of decode tokens (one token per step). 
        For a prefill phase under chunk size $C$, a standard trace-inferred 
        choice is $N_i = \lceil P_i / C \rceil$.
  \item aggregate token counts for the phase.
\end{itemize}

The estimation problem is therefore:
\emph{given only phase-level summaries, estimate the step-level coefficients
$\beta$ in Eq.~\eqref{eq:step_model}.}

% ============================================================
\subsection{Trace-Derived Token Pressures}
\label{sec:pressures}

To relate phase-level observations to step-level execution, we reconstruct
time-varying \emph{token pressures} from trace-visible request overlap.

\paragraph{Token pressures.}
Let
\begin{itemize}
  \item $p^{\mathrm{pf}}(t)$ denote the total number of prefill tokens
        processed \emph{per step} at time $t$,
  \item $p^{\mathrm{dec}}(t)$ denote the total number of decode tokens
        processed \emph{per step} at time $t$.
\end{itemize}
These functions are reconstructed deterministically from traces using
request overlap, known per-request token counts, and the engine’s
scheduling semantics.
They are expressed in units of tokens per step (not tokens per unit time).

% ============================================================
\subsection{Prefill Chunking and Partial-Chunk Correction}
\label{sec:prefill_correction}

Decode contributes exactly one token per active request per step.
Prefill, however, proceeds in chunks of size $C$ tokens per step, except for
the final prefill step of a request, which may process fewer than $C$ tokens.
Naively treating all prefill steps as full chunks introduces systematic bias.

For a request $r$ with prompt length $P_r$ and $N_r$ prefill steps, the final
prefill step processes
\begin{equation}
\rho_r = P_r - C\,(N_r-1),
\qquad \rho_r \in (0,C].
\label{eq:rho_def}
\end{equation}
The missing token mass relative to a full chunk is
\[
\mu_r = C - \rho_r \in [0,C).
\]

\paragraph{Naive full-chunk pressure.}
Let $a_r(t)\in\{0,1\}$ indicate whether request $r$ is in prefill at time $t$,
as determined from trace-visible phase boundaries.
The naive full-chunk prefill pressure is
\begin{equation}
p^{\mathrm{pf}}_{\mathrm{full}}(t)
=
C \sum_r a_r(t).
\end{equation}

\paragraph{End-localized partial-chunk correction.}
The missing mass $\mu_r$ should be subtracted from the prefill pressure
near the end of request $r$’s prefill phase.
Because traces do not expose global step boundaries, we localize this
correction in time using a weighting function.

Let $t_{r,\mathrm{pf\text{-}\mathrm{end}}}$ denote the trace-visible end time of request
$r$’s prefill phase.
Define a nonnegative correction weight $w_r(t)$ supported near
$t_{r,\mathrm{pf\text{-}\mathrm{end}}}$ and normalized such that
\[
\int w_r(t)\,dt = 1.
\]

The refined effective prefill pressure is
\begin{equation}
\tilde p^{\mathrm{pf}}(t)
=
p^{\mathrm{pf}}_{\mathrm{full}}(t)
-
\sum_r \mu_r\, w_r(t).
\label{eq:refined_prefill_pressure}
\end{equation}
This construction preserves total prefill token mass exactly while
localizing the partial-chunk correction near the end of prefill.

\paragraph{Interpretation of the correction weight.}
Although $w_r(t)$ is normalized in wall-clock time, it is interpreted as a
\emph{step-time attribution density}: $w_r(t)$ is chosen proportional to the
local step density so that $\mu_r w_r(t)$ represents missing prefill mass
expressed in units of tokens per step.
Under this interpretation, $\tilde p^{\mathrm{pf}}(t)$ in
Eq.~\eqref{eq:refined_prefill_pressure} remains a well-defined token pressure
(tokens/step), consistent with the step-level execution model.


\paragraph{Uniform redistribution as a special case.}
If $w_r(t)$ is chosen uniform over the prefill interval of request $r$, then
Eq.~\eqref{eq:refined_prefill_pressure} reduces to uniform redistribution of
$\rho_r/C$ across the prefill segment.
This simple choice avoids boundary assumptions but smooths localized effects.

\paragraph{Baseline usage of the correction.}
In the baseline estimator, $\tilde p^{\mathrm{pf}}(t)$ is used only through
time integrals of the form $\int \tilde p^{\mathrm{pf}}(t)\,dt$.
Under uniform $w_r(t)$, this subtracts exactly $\mu_r$ tokens per request from
the integrated prefill exposure, independent of step localization.
Pointwise interpretation of $\tilde p^{\mathrm{pf}}(t)$ is therefore not
required in the baseline.

\paragraph{Step-density--weighted localization.}
In the iterative estimator introduced below, we choose $w_r(t)$ proportional
to the estimated step density, restricted to a short trailing window before
$t_{r,\mathrm{pf\text{-}\mathrm{end}}}$.
This places the correction in \emph{step coordinates} rather than arbitrary
wall time, improving alignment with the underlying execution model.

\subsection{Baseline: Time-Integrated NNLS Estimation}
\label{sec:baseline}

We first present a simple, trace-only baseline estimator that fits phase
durations using time-integrated pressures.
Because pressures are expressed in \emph{tokens per step}, a time integral
$\int p(t)\,dt$ has units $(\text{tokens}/\text{step})\cdot \text{seconds}$.
To form regression features with units of \emph{tokens}---so that
$\beta_1,\beta_2$ retain the interpretation ``seconds per token''---we
convert time integrals into approximate \emph{step integrals} using a
phase-local constant step density.

\paragraph{Phase-local step density.}
For phase instance $i$, let $T_i=t_{i,e}-t_{i,s}$ be its observed duration and
let $N_i\in\mathbb{N}$ be the trace-inferred step count.
We define the (phase-local) step density proxy
\begin{equation}
\widehat\lambda_i \;=\; \frac{N_i}{T_i},
\qquad
\text{units: steps/second}.
\label{eq:baseline_lambda_hat}
\end{equation}
This assumes steps are spread approximately uniformly over the phase window,
and is used \emph{only} to convert time integrals into step-aggregated token
totals in the baseline.

\paragraph{Integrated token exposures (baseline features).}
We define baseline integrated exposures
\begin{align}
A^{\mathrm{pf}}_i
&=
\int_{t_{i,s}}^{t_{i,e}}
\tilde p^{\mathrm{pf}}(t)\,\widehat\lambda_i\,dt,
&
A^{\mathrm{dec}}_i
&=
\int_{t_{i,s}}^{t_{i,e}}
p^{\mathrm{dec}}(t)\,\widehat\lambda_i\,dt,
\label{eq:baseline_integrated_exposures}
\end{align}
which have units of tokens because $\tilde p^{\mathrm{pf}}(t)$ and
$p^{\mathrm{dec}}(t)$ are in tokens/step and $\widehat\lambda_i$ is in
steps/second.

\paragraph{Baseline predictor.}
The baseline predicts phase duration as
\begin{equation}
\widehat T_i(\beta)
=
\beta_0\,N_i
+
\beta_1\,A^{\mathrm{pf}}_i
+
\beta_2\,A^{\mathrm{dec}}_i,
\label{eq:baseline_predictor}
\end{equation}
where $\beta_0$ has units seconds/step and $\beta_1,\beta_2$ have units
seconds/token, consistent with the step-level model in
Eq.~\eqref{eq:step_model}.

\paragraph{NNLS estimation.}
Given a set of phase instances $\mathcal{I}$, we estimate $\beta$ via
non-negative least squares:
\begin{equation}
\min_{\beta \in \mathbb{R}^3_{\ge 0}}
\sum_{i\in\mathcal{I}}
\left(
\widehat T_i(\beta) - T_i
\right)^2 .
\label{eq:baseline_nnls}
\end{equation}

\paragraph{Limitation of the baseline.}
This estimator remains wall-clock--weighted because $A_i^{\mathrm{pf}}$ and
$A_i^{\mathrm{dec}}$ are formed by integrating over time within each phase,
using the phase-average step density proxy $\widehat\lambda_i$ rather than a
time-varying step density.
As a result, phases that coincide with long-step periods can exert
disproportionate influence, motivating the step-density--reweighted estimator
in Section~\ref{sec:step_density}.


% ============================================================
\subsection{Step-Density Reweighted Estimation}
\label{sec:step_density}

We now introduce an iterative estimator that shifts fitting from wall-clock
weighting toward step-level weighting.

\subsubsection{Local Step-Time Model and Step Density}

Given instantaneous pressures, we model the (latent) step duration at time $t$ as
\begin{equation}
\Delta(t;\beta)
=
\beta_0
+
\beta_1\,\tilde p^{\mathrm{pf}}(t)
+
\beta_2\,p^{\mathrm{dec}}(t),
\qquad \Delta(t;\beta) > 0.
\label{eq:local_step_time}
\end{equation}

Thus, $\Delta(t;\beta)$ represents the instantaneous duration of a busy-loop
step that would be executed at time $t$ given the token pressures observed at
that moment and execution coefficients $\beta$.


The corresponding step density is
\begin{equation}
\lambda(t;\beta) = \frac{1}{\Delta(t;\beta)}.
\label{eq:step_density}
\end{equation}

\subsubsection{Normalized Step Density within a Phase}

For phase instance $i$, define
\[
\Lambda_i(\beta)
=
\int_{t_{i,s}}^{t_{i,e}} \lambda(t;\beta)\,dt,
\qquad
q_i(t;\beta)
=
\frac{\lambda(t;\beta)}{\Lambda_i(\beta)}.
\]
By construction, $\int q_i(t;\beta)\,dt = 1$.
We treat $N_i$ as fixed and use $q_i(t;\beta)$ only as a trace-only
\emph{step-time attribution model} within the observed phase window; 
in other words, $q_i$ estimates where those steps occur in time.

\subsubsection{Step-Averaged Pressures}

Define
\begin{equation}
\bar p^{\mathrm{pf}}_i(\beta)
=
\int_{t_{i,s}}^{t_{i,e}} \tilde p^{\mathrm{pf}}(t)\, q_i(t;\beta)\,dt,
\qquad
\bar p^{\mathrm{dec}}_i(\beta)
=
\int_{t_{i,s}}^{t_{i,e}} p^{\mathrm{dec}}(t)\, q_i(t;\beta)\,dt.
\label{eq:step_averaged_pressures}
\end{equation}

Note that $\bar p_i^{\mathrm{pf}}(\beta)$ retains units of tokens/step, so
$N_i\bar p_i^{\mathrm{pf}}(\beta)$ is an \emph{effective step-aggregated prefill exposure}
for phase $i$ under the step-timing model $\lambda(t;\beta)$.
In the special case of approximately uniform step density within the phase,
this reduces to the baseline exposure $A_i^{\mathrm{pf}}$ (up to numerical integration error).


\subsubsection{MM-Style Iterative NNLS}

At iteration $r$, we freeze the step-averaged pressures computed using
$\beta^{(r)}$ and solve
\begin{equation}
\beta^{(r+1)}
=
\arg\min_{\beta\ge 0}
\sum_{i\in\mathcal{I}}
\left(
N_i\beta_0
+
N_i\bar p^{\mathrm{pf}}_i(\beta^{(r)})\beta_1
+
N_i\bar p^{\mathrm{dec}}_i(\beta^{(r)})\beta_2
-
T_i
\right)^2 .
\label{eq:sr_nnls}
\end{equation}
Optional damping
$\beta^{(r+1)} \leftarrow (1-\eta)\beta^{(r)} + \eta\beta^{(r+1)}$
improves numerical stability. 
We iterate until $\|\beta^{(r+1)} - \beta^{(r)}\|_2 < \epsilon$ or a maximum
iteration count is reached.

% ============================================================
\subsection{Robust Estimation and Fixed-Point View}
\label{sec:robust_fixed_point}

To mitigate outliers, we optionally replace the squared loss with a Huber loss
$\rho_\delta$.
The resulting mapping defines a fixed-point iteration
$\beta^{(r+1)}=\mathcal{F}(\beta^{(r)})$, terminated by relative change or a
fixed iteration budget.

% ============================================================
\subsection{Limitations and Scope}
\label{sec:limitations}

This approach assumes a single sequential busy-loop and accurate knowledge of
engine scheduling rules.
Partial-chunk corrections remain approximate due to unobserved step
boundaries.
Identifiability requires sufficient variation in token pressures.
Despite these limitations, the method provides a practical and non-intrusive
path to step-level calibration using only production traces.

\paragraph{Simulator linkage.}
The estimated coefficients $\beta^\star$ are used directly in the step-time
model (Eq.~\ref{eq:step_model}) of the discrete-event simulator.


% ============================================================
\subsection{Design of Inference Workloads for Calibration}
\label{sec:workload_design}

Identifiability of step-level execution coefficients requires sufficient
variation in instantaneous token pressures.
Highly homogeneous workloads may yield nearly collinear features, limiting
estimation precision even with large trace volumes.
We therefore explicitly design inference workloads that \emph{excite}
distinct execution regimes during calibration windows.

\subsubsection{Calibration Probes}

We construct a small set of \emph{calibration probes}, each defined by a
controlled combination of:
\begin{itemize}
  \item request arrival rate and burstiness,
  \item prompt length distribution,
  \item decode length distribution,
  \item maximum concurrency or batch limits.
\end{itemize}

These probes are executed using standard benchmarking or workload-generation
frameworks and produce ordinary production traces. No engine instrumentation,
step timing, or internal counters are used.

\subsubsection{Feature Diversity Objective}

Let $\phi_i(\beta)$ denote the step-resolved feature vector for phase $i$:
\[
\phi_i(\beta)
=
\left(
N_i,\;
N_i\bar p^{\mathrm{pf}}_i(\beta),\;
N_i\bar p^{\mathrm{dec}}_i(\beta)
\right).
\]

Calibration probes are selected to maximize diversity among these features
across phases, reducing collinearity between prefill, decode, and step-count
terms. In practice, this can be achieved by:
\begin{itemize}
  \item mixing prompt-heavy and decode-heavy workloads,
  \item varying concurrency independently of token lengths,
  \item introducing controlled arrival-rate ramps or bursts.
\end{itemize}

\subsubsection{Design-of-Experiments Perspective}

From a design-of-experiments viewpoint, probe selection aims to improve the
conditioning of the regression problem.
Simple heuristics include:
\begin{itemize}
  \item maximizing the determinant or minimizing the condition number of the
        empirical feature covariance matrix,
  \item ensuring coverage of distinct operating regimes
        (e.g., low-load vs.\ saturation).
\end{itemize}

These criteria are evaluated using trace-derived features only and do not
require knowledge of the true execution coefficients. These criteria 
depend only on aggregate variation in step-level feature magnitudes and 
are insensitive to the specific treatment of partial prefill
chunks, which affects only localized mass attribution within phases.

\paragraph{Separation of Concerns.}
Workload generators and benchmarking frameworks are used solely to induce
pressure variability during calibration. Estimation of step-level execution
coefficients relies exclusively on trace-visible phase timing and token
counts, preserving the trace-only and non-intrusive nature of the method.

% ============================================================
\subsection{Justification and Scope}
\label{sec:extended_justification}

Robust loss functions address unavoidable noise and outliers in production
traces without altering the underlying execution model.
The fixed-point formulation clarifies how step-density reweighting reconciles
time-based observations with step-based costs.
Finally, explicit workload design ensures that the estimation problem is
well-conditioned by construction, rather than relying on incidental workload
variation.

Together, these extensions strengthen the estimator’s practical reliability
and interpretability while preserving its central property: step-level
execution coefficients are inferred using only trace-visible information,
without engine instrumentation or per-step timing.

\begin{table}[t]
\centering
\caption{Summary of key symbols used in step-level coefficient estimation.}
\label{tab:symbol_glossary}
\begin{tabular}{lll}
\toprule
Symbol & Description & Units \\
\midrule
$\beta_0$ & Per-step fixed overhead & seconds / step \\
$\beta_1$ & Prefill token cost & seconds / token \\
$\beta_2$ & Decode token cost & seconds / token \\
$T_i$ & Observed phase duration & seconds \\
$N_i$ & Trace-inferred step count & steps \\
$p^{\mathrm{pf}}(t)$ & Prefill token pressure & tokens / step \\
$p^{\mathrm{dec}}(t)$ & Decode token pressure & tokens / step \\
$C$ & Prefill chunk size & tokens \\
$\rho_r$ & Tokens in final prefill chunk & tokens \\
$\mu_r$ & Missing prefill chunk mass & tokens \\
$\Delta(t;\beta)$ & Local step duration model & seconds / step \\
$\lambda(t;\beta)$ & Step density & steps / second \\
\bottomrule
\end{tabular}
\end{table}


\section{Evaluation}
\label{sec:evaluation}

\section{Implementation}
\label{sec:implementation}

\section{Limitations of our Approach}
\label{sec:limitations}

Our current approach has several important limitations:

\begin{enumerate}
    \item \textbf{Hardware and model specificity.}
    BLIS is calibrated for a fixed combination of GPU type and model
    architecture. We do not attempt to generalize across different hardware or
    models. If either changes, BLIS must be retrained with new data.

    \item \textbf{No pre-emption.}
    We do not model pre-emption of running requests. In practice, triggering
    pre-emption during request handling is rare and avoided by the scheduler
    logic, but its absence in our model means we cannot accurately predict
    latency under pre-emptive scenarios.

    \item \textbf{No speculative decoding.}
    We currently omit speculative decoding mechanisms (such as those recently
    introduced in vLLM as an experimental feature).
\end{enumerate}

\section{Future Work}
\label{sec:future-work}

\section{Conclusion}
\label{sec:conclusion}

\nocite{*}
\bibliographystyle{plain}
\bibliography{blis}

\appendix 

\section{Theory-to-Code Mapping for Baseline Estimation}
\label{sec:baseline_theory_to_code}

We infer per-step execution costs by reconstructing how much prefill and decode
work overlaps in time during each phase, correcting for prefill chunking,
converting wall-clock exposure into step-level token exposure, and fitting a
nonnegative linear regression using only trace-visible information.

\paragraph{Key ideas and implementation mapping.}
The baseline estimator follows six conceptual steps, each of which maps directly
to a small, well-defined portion of the reference implementation.

\begin{enumerate}
  \item \textbf{Estimate step-level costs from phase-level traces.} \\
  The goal is to estimate coefficients $(\beta_0,\beta_1,\beta_2)$ in the
  step-time model
  \[
  \Delta t
  =
  \beta_0
  +
  \beta_1\,(\text{prefill tokens})
  +
  \beta_2\,(\text{decode tokens}),
  \]
  even though production traces expose only phase start/end times and token
  counts, not step boundaries. Each trace phase contributes one regression
  equation relating its observed duration to its inferred step-level workload. \\
  \emph{Code mapping:} function \texttt{estimate\_betas\_baseline}, regression
  construction in Section~7 of the implementation.

  \item \textbf{Reconstruct system load via overlap-based token pressures.} \\
  At any moment in time, each active prefill request contributes approximately
  $C$ tokens per step, and each active decode request contributes approximately
  one token per step. When phases overlap in time, their contributions add. This
  yields time-varying token pressure signals for prefill and decode, expressed in
  units of tokens per step. \\
  \emph{Code mapping:} global time grid construction from phase boundaries, and
  pressure arrays \texttt{p\_pf\_full} and \texttt{p\_dec}.

  \item \textbf{Convert wall-clock integrals into step-level exposure.} \\
  Because pressures are measured in tokens per step but traces are measured in
  seconds, we introduce a phase-local step-density proxy
  $\widehat{\lambda}_i = N_i / T_i$ (steps per second), where $N_i$ is the
  trace-inferred step count and $T_i$ is the observed phase duration. Multiplying
  time integrals of pressure by $\widehat{\lambda}_i$ converts them into total
  token exposure. \\
  \emph{Code mapping:} computation of \texttt{lambda\_hat} and use of
  \texttt{integrate\_piecewise\_constant} to form $A_i^{\mathrm{pf}}$ and
  $A_i^{\mathrm{dec}}$.

  \item \textbf{Correct systematic bias from partial prefill chunks.} \\
  Prefill executes in chunks of size $C$ tokens per step, except for the final
  prefill step of a request, which may be smaller. Treating all prefill steps as
  full chunks overcounts work. For each request, we compute the missing token
  mass in the final chunk and subtract it from the integrated prefill exposure,
  redistributing it uniformly over the request's prefill interval. \\
  \emph{Code mapping:} computation of $\rho_r$ and $\mu_r$ for each prefill
  request and integrated correction applied when forming $A_i^{\mathrm{pf}}$.

  \item \textbf{Reduce each phase to a single linear regression row.} \\
  After computing step count and integrated token exposures, each phase instance
  yields one equation of the form
  \[
  T_i
  =
  \beta_0 N_i
  +
  \beta_1 A_i^{\mathrm{pf}}
  +
  \beta_2 A_i^{\mathrm{dec}},
  \]
  where all quantities on the right-hand side are derived from traces. Stacking
  all phases produces a linear system. \\
  \emph{Code mapping:} construction of the design matrix
  \texttt{X = [N\_i, A\_pf\_i, A\_dec\_i]} and target vector \texttt{y = T\_i}.

  \item \textbf{Solve a nonnegative least-squares problem to obtain simulator-ready coefficients.} \\
  The coefficients are estimated by solving a nonnegative least-squares problem,
  enforcing physical constraints that costs cannot be negative. The resulting
  coefficients can be used directly in the simulator's step-time model without
  further calibration. \\
  \emph{Code mapping:} \texttt{LinearRegression(fit\_intercept=False,
  positive=True)} and return of \texttt{BaselineBetaResult}.
\end{enumerate}

\paragraph{Summary.}
The baseline estimator is deliberately simple: it uses only phase-level timing
and token counts, reconstructs overlapping workload via deterministic rules,
applies a minimal bias correction for prefill chunking, and fits a convex
regression problem. This simplicity makes it robust, trace-only, and easy to
audit, while providing step-level coefficients suitable for discrete-event
simulation.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figs/overlap_phases.pdf}
  \caption{\textbf{Overlapping phase intervals and trace-derived quantities.}
  Each horizontal bar represents a trace-visible phase (prefill or decode) 
  of a request plotted on a shared time axis. Vertical dashed 
  lines mark the phase start and end times. The numbers in 
  parentheses indicate the trace-inferred number of busy-loop steps 
  executed during each phase (e.g., decode steps equal output tokens; 
  prefill steps equal chunked prefill iterations).}
  \label{fig:overlap_phases}
\end{figure}


\end{document}
