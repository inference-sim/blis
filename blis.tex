\documentclass[11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{times}
\usepackage{graphicx}
\usepackage{amsmath, amssymb}
\usepackage{bm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{hyperref}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{booktabs,longtable,array,makecell}
\setlength{\LTleft}{0pt}   % no left indent for longtable
\setlength{\LTright}{0pt}  % no right indent for longtable
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}p{#1}}

% --- Theorems, lemmas, claims, proofs ---
\usepackage{amsthm}

% Number theorems within sections; share a counter across theorem-like envs.
\theoremstyle{plain} % bold headings, italic body
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{claim}[theorem]{Claim}

\theoremstyle{definition} % bold heading, upright body
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark} % italic heading, upright body
\newtheorem{remark}[theorem]{Remark}
\newtheorem{note}[theorem]{Note}

% Optional: unnumbered versions
\newtheorem*{theorem*}{Theorem}
\newtheorem*{lemma*}{Lemma}
\newtheorem*{claim*}{Claim}

% Proof environment is provided by amsthm:
% \begin{proof} ... \end{proof}
% Customize QED symbol if you like:
\renewcommand{\qedsymbol}{$\blacksquare$}

% (Optional) If you want equations numbered by section (since you already use amsmath):
\numberwithin{equation}{section}



\title{BLIS: Blackbox Inference Performance Estimator}
\author{AI Platform Optimization Team}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This paper introduces BLIS, a system for blackbox inference performance estimation 
designed to model inference request flows and latency.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

\section{Related Work}
\label{sec:related-work}

\section{Design overview}
\label{sec:design-overview}

BLIS is primarily geared towards vLLM. We will describe extensions for other
inference platforms like sglang in Section \ref{sec:future-work}.

\subsection{vLLM overview}
\label{subsec:vllm-overview}
We will focus on handling of an inference request by vLLM. vLLM
has two main components: the API server and the engine core; these operate as separate threads and
communicate through a message queue.

\subsubsection{API server thread}
The API server is implemented using FastAPI. Its role is to handle client-facing tasks without blocking on actual model execution.
The API thread is responsible for:
\begin{enumerate}
    \item Receiving and tokenizing incoming requests from clients.
    \item Enqueuing tokenized requests into the message queue.
    \item Streaming back partial responses as they become available from the engine.
    \item Detokenizing completed sequences and packaging them into response
          objects.
    \item Emitting the final response back to the client.
\end{enumerate}

\subsubsection{Engine core thread}
The engine core runs the central \emph{inference loop}. This loop is driven by a
scheduler that repeatedly:
\begin{enumerate}
    \item Collects pending requests from the queue, forming a dynamic batch.
    \item Determines whether a request is in a \emph{prefill} (first token)
          or \emph{decode} (subsequent tokens) phase.
    \item Executes the forward pass on the GPU for that batch.
    \item Updates KV cache blocks, manages allocation and eviction, and applies
          prefix caching optimizations when possible.
    \item Returns generated tokens back to the API layer, either for streaming
          or final response.
\end{enumerate}
We will refer to this engine loop as the \emph{busy loop} since it continuously
steps through request-batches at a fine-grained timescale.

\subsection{Design of BLIS}
\label{subsec:blis-design}

\section{Latency model}
\label{sec:latency-model}

\subsection{Request types}
\label{subsubsec:request-types}

Requests can be of two types: \textit{prefill-only} or \textit{decode}. 
Prefill-only requests have their maximum output length set to $1$, and \textit{decode}
requests have this set to a number greater than $1$.

\subsection{Request life-cycle}
\label{subsubsec:request-life-cycle}

\begin{figure}[!ht]
  \centering
  \includegraphics[height=0.8\textwidth]{figs/reqlifecycle.png}
  \caption{Request Lifecycle}
  \label{fig:req-lifecycle}
\end{figure}

The life of an inference request in the system passes through multiple stages 
as  illustrated in Figure \ref{fig:req-lifecycle} and described below.

\paragraph*{1. Ingress.}
The client emits an inference request $i$ to the server. This contributes 
a network latency that depends linearly on the number of input tokens 
$\ell_i^{\text{in}}$.

\paragraph*{2. Pre-process.}
Upon arrival, the API server tokenizes the prompt, performs light
validation, and enqueues the request into the message queue. This contributes 
a latency that depends linearly on $\ell_i^{\text{in}}$.

\paragraph*{3. Scheduling.} The request waits in the queue until there is 
sufficient GPU capacity. During this waiting period, other requests get to participate
in the busy loop iterations.

\paragraph*{4. Chunked prefill.} The uncached portion of this request is 
prefilled one chunk at a time. The latency of this stage equals the sum of the 
duration of the busy loop iterations in which this request prefills.

\paragraph*{5. Decode.} The latency of this stage equals the sum of the duration
of the busy loop iterations in which this request is in its decode phase. 
The first output token is generated during the prefill phase, and subsequent 
tokens are generated in the decode iterations.

\paragraph*{6. Post-process.} Once all output tokens are produced, 
the API server detokenizes them and
packages the final response. The latency of this stage depends linearly on the 
number of output tokens $\ell_i^{\text{out}}$.

\paragraph*{7. Egress.}
The server emits response $i$ to the
client. This contributes a network latency that depends linearly 
on the number of output tokens $\ell_i^{\text{out}}$.

\iffalse
\subsection{End-to-End request latency model}
\label{subsec:end-to-end-latency-model}

Let $\lambda_j$ denote the duration of busy loop iteration $j$. We start
with a high-level model for the end-to-end latency as follows.

\begin{align}
e_i \approx \alpha_0
 + \alpha_1 \ell_i^{\text{in}}
 + \alpha_2 \ell_i^{\text{out}}
 + \sum_{\text{i waits at j}} \lambda_j
 + \sum_{\text{i prefills at j}} \lambda_j
 + \sum_{\text{i decodes at j}} \lambda_j
\label{eq:e2e-param}
\end{align}

\textbf{Note:} When we write $a \approx b$, it is shorthand for $a=b+\varepsilon$ 
where $\varepsilon$ is a residual noise term. Regression equations such 
as \eqref{eq:e2e-param} are understood to hold up to residual noise term.

\subsubsection{Busy loop}
\label{subsubsec:busy-loop-latency-model}
Let $\chi_{j,i}^{\text{hit}}$ and $\chi_{j,i}^{\text{miss}}$ be the numbers of 
cached and uncached (new) tokens of request $i$ during iteration $j$.
Define the following request sets.
\begin{align*}
  \mathcal{P}_j & = & \{i:\text{$i$ is in prefill at $j$}\} \nonumber \\
  \mathcal{B}^{(1)}_j & = & \{i:\text{$i$ is in its first busy loop iteration at $j$}\} \nonumber \\
  \mathcal{D}_j & = & \{i:\text{$i$ is decoding at $j$}\} \nonumber \\
  \mathcal{F}_j & = & \{i:\text{$i$ finishes at $j$}\}
\end{align*}
Let $\ell_{j,i}$ denote the context length of request $i$ at the 
start of iteration $j$.

\paragraph*{Quadratic prefill term.} We define a quadratic term as follows.
\begin{align}
\psi_j
  = \sum_{i \in \mathcal{P}_j}
    \Big(
      \chi_{j,i}^{\text{miss}} \cdot \chi_{j,i}^{\text{hit}}
      + \big(\chi_{j,i}^{\text{miss}}\big)^2
    \Big)
\label{eq:quad}
\end{align}

\paragraph*{Latency model.}
Let $L^{\text{dec}}_j=\sum_{i\in\mathcal{D}_j}\ell_{j,i}$ be the total decode
context length in iteration $j$, and
$L^{\text{fin}}_j=\sum_{i\in\mathcal{F}_j}\ell_{j,i}$ the total context length of
requests that finish in $j$ (a proxy for KV cache tokens to free). 
We model the iteration time as
\begin{align}
\lambda_j 
  \approx \beta_0
  + \beta_1 \sum_{i \in \mathcal{B}^{(1)}_j} \chi_{j,i}^{\text{hit}}
  + \beta_2 \sum_{i \in \mathcal{P}_j}      \chi_{j,i}^{\text{miss}}
  + \beta_3 \psi_j 
  + \beta_4 |\mathcal{D}_j|
  + \beta_5 L^{\text{dec}}_j
  + \beta_6 |\mathcal{F}_j|
  + \beta_7 L^{\text{fin}}_j
\label{eq:busy-loop-expanded}
\end{align}

\paragraph*{Justification.} Each busy-loop iteration mixes four kinds of work.

\begin{enumerate}
  \item \emph{Fixed work.} An iteration-invariant cost (\(\beta_0\)).
  \item \emph{Prefill work.} The first prefill iteration of a request involves hashing
its input tokens to determine the number of cache hits (linear in hits; \(\beta_1\)).
Every new token pays a linear per-token cost (embeddings, MLP, KV writes; \(\beta_2\)).
Attention mechanism during prefill is quadratic: every new token 
(cache miss) must attend to all tokens already in the context (cache hit) 
plus the other new tokens from the same chunk (\(\psi_j\) in~\eqref{eq:quad}; 
\(\beta_3\)).
  \item \emph{Decode work.} Decode generates one token per active request:
there is a per-request overhead (\(\beta_4\)) and compute that scales with
the request’s current context length (\(\beta_5\)).
  \item \emph{Completion work.} When a request completes, we free KV blocks, update
refcounts, and serialize output; this scales linearly in the number of 
finished requests 
(\(\beta_6\)) and the total length of finished requests (\(\beta_7\)).
\end{enumerate}

The coefficients \(\{\beta_k\}\) absorb model/hardware constants
(layers, heads, width, precision, kernel efficiency).


\section{Estimation}
\label{sec:estimation}

\subsection{Base Estimation with Linear Regression}
\label{subsec:base-estimation-linear-regression}

Equation (\ref{eq:busy-loop-expanded}) is a quadratic function of  
$\boldsymbol{\chi^{\text{miss}}}$ and $\boldsymbol{\chi^{\text{hit}}}$; however,  
it is a linear function of $\boldsymbol{\alpha}$ 
and $\boldsymbol{\beta}$. This enables the estimation of $\boldsymbol{\alpha}$ 
and $\boldsymbol{\beta}$ through linear regression as described
in Scenario 3.

\subsubsection{Scenario 3}
\label{subsubsec:scenario3}

Scenario~3 involves generating request quadruples (``quads'').  

\paragraph*{Setup.}
Unless otherwise specified, all lengths are in tokens. 
Fix a chunk size $C\in\{256,512,1024,2048,4096\}$, 
a block size $b\in\{8,16,32\}$, and maximum sequence length 
$M = 8192$ blocks.

Each quad $i$ consists of four requests $(i,1)$, $(i,2)$, $(i,3)$, and $(i,4)$.
Requests within a quad may share prefixes, while requests across quads do not. 
Requests are issued strictly sequentially; a request 
is emitted by the client only after it receives the response to the previous request:
$\ldots \to (i-1, 4) \to (i,1) \to (i,2) \to (i,3) \to (i,4) \to (i + 1,1) \to \ldots$ 

The input length, output length, and the content of the requests are as follows.

\begin{enumerate}
  \item $(i,1)$ is a prefill-only request with input length 
        $4 + \Delta_{i,1}$. Its first four tokens are distinct from the 
        first four tokens of any request in quads $\{1, 2, \ldots, i-1 \}$. 
        This guarantees that request $(i, 1)$ does not share a prefix with any 
        previously generated requests. Its maximum output length is set to $1$.
  \item $(i,2)$ is a prefill-only request with two input segments, of total length 
        $5 + \Delta_{i,1} + \Delta_{i,2}$. 
        The first segment is identical to $(i,1)$; the second has length 
        $1 + \Delta_{i,2}$. Its maximum output length is set to $1$.
  \item The input sequence of $(i,3)$ is identical to that of $(i,1)$. 
        Its maximum output length is set to $1 + \Delta_{i,3}$.
  \item $(i,4)$ has two input segments. The first segment is identical to $(i,1)$.  
        The second segment has length $1 + \Delta_{i,2}$, but its first token  
        differs from the first token of the second segment in $(i,2)$. 
        This guarantees that request $(i, 4)$ shares a prefix of length exactly 
        $4 + \Delta_{i,1}$ with other requests in this quad. Its maximum output length 
        is set to $1 + \Delta_{i,3}$ blocks.
\end{enumerate}

The structure of the requests in the quad is summarized in the following table.

\begin{table}[h]
\centering
\caption{Structure of requests in quad in Scenario 3; ``='' indicates a segment 
that is identical to the $(i,1)$ input sequence.}
\label{tab:quad-structure}
\begin{tabular}{@{}c|c|c|c@{}}
\toprule
Request & Input Segments & Total Input & Output (max) \\ \midrule
$(i,1)$ & $[4 + \Delta_{i,1}]$ 
        & $4 + \Delta_{i,1}$ 
        & 1 (prefill only) \\
$(i,2)$ & $[= (i,1)], [1 + \Delta_{i,2}]$ 
        & $5 + \Delta_{i,1} + \Delta_{i,2}$ 
        & 1 (prefill only) \\
$(i,3)$ & $[= (i,1)]$ 
        & $4 + \Delta_{i,1}$ 
        & $1 + \Delta_{i,3}$ \\
$(i,4)$ & $[= (i,1)], [1 + \Delta_{i,2}]$
        & $5 + \Delta_{i,1} + \Delta_{i,2}$ 
        & $1 + \Delta_{i,3}$ \\ \bottomrule
\end{tabular}
\end{table}

\paragraph*{Sampling.}
Requests in a quad have a maximum length budget of $M$ that must be divided across input segments and output, 
with variability. We therefore sample the $\Delta$s using a Dirichlet–Multinomial: 
a Dirichlet distribution enforces the simplex constraint and provides a 
single concentration knob to control balanced vs. skewed splits, and the Multinomial yields integer lengths. 
This also induces realistic negative correlations between parts (e.g., longer Segment~1 leaves fewer tokens 
for Segment~2 or output). The sampler is given in Algorithm~\ref{algo:sc3sampler}.

\begin{algorithm}[th]
  \caption{Segment lengths sampler in Scenario 3 (Dirichlet–Multinomial)}
  \label{algo:sc3sampler}
  \begin{algorithmic}[1]
    \Require Budget $N = M - 6$; concentration $\boldsymbol{\eta}=(\eta_1, \eta_2, \eta_3)$
    \State Sample proportions $\mathbf{p} \sim \mathrm{Dirichlet}(\boldsymbol{\eta})$
    \State Sample counts $(\Delta_{i,1}, \Delta_{i,2}, \Delta_{i,3}) \sim \mathrm{Multinomial}\left(N, \mathbf{p}\right)$
    \State \textbf{return} $\Delta_{i,1}, \Delta_{i,2}, \Delta_{i,3}$
  \end{algorithmic}
\end{algorithm}

\begin{theorem}
\label{theo:scenario3}
Define the following quantities.
\begin{align*}
& q_1 = \left\lfloor \frac{4 + \Delta_{i,1}}{C} \right\rfloor  
& r_1 = \left\lceil \frac{4 + \Delta_{i,1}}{C} \right\rceil 
& s_1 = \frac{4 + \Delta_{i,1}}{C} - q_1 & & s_1 \in [0,1) \nonumber \\
& q_2 = \left\lfloor \frac{1 + \Delta_{i,2}}{C} \right\rfloor
& r_2 = \left\lceil \frac{1 + \Delta_{i,2}}{C} \right\rceil
& s_2 = \frac{1 + \Delta_{i,2}}{C} - q_2 & & s_2 \in [0,1) \nonumber \\
& q_4 = \left\lfloor \frac{1 + \Delta_{i,2}}{C} \right\rfloor
& r_4 = \left\lceil \frac{1 + \Delta_{i,2}}{C} \right\rceil
& s_4 = \frac{1 + \Delta_{i,2}}{C} - q_4 & & s_4 \in [0,1)
\end{align*}
Recall that $\ell_i^{\text{out}}$ is the realized output length. 
The end-to-end latencies in Scenario 3 can be expressed as follows.
\begin{align}
e_{i,1} & \approx \alpha_0
  + \alpha_1 (4 + \Delta_{i,1})
  + \alpha_2 \nonumber\\
& + \beta_0 r_1
  + \beta_2 (4 + \Delta_{i,1})
  + \beta_3 C^2 \left(\frac{q_1 (q_1 + 1)}{2} + q_1 s_1 + s_1^2\right)
  + \beta_6
  + \beta_7 (5 + \Delta_{i,1}) \label{eq:req1-latency} \\
e_{i,2} & \approx \alpha_0
  + \alpha_1 \big(5 + \Delta_{i,1} + \Delta_{i,2}\big)
  + \alpha_2 \nonumber\\
& + \beta_0 r_2
  + \beta_1 (4 + \Delta_{i,1})
  + \beta_2 (1 + \Delta_{i,2}) \nonumber\\
& + \beta_3 \Big(
      (4 + \Delta_{i,1}) (1 + \Delta_{i,2})
      + C^2 \big(\tfrac{q_2(q_2+1)}{2} + q_2 s_2 + s_2^2\big)
    \Big)
  + \beta_6
  + \beta_7 \big(6 + \Delta_{i,1} + \Delta_{i,2}\big)
\label{eq:req2-latency} \\
e_{i,3} & \approx \alpha_0
  + \alpha_1 \big(4 + \Delta_{i,1}\big)
  + \alpha_2 \ell_i^{\text{out}} \nonumber \\
& + \beta_0 \ell_i^{\text{out}}
  + \beta_1 \big(4 + \Delta_{i,1}\big) \nonumber \\
&  + \beta_4 \ell_i^{\text{out}}
  + \beta_5 \cdot \frac{\ell_i^{\text{out}} \big( 2(4 + \Delta_{i,1}) + \ell_i^{\text{out}} - 1 \big)}{2}
  + \beta_6
  + \beta_7 \big(4 + \Delta_{i,1} + \ell_i^{\text{out}}\big)
\label{eq:req3-latency} \\
e_{i,4} & \approx \alpha_0
  + \alpha_1 \big(5 + \Delta_{i,1} + \Delta_{i,2}\big)
  + \alpha_2 \ell_i^{\text{out}} \nonumber\\
& + \beta_0 (r_4 + \ell_i^{\text{out}} - 1)
  + \beta_1 \big(4 + \Delta_{i,1}\big)
  + \beta_2 \big(1 + \Delta_{i,2}\big) \nonumber\\
& + \beta_3 \Big(
      (4 + \Delta_{i,1}) (1 + \Delta_{i,2})
      + C^2 \big(\tfrac{q_4(q_4+1)}{2} + q_4 s_4 + s_4^2\big)
    \Big) \nonumber\\
& + \beta_4 (\ell_i^{\text{out}} - 1)
  + \beta_5 \cdot \frac{(\ell_i^{\text{out}} - 1)\big(2(5 + \Delta_{i,1} + \Delta_{i,2}) + (\ell_i^{\text{out}} - 1) - 1\big)}{2} \nonumber\\
& + \beta_6
  + \beta_7 \big(5 + \Delta_{i,1} + \Delta_{i,2} + \ell_i^{\text{out}}\big)
\label{eq:req4-latency}
\end{align}

\end{theorem}

\begin{proof}[\textbf{Proof of \eqref{eq:req1-latency}}.]
Start from the end-to-end model \eqref{eq:e2e-param}, where the busy-loop
contribution is $\sum_j \lambda_j$ over the iterations in which $(i,1)$ is active.

\emph{(i) Prefill-only structure.}
Request $(i,1)$ shares no prefix with earlier requests by construction, so all its
input tokens are uncached (misses). Since it is prefill-only, it participates in
exactly $r_1 = \lceil (4 + \Delta_{i,1})/C \rceil$ prefill iterations and 
no decode iterations.
Thus, the fixed per-iteration cost contributes $\beta_0 r_1$, while the first-prefill
hit term contributes $0$ (there are no hits in the first chunk).

\emph{(ii) Linear per-miss work.}
All $4 + \Delta_{i,1}$ input tokens are new, giving the linear term
$\beta_2 (4 + \Delta_{i,1})$.

\emph{(iii) Quadratic prefill work.}
Let the prefill be split into $q_1$ full chunks of size $C$ and one final partial
chunk of size $s_1 C$ (with $s_1 = 0$ meaning no partial chunk). 
In chunk $t=1, \dots, q_1$,
the request has $\chi^{\text{miss}} = C$ new tokens and $\chi^{\text{hit}} = (t-1)C$
cached tokens from earlier chunks of the \emph{same} request. Hence
\[
\chi^{\text{miss}}\chi^{\text{hit}} + (\chi^{\text{miss}})^2
= C \cdot (t-1)C + C^2 = C^2 t
\]
Summing over $t = 1$ to $q_1$ yields $C^2 \cdot \frac{q_1(q_1 + 1)}{2}$. 
If $s_1 > 0$, the last
partial chunk has $\chi^{\text{miss}} = s_1 C$ and $\chi^{\text{hit}} = q_1 C$, 
contributing $C^2(q_1 s_1 + s_1^2)$. Together these give the quadratic term
$\beta_3 C^2 \left( \frac{q_1(q_1 + 1)}{2} + q_1 s_1 + s_1^2 \right)$.

\emph{(iv) Decode and completion terms.}
There is no decode for a prefill-only request, so the $\beta_4$ and $\beta_5$
terms are absent. The request finishes at the end of its last prefill iteration,
so exactly one completion event occurs, contributing $\beta_6$ plus a cost that
scales with the finished request’s context length. Using \eqref{eq:busy-loop-expanded},
this adds $\beta_7 L^{\text{fin}}$, and for $(i,1)$, including the single output 
token at finish, $L^{\text{fin}} = 5 + \Delta_{i,1}$, giving $\beta_7 (5 + \Delta_{i,1})$.

\emph{(v) Ingress/egress and API costs.}
The linear ingress/egress/API terms contribute
$\alpha_0 + \alpha_1(4 + \Delta_{i,1}) + \alpha_2$ (the last because the maximum
output is $1$).

Collecting all nonzero contributions gives \eqref{eq:req1-latency}.
\end{proof}

\begin{proof}[\textbf{Proof of \eqref{eq:req2-latency}}.]
Start from \eqref{eq:e2e-param}. Since requests are issued sequentially, 
$(i,2)$ experiences no queueing from other requests; its busy-loop contribution 
is the sum of $\lambda_j$ over iterations in which $(i,2)$ is in prefill.

\emph{(i) Prefill structure and iteration count.}
Request $(i,2)$ is prefill-only with a two-segment input: 
a cached prefix of length $4 + \Delta_{i,1}$ (identical to $(i,1)$) and 
a new segment of length $1 + \Delta_{i,2}$. Only the \emph{new} tokens are chunked, 
so the number of prefill iterations is $r_2 = \lceil (1 + \Delta_{i,2})/C \rceil$, 
giving the fixed cost $\beta_0 r_2$. Because cached tokens are already present at the 
first prefill iteration, the first-chunk hit term contributes $\beta_1 (4 + \Delta_{i,1})$.

\emph{(ii) Linear per-miss work.}
Exactly $1 + \Delta_{i,2}$ tokens are new, yielding the 
linear term $\beta_2 (1 + \Delta_{i,2})$.

\emph{(iii) Quadratic prefill work.}
Split the new segment into $q_2$ full chunks of size $C$ and one 
final partial chunk of size $s_2C$ (with $s_2 = 0$ implying no partial chunk).
For a full chunk $t=1, \ldots, q_2$, the new tokens (misses) are $C$ and the 
cached tokens (hits) are $(4 + \Delta_{i,1}) + (t-1)C$ (the prefix plus prior new tokens). 
Hence the quadratic contribution per full chunk is
\[
\chi^{\text{miss}}\chi^{\text{hit}} + (\chi^{\text{miss}})^2
= C\big((4 + \Delta_{i,1}) + (t-1)C\big) + C^2
= C(4 + \Delta_{i,1}) + C^2 t.
\]
Summing over $t=1$ to $q_2$ gives $C(4 + \Delta_{i,1}) q_2 + C^2 \tfrac{q_2(q_2 + 1)}{2}$.
If $s_2 > 0$, the partial chunk contributes
$s_2 C \cdot ((4 + \Delta_{i,1}) + q_2C) + (s_2C)^2
= C (4 + \Delta_{i,1}) s_2 + C^2 (q_2 s_2 + s_2^2)$.
Combining full and partial chunks,
\[
\sum \big(\chi^{\text{miss}}\chi^{\text{hit}} + (\chi^{\text{miss}})^2\big)
= (4 + \Delta_{i,1})(1 + \Delta_{i,2}) + C^2 \Big(\tfrac{q_2 (q_2 + 1)}{2} + q_2 s_2 + s_2^2 \Big)
\]
which yields the $\beta_3$ term in \eqref{eq:req2-latency}.

\emph{(iv) Decode and completion terms.}
There is no decode, so $\beta_4, \beta_5$ do not appear. 
The request finishes at the end of its last prefill iteration, 
contributing one completion event ($\beta_6$) and a length-proportional cost 
$\beta_7 L^{\text{fin}}$. At finish, the context contains all input tokens 
plus the single output token generated at prefill completion, 
i.e., $L^{\text{fin}} = (5 + \Delta_{i,1} + \Delta_{i,2}) + 1 = 6 + \Delta_{i,1} + \Delta_{i,2}$.

\emph{(v) Ingress/egress and API costs.}
These contribute $\alpha_0 + \alpha_1 (5 + \Delta_{i,1} + \Delta_{i,2}) + \alpha_2$.

Collecting all contributions gives \eqref{eq:req2-latency}.
\end{proof}

\begin{proof}[\textbf{Proof of \eqref{eq:req3-latency}}.]
Start from \eqref{eq:e2e-param}. The input of $(i,3)$ equals $(i,1)$, so the entire
prefix of length $4 + \Delta_{i,1}$ is cached.

\emph{(i) First-iteration hit cost.}
The engine must account for cached tokens
once when the request first participates in the busy loop (a decode iteration here).
This yields the hit term $\beta_1 (4 + \Delta_{i,1})$.

\emph{(ii) No prefill-miss work.}
There are no prefill iterations for $(i,3)$, so the $\beta_2$ and $\beta_3$ terms
are absent.

\emph{(iii) Decode iterations.}
The request produces exactly $\ell_i^{\text{out}}$ tokens via decode, so there are
$\ell_i^{\text{out}}$ iterations, contributing $\beta_0 \ell_i^{\text{out}}$ from fixed cost
and $\beta_4 \ell_i^{\text{out}}$ from per-request overhead.

\emph{(iv) Decode context-length work.}
Decode iteration $t=0,\ldots,\ell_i^{\text{out}}-1$ sees context
$ (4 + \Delta_{i,1}) + t$. Hence
\[
\sum \ell_{j,i}
= \sum_{t=0}^{\ell_i^{\text{out}} - 1} \big(4 + \Delta_{i,1} + t\big)
= \frac{\ell_i^{\text{out}} \big( 2(4 + \Delta_{i,1}) + \ell_i^{\text{out}} - 1 \big)}{2},
\]
which yields the $\beta_5$ term in \eqref{eq:req3-latency}.

\emph{(v) Completion work.}
Exactly one completion occurs, contributing $\beta_6$ and a length-proportional cost
with $L^{\text{fin}} = (4 + \Delta_{i,1}) + \ell_i^{\text{out}}$, giving
$\beta_7 \big(4 + \Delta_{i,1} + \ell_i^{\text{out}}\big)$.

\emph{(vi) Ingress/egress and API costs.}
These contribute $\alpha_0 + \alpha_1 (4 + \Delta_{i,1}) + \alpha_2 \ell_i^{\text{out}}$.

Collecting terms gives \eqref{eq:req3-latency}.
\end{proof}

\begin{proof}[\textbf{Proof of \eqref{eq:req4-latency}}.]
Start from \eqref{eq:e2e-param}. Request $(i,4)$ has a cached prefix of length
$4+\Delta_{i,1}$ (identical to $(i,1)$), followed by a new segment of length
$1+\Delta_{i,2}$, and it generates $\ell_i^{\text{out}}$ output tokens.

\emph{(i) Prefill structure and iteration count.}
Only the \emph{new} segment is chunked, so the number of prefill iterations is
$r_4=\lceil(1+\Delta_{i,2})/C\rceil$, contributing the fixed cost $\beta_0 r_4$.
Because cached tokens are present from the first prefill iteration, the first-iteration
hit term contributes $\beta_1(4+\Delta_{i,1})$.

\emph{(ii) Linear per-miss work.}
Exactly $1+\Delta_{i,2}$ tokens are new, yielding $\beta_2(1+\Delta_{i,2})$.

\emph{(iii) Quadratic prefill work.}
Split the new segment into $q_4$ full chunks of size $C$ and a final partial of size
$s_4C$ (when $s_4=0$ there is no partial). For a full chunk $t=1,\ldots,q_4$,
$\chi^{\text{miss}}=C$ and $\chi^{\text{hit}}=(4+\Delta_{i,1})+(t-1)C$, so
\[
\chi^{\text{miss}}\chi^{\text{hit}} + (\chi^{\text{miss}})^2
= C(4+\Delta_{i,1}) + C^2 t.
\]
Summing full chunks gives $C(4+\Delta_{i,1})q_4 + C^2\frac{q_4(q_4+1)}{2}$.
If $s_4>0$, the partial contributes $C(4+\Delta_{i,1})s_4 + C^2(q_4 s_4 + s_4^2)$.
Combining, the quadratic work equals
\[
(4+\Delta_{i,1})(1+\Delta_{i,2})
+ C^2\!\left(\tfrac{q_4(q_4+1)}{2} + q_4 s_4 + s_4^2\right),
\]
yielding the $\beta_3$ term.

\emph{(iv) Decode iterations.}
Since the first output token is produced at the end of prefill, decode runs for
$\ell_i^{\text{out}}-1$ iterations. This contributes $\beta_0(\ell_i^{\text{out}}-1)$
from fixed cost and $\beta_4(\ell_i^{\text{out}}-1)$ from per-request overhead.

\emph{(v) Decode context-length work.}
Decode iteration $t=0,\ldots,\ell_i^{\text{out}}-2$ sees context
$ (5+\Delta_{i,1}+\Delta_{i,2}) + t$, so
\[
\sum \ell_{j,i}
= \sum_{t=0}^{\ell_i^{\text{out}}-2} \big(5+\Delta_{i,1}+\Delta_{i,2}+t\big)
= \frac{(\ell_i^{\text{out}}-1)\big(2(5+\Delta_{i,1}+\Delta_{i,2}) + (\ell_i^{\text{out}}-1) - 1\big)}{2},
\]
giving the $\beta_5$ term.

\emph{(vi) Completion work.}
Exactly one completion occurs, contributing $\beta_6$ and a length-proportional cost
with $L^{\text{fin}}=(5+\Delta_{i,1}+\Delta_{i,2})+\ell_i^{\text{out}}$, hence the
$\beta_7$ term shown.

\emph{(vii) Ingress/egress and API costs.}
These contribute $\alpha_0 + \alpha_1(5+\Delta_{i,1}+\Delta_{i,2})
+ \alpha_2 \ell_i^{\text{out}}$.

Collecting all terms yields \eqref{eq:req4-latency}.
\end{proof}


\subsection{Refinements using Blackbox Optimization}
\label{subsec:refinements-blackbox-optimization}

\subsubsection{Scenario 4: Stress Workloads for Identifiability}
\label{subsubsec:scenario4}

\paragraph*{Why Scenario~4?}
In Scenario~3 we issued requests strictly one after another. 
This was useful for controlled analysis, but it makes some coefficients in 
(\ref{eq:busy-loop-expanded}) hard to tell apart. 
For example, the fixed per-iteration cost $\beta_0$ and the 
per-finish cost $\beta_6$ always appear together, so 
their effects ``bunch up.'' 
Scenario~4 is designed to break such correlations by mixing 
different kinds of requests \emph{concurrently}.

\paragraph*{Key idea.}
We want situations where:
\begin{enumerate}
  \item Busy loop iterations run for a long time \emph{without any finishes}
        (so we can clearly see $\beta_0$, $\beta_4$, $\beta_5$).
  \item Bursts of finishes happen with very little else going on
        (so we can clearly see $\beta_6$, $\beta_7$).
  \item Large uncached segments appear with different chunk sizes
        (so we can measure quadratic prefill effects $\beta_2$, $\beta_3$).
\end{enumerate}

\paragraph*{How to craft the workload.}
We design ``episodes'' that deliberately excite different parts of the latency model:
\begin{itemize}
  \item \textbf{Episode A (Decode plateaus):} Launch several long-output decode requests with cached prefixes. 
  This produces many iterations with large decode load but no completions --- a clean signal for $\beta_0$, $\beta_4$, and $\beta_5$.
  \item \textbf{Episode B (Finish spikes):} Launch many short-output requests close together. 
  Their completions cluster, producing spikes that isolate $\beta_6$ and $\beta_7$.
  \item \textbf{Episode C (Quadratic prefills):} Launch requests with long uncached segments under varying chunk sizes. 
  This creates strong quadratic work that identifies $\beta_2$ and $\beta_3$.
  \item \textbf{Episode D (Controls):} Run one request at a time. 
  These anchor the fixed overhead terms $\alpha_0$ and $\beta_0$.
\end{itemize}

By mixing these episodes and varying chunk sizes $C$, block sizes $b$, and output lengths,
we obtain diverse situations where the different cost terms can be estimated separately.

\vspace{0.75em}
\noindent\textbf{Takeaway.}  
Unlike Scenario~3, requests in Scenario~4 should \emph{not} be strictly sequential.  
They should overlap and stress the system in different ways to tease apart 
the contributions of each coefficient.

\subsection{Blackbox Optimization after Regression}
\label{subsec:blackbox-after-ols}

\paragraph*{Step 1: Linear regression as a baseline.}
We first fit coefficients $(\alpha, \beta)$ using regression, 
because (\ref{eq:e2e-param}) and (\ref{eq:busy-loop-expanded}) are linear in the coefficients. 
This gives us good initial values quickly.

\paragraph*{Step 2: Refine with blackbox optimization.}
In reality, our predictor never matches the true system exactly:
queueing noise, kernel implementations, and clock jitter 
all introduce extra effects. 
So we refine coefficients by treating the simulator + hardware as a 
\emph{blackbox function}: input = coefficients, output = prediction error. 

We want provably good algorithms that work even when we only observe noisy end-to-end latencies.

\paragraph*{Provably convergent choice: SPSA.}
A simple but powerful option is \emph{Simultaneous Perturbation Stochastic Approximation (SPSA)}:
\begin{itemize}
  \item At each step, we perturb all coefficients in random directions (e.g., $\pm 1$ signs).
  \item We measure the loss (prediction error) at two points: ``plus perturbation'' and ``minus perturbation.''
  \item From these two measurements we estimate a gradient and update all coefficients at once.
\end{itemize}
SPSA needs only two measurements per step \emph{regardless of how many coefficients we have}, 
and it comes with convergence guarantees under standard step-size rules. 
This makes it practical and theoretically sound.

\paragraph*{Other practical options.}
In addition to SPSA:
\begin{itemize}
  \item \textbf{Zeroth-order SGD:} Similar to SPSA but uses Gaussian perturbations; often smoother.
  \item \textbf{BOBYQA/Cobyla:} Derivative-free optimizers that work well with box constraints.
  \item \textbf{Bayesian optimization:} Useful if we reparameterize to a low dimension.
  \item \textbf{CMA-ES / NES:} Robust global search strategies when the loss landscape has many local minima.
\end{itemize}

\paragraph*{Practical details.}
\begin{itemize}
  \item Use regression estimates as the starting point.
  \item Impose box constraints (e.g., $\beta_2,\beta_3,\beta_5,\beta_6,\beta_7 \ge 0$).
  \item Average losses over repeated runs of Scenario~4 episodes to reduce noise.
  \item Early stop when validation episodes stop improving.
\end{itemize}

\paragraph*{Summary.}
The strategy is:
\begin{enumerate}
  \item Design workloads (Scenario~4) that disentangle the cost terms.
  \item Run linear regression for a fast baseline estimate.
  \item Apply blackbox optimization (e.g., SPSA) for refinement, 
  which provably converges using only end-to-end latency observations.
\end{enumerate}
This combination balances practicality (OLS initialization), 
theory (provable blackbox convergence), and engineering usability.

\fi

\section{Trace-Only Estimation of Step-Level Execution Coefficients}
\label{sec:step_beta_estimation}

\subsection{Problem Setup: What We Want and What Traces Provide}

We aim to estimate \emph{step-level} execution coefficients for a
vLLM-style inference engine using \emph{trace data alone}.
A \emph{step} is one iteration of the engine's busy loop (i.e., one forward-pass
cycle that advances prefill and/or decode work for the set of active requests).
At step $k$, we model the wall-clock step duration as
\begin{equation}
\Delta t_k
=
\beta_0
+
\beta_1\,T^{\mathrm{pf}}_k
+
\beta_2\,T^{\mathrm{dec}}_k,
\label{eq:step_model}
\end{equation}
where $\beta_0$ is a fixed per-step overhead (seconds/step), and
$\beta_1,\beta_2$ are token-proportional costs (seconds/token) for
prefill and decode tokens, respectively.
The quantities $T^{\mathrm{pf}}_k$ and $T^{\mathrm{dec}}_k$ denote the total number
of prefill and decode tokens processed \emph{in that step} across all requests.

\paragraph{Why step-level coefficients matter.}
Our simulator advances one busy-loop iteration at a time.
Accurate simulation therefore requires predicting not only aggregate phase
durations, but also how many steps execute under load and how step time changes
as the set of active requests evolves.
The coefficients $\beta$ in Eq.~\eqref{eq:step_model} are the basic parameters
that govern this step-by-step evolution.

\paragraph{Trace-only challenge.}
Production traces typically do \emph{not} expose step boundaries or per-step token
counts $\{T^{\mathrm{pf}}_k,T^{\mathrm{dec}}_k\}$.
Instead, traces expose request-level \emph{phase instances} with:
(i) prefill start/end timestamps, (ii) decode start/end timestamps,
(iii) aggregate token counts for each phase, and (iv) the total number of
busy-loop iterations executed during that phase.\footnote{In our setting, the
phase-level step count is available from standard runtime tracing/counters, but
the time and token composition of individual steps is not.}
The central estimation problem is therefore: \emph{given phase-level trace
summaries, estimate the step-level coefficients $\beta$ that best explain the
observed phase durations.}

\subsection{Trace-Derived Pressures and a Prefill Correction}
\label{sec:effective_pressures}

We next define the trace-derived quantities used by our baseline estimator.

\paragraph{Phase instances.}
Let $i$ index a single phase instance (prefill or decode) of a request.
The trace provides:
start time $t_{i,s}$, end time $t_{i,e}$, observed duration
\begin{equation}
T_i = t_{i,e}-t_{i,s},
\end{equation}
and observed step count $N_i \in \mathbb{N}$ (the number of busy-loop iterations
executed while this phase was active).

\paragraph{Token pressures.}
To connect phase-level observations to the step model in
Eq.~\eqref{eq:step_model}, we work with \emph{token pressures}---the total tokens
scheduled per step by the engine at time $t$:
\begin{itemize}
  \item $p^{\mathrm{pf}}(t)$: total prefill tokens scheduled per step at time $t$,
  \item $p^{\mathrm{dec}}(t)$: total decode tokens scheduled per step at time $t$.
\end{itemize}
These functions are reconstructed from trace-visible overlaps among requests and
their phase-level token counts (the exact reconstruction procedure is
deterministic and does not require engine instrumentation; we use it only as an
input signal to the regression below).

\paragraph{Prefill chunking and a bias in naive pressures.}
Decode is straightforward: each active decode request contributes one token per
step, so aggregating decode pressure is exact.
Prefill is chunked: each request prefill proceeds in chunks of size $C$ tokens
per step, except for the final prefill step, which may process fewer than $C$
tokens. If we treat every prefill step as a full chunk, we systematically
overestimate prefill load near completion boundaries, especially under
overlapping requests.

\paragraph{Effective prefill pressure.}
Consider a request $r$ with prompt length $P_r$ tokens and $N_r$ prefill steps.
The final prefill step processes
\begin{equation}
\rho_r = P_r - C\,(N_r-1),
\qquad
\rho_r \in (0,C].
\label{eq:rho_def}
\end{equation}
Define the \emph{effective} per-step prefill contribution of request $r$ as
\begin{equation}
\tilde C_r(k) =
\begin{cases}
C, & \text{for non-final prefill steps},\\
\rho_r, & \text{for the final prefill step}.
\end{cases}
\label{eq:Cr_tilde}
\end{equation}
Let $\mathcal{R}(t)$ be the set of requests executing prefill at time $t$.
The effective prefill pressure is then
\begin{equation}
\tilde p^{\mathrm{pf}}(t)
=
\sum_{r \in \mathcal{R}(t)} \tilde C_r(t).
\label{eq:pf_pressure_tilde}
\end{equation}
Because traces do not reveal which prefill step is ``final'' at any instant,
we implement Eq.~\eqref{eq:pf_pressure_tilde} by distributing the fractional
chunk $\rho_r/C$ uniformly over the last prefill segment of request $r$.
This preserves the correct aggregate prefill token mass while avoiding a
systematic overestimate at prefill completion.

\subsection{Baseline: One-Shot Estimation from Phase Durations}
\label{sec:duration_baseline}

We now present a trace-only baseline estimator for $\beta$.
It uses phase-level step counts $N_i$ to account for fixed per-step overhead,
but it aggregates token-dependent work through \emph{time-integrated pressures}
rather than step-resolved dynamics.
This produces a simple, single-shot convex regression problem and serves as a
strong reference point for more detailed step-matching estimators.

\paragraph{Baseline assumption.}
Over the lifetime of phase instance $i$, we assume that:
(i) each of its $N_i$ busy-loop iterations incurs a fixed overhead $\beta_0$, and
(ii) token-dependent costs accumulate proportionally to the total amount of
prefill and decode work scheduled while the phase is active.

\paragraph{Phase duration predictor.}
Define the integrated (time-accumulated) pressures over the phase interval
$[t_{i,s},t_{i,e}]$:
\begin{equation}
A^{\mathrm{pf}}_i
=
\int_{t_{i,s}}^{t_{i,e}} \tilde p^{\mathrm{pf}}(t)\,dt,
\qquad
A^{\mathrm{dec}}_i
=
\int_{t_{i,s}}^{t_{i,e}} p^{\mathrm{dec}}(t)\,dt.
\label{eq:pressure_areas}
\end{equation}
These are directly computable from trace-derived pressures.
The baseline predicts phase duration as the affine model
\begin{equation}
\widehat T_i(\beta)
=
\beta_0\,N_i
+
\beta_1\,A^{\mathrm{pf}}_i
+
\beta_2\,A^{\mathrm{dec}}_i.
\label{eq:That_baseline_steps}
\end{equation}

\iffalse
\paragraph{One-shot convex regression.}
Given a collection of phase instances $\mathcal{I}$, we estimate $\beta$ via
regularized least squares:
\begin{equation}
\min_{\beta \in \mathbb{R}^3_{\ge 0}}
\;\;
\sum_{i \in \mathcal{I}}
\left(
\widehat T_i(\beta) - T_i
\right)^2
+
\lambda \,\|\beta - \beta^{\mathrm{warm}}\|_2^2,
\label{eq:duration_regression_steps}
\end{equation}
where $\lambda \ge 0$ controls shrinkage toward a warm-start
$\beta^{\mathrm{warm}}$ (e.g., a prior estimate from controlled microbenchmarks).
Because $\widehat T_i(\beta)$ is affine in $\beta$, the objective in
Eq.~\eqref{eq:duration_regression_steps} is a convex quadratic program.

\paragraph{What this baseline captures---and what it misses.}
Equation~\eqref{eq:That_baseline_steps} uses the observed step count $N_i$ to
model fixed per-step overhead exactly: each additional busy-loop iteration
increases predicted time by $\beta_0$.
Token-dependent work, however, is summarized only through integrated pressures
$A^{\mathrm{pf}}_i$ and $A^{\mathrm{dec}}_i$, which discard how load varies
\emph{across steps} as requests enter and exit.
Consequently, the estimator can fit phase durations well in aggregate while
still failing to reproduce step-level dynamics under rapidly changing overlap.
In evaluation, this baseline therefore isolates the value of explicitly
modeling step-resolved load variation beyond time-averaged (integrated) signals.
\fi

\subsubsection{Non-Negative Linear Regression ($\lambda = 0$)}
\label{sec:nnls_simplification}
Given a collection of phase instances $\mathcal{I}$, we estimate $\beta$ via 
\emph{non-negative linear least
squares} (NNLS):
\begin{equation}
\min_{\beta \in \mathbb{R}^3_{\ge 0}}
\;\;
\sum_{i \in \mathcal{I}}
\left(
\beta_0\,N_i
+
\beta_1\,A^{\mathrm{pf}}_i
+
\beta_2\,A^{\mathrm{dec}}_i
-
T_i
\right)^2 .
\label{eq:nnls_duration}
\end{equation}

\paragraph{Interpretation.}
In this form, the regression directly decomposes each observed phase duration
into three additive contributions:
a fixed cost per busy-loop iteration ($\beta_0 N_i$),
a cost proportional to cumulative prefill pressure
($\beta_1 A^{\mathrm{pf}}_i$),
and a cost proportional to cumulative decode pressure
($\beta_2 A^{\mathrm{dec}}_i$).
The non-negativity constraints enforce physically meaningful coefficients:
negative per-step overheads or negative per-token costs are ruled out by design.

\paragraph{Why NNLS is a useful baseline.}
The NNLS formulation has no hyperparameters, requires no warm-start, and can be
solved efficiently with standard convex solvers.
It therefore serves as a transparent, \emph{data-driven} baseline for
trace-only estimation.
Any improvement achieved by more sophisticated estimators—such as
step-resolved matching or iterative refinement—can be cleanly attributed to
better modeling of within-phase step dynamics rather than regularization or
prior information.

\paragraph{What this baseline captures---and what it misses.}
Equation~\eqref{eq:That_baseline_steps} uses the observed step count $N_i$ to
model fixed per-step overhead exactly: each additional busy-loop iteration
increases predicted time by $\beta_0$.
Token-dependent work, however, is summarized only through integrated pressures
$A^{\mathrm{pf}}_i$ and $A^{\mathrm{dec}}_i$, which discard how load varies
\emph{across steps} as requests enter and exit.
Consequently, the estimator can fit phase durations well in aggregate while
still failing to reproduce step-level dynamics under rapidly changing overlap.
In evaluation, this baseline therefore isolates the value of explicitly
modeling step-resolved load variation beyond time-averaged (integrated) signals.


\subsubsection{Stabilizing NNLS via Feature Normalization and Cross-Fitting}
\label{sec:nnls_crossfit}

While the NNLS formulation in Eq.~\eqref{eq:nnls_duration} is simple and
interpretable, in practice it can exhibit sensitivity to trace composition and
numerical conditioning when pressure features have very different scales or when
the trace set provides limited excitation along certain dimensions.
We therefore adopt a lightweight stabilization procedure that preserves the
trace-only and convex nature of the estimator, without introducing an explicit
regularization term.

\paragraph{Feature normalization.}
Let $x_i = (N_i, A^{\mathrm{pf}}_i, A^{\mathrm{dec}}_i)$ denote the feature vector
for phase instance $i$.
We rescale each feature dimension by a fixed, data-derived positive scale
$s_j > 0$, computed once over the full trace set:
\begin{equation}
\tilde x_{ij} = \frac{x_{ij}}{s_j},
\qquad
s_j = \mathrm{scale}\bigl(\{x_{ij}\}_{i \in \mathcal{I}}\bigr),
\label{eq:feature_scaling}
\end{equation}
where $\mathrm{scale}(\cdot)$ denotes a robust column scale such as the RMS or
median.
This scale-only normalization improves numerical conditioning and reduces
spurious changes in the NNLS active set caused purely by feature magnitude
differences, while preserving the physical interpretation that zero features
correspond to zero scheduled work.
All NNLS solves described below are performed in the normalized feature space,
with coefficients mapped back to physical units afterward.


\paragraph{Cross-fitting with NNLS.}
To reduce sensitivity to the specific set of phase instances included in the fit,
we use $K$-fold cross-fitting.
The trace instances are partitioned into $K$ disjoint folds
$\{\mathcal{I}_1,\dots,\mathcal{I}_K\}$.
For each fold $k$, we solve the NNLS problem on the remaining data:
\begin{equation}
\hat\beta^{(k)}_{\mathrm{norm}}
=
\arg\min_{\beta \ge 0}
\sum_{i \in \mathcal{I}\setminus\mathcal{I}_k}
\left(
\beta^\top \tilde x_i - T_i
\right)^2 .
\label{eq:nnls_crossfit_solve}
\end{equation}
The resulting coefficients are then mapped back to the original units via
$\hat\beta^{(k)}_j = \hat\beta^{(k)}_{\mathrm{norm},j} / s_j$.

\paragraph{Aggregation and final estimate.}
The final coefficient estimate is obtained by aggregating across folds:
\begin{equation}
\hat\beta^{\mathrm{final}}
=
\mathrm{median}_{k=1,\dots,K}
\;
\hat\beta^{(k)} .
\label{eq:nnls_crossfit_aggregate}
\end{equation}
We use the coordinate-wise median rather than the mean to improve robustness in
regimes where NNLS constraints intermittently bind (e.g., when a coefficient is
driven to zero in some folds but not others).

\paragraph{Interpretation and diagnostics.}
This procedure can be viewed as enforcing \emph{reproducibility across trace
subsets} rather than shrinking coefficients toward a prior.
Fold-to-fold variation in $\hat\beta^{(k)}$ provides a direct diagnostic of
parameter identifiability: large variance or frequent constraint activation
indicates that the trace set does not sufficiently excite the corresponding
dimension of the step model.
Importantly, the estimator remains trace-only and convex at every stage, and
does not rely on engine instrumentation or external calibration.



\iffalse 
\subsection{Continuous-Time Surrogate for Step Execution}

Let $i$ index a phase instance (either prefill or decode) of a request.
From trace data, each phase provides:
\begin{itemize}
  \item start and end times $(t_{i,s}, t_{i,e})$,
  \item observed duration $T_i = t_{i,e} - t_{i,s}$,
  \item observed number of busy-loop iterations $N_i$,
  \item time-varying token pressures:
  \begin{itemize}
    \item $p^{\mathrm{pf}}(t)$: total prefill tokens scheduled per step at time $t$,
    \item $p^{\mathrm{dec}}(t)$: total decode tokens scheduled per step at time $t$,
  \end{itemize}
  reconstructed from overlapping requests in the trace.
\end{itemize}

We define a continuous-time analogue of step duration:
\begin{equation}
\Delta(t;\beta)
=
\beta_0
+
\beta_1\,p^{\mathrm{pf}}(t)
+
\beta_2\,p^{\mathrm{dec}}(t),
\qquad
\Delta(t;\beta) > 0.
\label{eq:delta_t}
\end{equation}

\paragraph{Interpretation.}
If token pressures were constant, each busy-loop iteration would take
$\Delta(t;\beta)$ seconds.
When pressures vary over time, the engine executes steps at an
instantaneous rate $1/\Delta(t;\beta)$.

---

\subsection{Predicted Step Count as a Convex Surrogate}

We approximate the number of steps executed during phase $i$ by integrating
the instantaneous step rate:
\begin{equation}
\widehat N_i(\beta)
=
\int_{t_{i,s}}^{t_{i,e}}
\frac{1}{\Delta(t;\beta)}\,dt.
\label{eq:Nhat_continuous}
\end{equation}

\paragraph{Convexity.}
$\Delta(t;\beta)$ is affine and strictly positive in $\beta$,
so $1/\Delta(t;\beta)$ is convex.
Therefore, $\widehat N_i(\beta)$ is a convex function of $\beta$.

\paragraph{Systems meaning.}
$\widehat N_i(\beta)$ predicts how many busy-loop iterations a
step-based simulator would execute when replaying the trace-derived
load profile.
Matching $\widehat N_i(\beta)$ to observed step counts is thus the
correct objective for step-accurate simulation.

---

\subsection{Discrete Approximation via Quadrature}

We approximate Eq.~\eqref{eq:Nhat_continuous} numerically.
Partition the interval $[t_{i,s}, t_{i,e}]$ into $m$ subintervals
of width $w_{i,j}=T_i/m$.
Let $t_{i,j}$ be a representative time in each subinterval.

The predicted step count is approximated as
\begin{equation}
\widehat N_i(\beta)
\approx
\sum_{j=1}^{m}
\frac{w_{i,j}}
{\beta_0
 + \beta_1\,p^{\mathrm{pf}}(t_{i,j})
 + \beta_2\,p^{\mathrm{dec}}(t_{i,j})}.
\label{eq:Nhat_discrete}
\end{equation}

We use either midpoint quadrature or stratified Monte Carlo sampling;
both preserve convexity and work well in practice.

\subsection{Sequential Convex Step-Count Matching}
\label{sec:ccp_step_matching}

Directly enforcing $\widehat N_i(\beta)=N_i$ is nonconvex because
$\widehat N_i(\beta)$ is convex in $\beta$.
Equivalently, the equality decomposes into one convex and one nonconvex inequality:
$\widehat N_i(\beta)\le N_i$ is convex, while
$\widehat N_i(\beta)\ge N_i$ is not.
We resolve this asymmetry with a standard sequential convex (CCP-style) method
that solves a \emph{convex quadratic program} at each iteration.

\paragraph{Convex--nonconvex split with slacks.}
For each phase instance $i$, we introduce nonnegative slacks $u_i,s_i\ge 0$ and impose:
\begin{align}
\textbf{(Upper bound, convex and exact)} \qquad
& \widehat N_i(\beta) \;\le\; N_i + u_i,
\label{eq:ccp_upper}
\\
\textbf{(Lower bound, convex after linearization)} \qquad
& \widehat N_i(\beta) \;\ge\; N_i - s_i.
\label{eq:ccp_lower_nonconvex}
\end{align}
The upper bound \eqref{eq:ccp_upper} is already convex.
To handle the nonconvex lower bound \eqref{eq:ccp_lower_nonconvex}, we replace
$\widehat N_i(\beta)$ by a first-order global under-estimator at the current iterate.

\paragraph{Linearized lower bound.}
At iteration $r$, given $\beta^{(r)}$, convexity yields the global bound
\[
\widehat N_i(\beta)
\;\ge\;
\widehat N_i(\beta^{(r)})
+
\nabla \widehat N_i(\beta^{(r)})^\top(\beta-\beta^{(r)}).
\]
We therefore enforce the lower bound conservatively via
\begin{equation}
\widehat N_i(\beta^{(r)})
+
\nabla \widehat N_i(\beta^{(r)})^\top(\beta-\beta^{(r)})
\;\ge\;
N_i - s_i,
\qquad s_i\ge 0,
\label{eq:ccp_lower_linearized}
\end{equation}
which is linear in $\beta$ and thus convex.

\paragraph{Per-iteration convex program (QP).}
Each CCP iteration solves the following convex quadratic program:
\begin{align}
\min_{\beta,\{u_i,s_i\}}
\quad
& \sum_i \left(u_i^2 + s_i^2\right)
\;+\;
\lambda \|\beta-\beta^{\mathrm{warm}}\|_2^2
\label{eq:ccp_qp_obj}
\\
\text{s.t.}\quad
&
\widehat N_i(\beta) \le N_i + u_i,
\qquad u_i \ge 0,
\quad \forall i,
\label{eq:ccp_qp_upper}
\\
&
\widehat N_i(\beta^{(r)})
+
\nabla \widehat N_i(\beta^{(r)})^\top(\beta-\beta^{(r)})
\ge N_i - s_i,
\qquad s_i \ge 0,
\quad \forall i,
\label{eq:ccp_qp_lower}
\\
&
\beta \ge 0,
\qquad
\Delta(t_{i,j};\beta) \ge \varepsilon,
\quad \forall (i,j).
\label{eq:ccp_qp_domain}
\end{align}
Here $\beta^{\mathrm{warm}}$ is a warm start (e.g., from the one-shot baseline),
$\lambda>0$ stabilizes the solution when the data is weakly informative,
and $\varepsilon>0$ ensures the step duration remains strictly positive at
all quadrature points.

\paragraph{Feasibility vs.\ fitting.}
This is \emph{not} merely a feasibility procedure.
The slacks $(u_i,s_i)$ guarantee feasibility, while the objective
\eqref{eq:ccp_qp_obj} drives both toward zero, i.e., toward two-sided
step-count matching.
In the ideal noiseless setting, the optimum achieves $u_i=s_i=0$ for all $i$.

\paragraph{Engineering meaning.}
Each iteration adjusts $\beta$ so that, under the reconstructed load profile,
the simulator predicts the same number of busy-loop iterations as observed in
the trace, while preserving physically meaningful coefficients (nonnegativity
and positive step times). This directly targets step-accurate discrete-event
simulation rather than duration-only agreement.

We may also want to read, use and cite ... https://arxiv.org/pdf/2302.02536 and variants
like https://www.jhuapl.edu/spsa/pdf-spsa/bhatnagar\_kowshick\_simulation05.pdf.

\fi



\section{Evaluation}
\label{sec:evaluation}

\section{Implementation}
\label{sec:implementation}

\section{Limitations of our Approach}
\label{sec:limitations}

Our current approach has several important limitations:

\begin{enumerate}
    \item \textbf{Hardware and model specificity.}
    BLIS is calibrated for a fixed combination of GPU type and model
    architecture. We do not attempt to generalize across different hardware or
    models. If either changes, BLIS must be retrained with new data.

    \item \textbf{No pre-emption.}
    We do not model pre-emption of running requests. In practice, triggering
    pre-emption during request handling is rare and avoided by the scheduler
    logic, but its absence in our model means we cannot accurately predict
    latency under pre-emptive scenarios.

    \item \textbf{No speculative decoding.}
    We currently omit speculative decoding mechanisms (such as those recently
    introduced in vLLM as an experimental feature).
\end{enumerate}

\section{Future Work}
\label{sec:future-work}

\section{Conclusion}
\label{sec:conclusion}

\nocite{*}
\bibliographystyle{plain}
\bibliography{blis}

\end{document}
