\documentclass[11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{times}
\usepackage{graphicx}
\usepackage{amsmath, amssymb}
\usepackage{hyperref}
\usepackage{geometry}
\geometry{margin=1in}

\title{BLIS: Blackbox Inference Simulator}
\author{AI Platform Optimization Team}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This paper introduces BLIS, a blackbox inference simulator designed to model request flows and latency in large language model (LLM) inference workloads.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

\section{Related Work}
\label{sec:related-work}

\section{Design overview}
\label{sec:design-overview}

Our simulator is primarily geared towards vLLM. We will describe extensions for other
inference platforms like sglang in Section \ref{sec:future-work}.

\subsection{vLLM overview}
\label{subsec:vllm-overview}
We will focus on handling of an inference request by vLLM. vLLM
has two main components: the API server and the engine core; these operate as separate threads and
communicate through a message queue.

\paragraph{API server thread.}
The API server is implemented using FastAPI. Its role is to handle client-facing tasks without blocking on actual model execution.
The API thread is responsible for:
\begin{enumerate}
    \item Receiving and tokenizing incoming requests from clients.
    \item Enqueuing tokenized requests into the message queue.
    \item Streaming back partial responses as they become available from the engine.
    \item Detokenizing completed sequences and packaging them into response
          objects.
    \item Emitting the final response back to the client.
\end{enumerate}

\paragraph{Engine core thread.}
The engine core runs the central \emph{inference loop}. This loop is driven by a
scheduler that repeatedly:
\begin{enumerate}
    \item Collects pending requests from the queue, forming a dynamic batch.
    \item Determines whether each request is in a \emph{prefill} (first token)
          or \emph{decode} (subsequent tokens) stage.
    \item Executes the forward pass on the GPU for that batch.
    \item Updates KV cache blocks, manages allocation and eviction, and applies
          prefix caching optimizations when possible.
    \item Returns generated tokens back to the API layer, either for streaming
          or final response.
\end{enumerate}
We will refer to this engine loop as the \emph{busy loop} since it continuously
steps through request-batches at a fine-grained timescale.

\subsection{Simulator design}
\label{subsec:simulator-design}

\section{Latency model}
\label{sec:latency-model}

\subsubsection{Request life-cycle}
\label{subsubsec:request-life-cycle}

We formalize the progression of a single request $i$ through the inference system.
Each stage is associated with latencies and variables in
Table~\ref{tab:notation} that are estimated, and collectively, they determine the end-to-end delay
$e_i$.

\paragraph{1. Ingress.}
The client emits request $i$, which traverses the network before reaching the
server. This contributes a network ingress delay $n_i^{\text{in}}$. From the
simulation perspective, this has a linear dependency on the number of input tokens in the request.

\paragraph{2. Pre-process.}
Upon arrival, the API server thread tokenizes the prompt, performs light
validation, and enqueues the request into the message queue contributing a latency of $m_i$. This has a linear dependence 
on the number of input tokens.

\paragraph{3. Scheduling.} The request waits in the queue until there is sufficient GPU capacity; here, sufficiency
is determined by the batching policy, token budget, and other vLLM params. The duration of this waiting period equals the
sum of the durations of the busy loop iterations within this period.

\paragraph{4. Chunked prefill.} The duration of prefill equals the
sum of the durations of the busy loop iterations wherein the request participated in its prefill phase.

\paragraph{5. Decode.} The duration of decode equals the
sum of the durations of the busy loop iterations wherein the request participated in its decode phase.

\paragraph{6. Post-process.} Once all output tokens are produced, the API server detokenizes them and
packages the final response. This incurs a post-process latency $c_i$, with a linear dependence on 
the number of input and output tokens $t_i^{in}$ and $t_i^{out}$.

\paragraph{7. Egress.}
The server emits response $i$, which traverses the network before reaching the
client. This contributes a network egress delay $n_i^{\text{out}}$, with a linear
dependence on the number of tokens in the response: $t_i^{in} + t_i^{out}$.

\begin{table}[ht]
\centering
\caption{Notation for request life-cycle and busy-loop model.}
\label{tab:notation}
\begin{tabular}{ll}
\hline
\textbf{Symbol} & \textbf{Meaning} \\
\hline
\multicolumn{2}{l}{\textit{Per-request variables}} \\
\hline
$e_i$ & End-to-end latency of request $i$ \\
$n_i^{\text{in}}, n_i^{\text{out}}$ & client-to-server and server-to-client communication latencies for request $i$ \\
$n_i$ & Total client-server communication latency for request $i$; equals $n_i^{\text{in}} + n_i^{\text{out}}$  \\
$m_i$ & Pre-processing latency (API $\to$ message queue) for request $i$ \\
$s_i$ & Waiting time in the message queue for request $i$ \\
$p_i$ & Prefill latency (scheduled $\to$ first token) for request $i$ \\
$d_i$ & Decode latency (first $\to$ last token) for request $i$ \\
$c_i$ & Post-processing latency for request $i$ \\
$t_i^{\text{in}}, t_i^{\text{out}}$ & Input / output tokens of request $i$ \\
\hline
\multicolumn{2}{l}{\textit{Busy-loop variables (iteration $j$)}} \\
\hline
$\lambda_j$ & Duration (length) of busy-loop iteration $j$ \\
$\rho_j$ & Requests in the message queue at the start of iteration $j$ \\
$\tau_j$ & Total input tokens across all queued requests at $j$ \\
$\pi_j$ & Number of prefill requests in $\mathcal{B}_j$ \\
$\delta_j$ & Number of decode requests active in batch at $j$ \\
$\chi_{j,i}^{\text{hit,new}}$ & Cache hits (counted in blocks) in \emph{new} prefill blocks for request $i$ at $j$ \\
$\chi_{j,i}^{\text{miss}}$ & Cache misses (counted in fractional blocks) in \emph{new} prefill blocks for request $i$ at $j$; \\
                           & for ease of estimation, we define this as a fraction; specifically, the ratio between \\
                           & number of tokens involved in the cache miss and the block size \\
$\chi_{j,i}^{\text{hit}}$ & \emph{Total} cache hits (counted in blocks) for request $i$ at $j$ \\
$\tau_j^{\text{dec}}$ & Total decode tokens across all decode requests in batch at $j$ \\
$\psi_j$ & Quadratic cache penalty term at $j$ (Def.~\ref{eq:quad}) \\
$\rho_j^{\text{pre-fin}}$ & Number of prefill requests that finished in iteration $j$ \\
$\rho_j^{\text{dec-fin}}$ & Number of decode requests that finished in iteration $j$ \\
$\tau_j^{\text{fin}}$ & Number of tokens across all \emph{decode} requests that finished in $j$ \\
\hline
\multicolumn{2}{l}{\textit{Sets}} \\
\hline
$\mathcal{B}_j$ & Set of requests included in the batch at iteration $j$ \\
$\mathcal{S}_i$ & Iterations where request $i$ is waiting in the message queue \\
$\mathcal{P}_i$ & Iterations where request $i$ participates in prefill \\
$\mathcal{D}_i$ & Iterations where request $i$ participates in decode \\
\hline
\multicolumn{2}{l}{\textit{Coefficients}} \\
\hline
$\alpha_0$, $\alpha_1$, $\alpha_2$, $\beta_0$, $\beta_1$, $\ldots$ & Coefficients for parametric models \\
\hline
\multicolumn{2}{l}{\textit{vLLM Params}} \\
\hline
$b$ & Number of tokens in a block \\
\end{tabular}
\end{table}

\subsection{End-to-End request latency model}
\label{subsec:end-to-end-latency-model}

\paragraph{Basic decomposition.}
\begin{align}
e_i = n_i + m_i + s_i + p_i + d_i + c_i
\label{eq:end-to-end}
\end{align}

\paragraph{Suitable for estimation.}
\begin{align}
e_i = \alpha_0
 + \alpha_1\, t_i^{\text{in}}
 + \alpha_2\, t_i^{\text{out}}
 + \sum_{j \in \mathcal{S}_i} \lambda_j
 + \sum_{j \in \mathcal{P}_i} \lambda_j
 + \sum_{j \in \mathcal{D}_i} \lambda_j
\label{eq:e2e-param}
\end{align}

\subsubsection{Busy-loop latency model}
\label{subsubsec:busy-loop-latency-model}
We first define the quadratic term that captures the quadratic size of the KV tensor matrices
that computed when there are cache misses and cache hits for requests during iteration $j$:
\begin{align}
\psi_j
  = \sum_{i \in \mathcal{B}_j}
    \Big(
      \chi_{j,i}^{\text{miss}} \cdot \chi_{j,i}^{\text{hit}}
      + \big(\chi_{j,i}^{\text{miss}}\big)^2
    \Big)
\label{eq:quad}
\end{align}

The duration of iteration $j$ is now modeled as a quadratic function of features derived from the contents of the queue, batch, 
and cache as follows:
\begin{align}
\lambda_j & = \beta_0 && \text{(intercept)} \\
 & + \beta_1 \rho_j
 + \beta_2 \tau_j && \text{(scheduling)} \\
 & + \beta_3 \pi_j
 + \beta_4 \sum_{i \in \mathcal{B}_j} \chi_{j,i}^{\text{hit,new}}
 + \beta_5 \sum_{i \in \mathcal{B}_j} \chi_{j,i}^{\text{miss}} 
 + \beta_6 \psi_j && \text{(prefill; last term is quadratic)} \\
 & + \beta_7 \delta_j 
 + \beta_8 \tau_j^{\text{dec}} && \text{(decode)} \\
 & + \beta_9 \rho_j^{\text{pre-fin}}
 + \beta_{10} \rho_j^{\text{dec-fin}}
 + \beta_{11} \tau_j^{\text{fin}} && \text{(post-process)}
\label{eq:busy-loop-expanded}
\end{align}

\section{Estimation}
\label{sec:estimation}

\subsection{Base Estimation with Linear Regression}
\label{subsec:base-estimation-linear-regression}

We estimate the model coefficients through a set of scenarios as follows. Each scenario involves 
the client sending an inference request workload to the server, and defines a 
set of conditions that the inference request workload needs to satisfy.

\subsubsection{Scenario 1}
\label{subsubsec:scenario1}

\begin{enumerate}
    \item Requests arrive strictly sequentially: request $i+1$ is emitted by the client only after
    the final response for request $i$ has been received by it.
    \item Each request is a prefill request. 
    \item Each request consists of a single prompt, and is aimed at the \textit{/v1/completions} endpoint of vLLM.
    \item Each prompt length is $\leq$ the configured chunk size.
    \item Prompts have no prefix overlap across requests. Equivalently, the first block
    of tokens for every request is unique.
\end{enumerate}

Under these conditions, the end-to-end latency model reduces to a simplified form as follows:

\begin{align}
 \forall i \in \text{Scenario 1}:\quad 
   e_i &= (\alpha_0 + \beta_0 + \beta_1 + \beta_3 + \beta_9 + \beta_{11}) \nonumber \\
       &\quad + \big(\alpha_1 + \beta_2 + \frac{\beta_5}{b} + \beta_{11}\big) \cdot t_i^{\text{in}}
       + \beta_6 \left( \frac{t_i^{\text{in}}}{b} \right)^2 \label{eqn:scenario1} \\
   &= \gamma_0 + \gamma_1 t_i^{\text{in}} + \beta_6 \left( \frac{t_i^{\text{in}}}{b} \right)^2 \nonumber
\end{align}

with definitions
\begin{align}
   \gamma_0 &\doteq \alpha_0 + \beta_0 + \beta_1 + \beta_3 + \beta_9 + \beta_{11} \\
   \gamma_1 &\doteq \alpha_1 + \beta_2 + \frac{\beta_5}{b} + \beta_{11}
\end{align}

\paragraph{Estimation.}
The parameters of interest in this scenario are $\gamma_0$, $\gamma_1$, and $\beta_6$.
These can be estimated by generating workloads that strictly satisfy the conditions
above. To ensure sufficient variation in prompt length, we configure the chunk
size to be large (e.g., $2048$ tokens), which allows $t_i^{\text{in}}$ to vary
across a wide range. The block size $b$ is fixed at $16$, consistent with typical
production settings. With this setup, ordinary least squares regression on the
measured end-to-end latencies $e_i$ yields estimates for the three parameters.


\subsection{Refinements using Blackbox Optimization}
\label{subsec:refinements-blackbox-optimization}

\section{Evaluation}
\label{sec:evaluation}

\section{Implementation}
\label{sec:implementation}

\section{Limitations of our Approach}
\label{sec:limitations}

Our current approach has several important limitations:

\begin{enumerate}
    \item \textbf{Hardware and model specificity.}
    The simulator is calibrated for a fixed combination of GPU type and model
    architecture. We do not attempt to generalize across different hardware or
    models. If either changes, the simulator must be retrained with new data.

    \item \textbf{No pre-emption.}
    We do not simulate pre-emption of running requests. In practice, triggering
    pre-emption during request handling is rare and avoided by the scheduler
    logic, but its absence in our model means we cannot accurately predict
    latency under pre-emptive scenarios.

    \item \textbf{No speculative decoding.}
    We currently omit speculative decoding mechanisms (such as those recently
    introduced in vLLM as an experimental feature).
\end{enumerate}

\section{Future Work}
\label{sec:future-work}

\section{Conclusion}
\label{sec:conclusion}

\bibliographystyle{plain}
\bibliography{blis}

\end{document}
