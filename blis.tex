\documentclass[11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{times}
\usepackage{graphicx}
\usepackage{amsmath, amssymb}
\usepackage{bm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{hyperref}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{booktabs,longtable,array,makecell}
\setlength{\LTleft}{0pt}   % no left indent for longtable
\setlength{\LTright}{0pt}  % no right indent for longtable
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}p{#1}}

% --- Theorems, lemmas, claims, proofs ---
\usepackage{amsthm}

% Number theorems within sections; share a counter across theorem-like envs.
\theoremstyle{plain} % bold headings, italic body
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{claim}[theorem]{Claim}

\theoremstyle{definition} % bold heading, upright body
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark} % italic heading, upright body
\newtheorem{remark}[theorem]{Remark}
\newtheorem{note}[theorem]{Note}

% Optional: unnumbered versions
\newtheorem*{theorem*}{Theorem}
\newtheorem*{lemma*}{Lemma}
\newtheorem*{claim*}{Claim}

% Proof environment is provided by amsthm:
% \begin{proof} ... \end{proof}
% Customize QED symbol if you like:
\renewcommand{\qedsymbol}{$\blacksquare$}

% (Optional) If you want equations numbered by section (since you already use amsmath):
\numberwithin{equation}{section}



\title{BLIS: Blackbox Inference Performance Estimator}
\author{AI Platform Optimization Team}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This paper introduces BLIS, a system for blackbox inference performance estimation 
designed to model inference request flows and latency.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

\section{Related Work}
\label{sec:related-work}

\section{Design overview}
\label{sec:design-overview}

BLIS is primarily geared towards vLLM. We will describe extensions for other
inference platforms like sglang in Section \ref{sec:future-work}.

\subsection{vLLM overview}
\label{subsec:vllm-overview}
We will focus on handling of an inference request by vLLM. vLLM
has two main components: the API server and the engine core; these operate as separate threads and
communicate through a message queue.

\subsubsection{API server thread}
The API server is implemented using FastAPI. Its role is to handle client-facing tasks without blocking on actual model execution.
The API thread is responsible for:
\begin{enumerate}
    \item Receiving and tokenizing incoming requests from clients.
    \item Enqueuing tokenized requests into the message queue.
    \item Streaming back partial responses as they become available from the engine.
    \item Detokenizing completed sequences and packaging them into response
          objects.
    \item Emitting the final response back to the client.
\end{enumerate}

\subsubsection{Engine core thread}
The engine core runs the central \emph{inference loop}. This loop is driven by a
scheduler that repeatedly:
\begin{enumerate}
    \item Collects pending requests from the queue, forming a dynamic batch.
    \item Determines whether a request is in a \emph{prefill} (first token)
          or \emph{decode} (subsequent tokens) phase.
    \item Executes the forward pass on the GPU for that batch.
    \item Updates KV cache blocks, manages allocation and eviction, and applies
          prefix caching optimizations when possible.
    \item Returns generated tokens back to the API layer, either for streaming
          or final response.
\end{enumerate}
We will refer to this engine loop as the \emph{busy loop} since it continuously
steps through request-batches at a fine-grained timescale.

\subsection{Design of BLIS}
\label{subsec:blis-design}

\section{Latency model}
\label{sec:latency-model}

\subsubsection{Request types}
\label{subsubsec:request-types}

Requests can be of two types: \textit{prefill-only} or \textit{decode}. 
Prefill-only requests have their maximum output length set to $1$, and \textit{decode}
requests have this set to a number greater than $1$.

\subsubsection{Request life-cycle}
\label{subsubsec:request-life-cycle}

\begin{figure}[!ht]
  \centering
  \includegraphics[height=0.8\textwidth]{figs/reqlifecycle.png}
  \caption{Request Lifecycle}
  \label{fig:req-lifecycle}
\end{figure}

The life of an inference request in the system passes through multiple stages 
as  illustrated in Figure \ref{fig:req-lifecycle} and described below.

\paragraph{1. Ingress.}
The client emits an inference request $i$ to the server. This contributes 
a network latency that depends linearly on the number of input tokens 
$t_i^{\text{in}}$.

\paragraph{2. Pre-process.}
Upon arrival, the API server tokenizes the prompt, performs light
validation, and enqueues the request into the message queue. This contributes 
a latency that depends linearly on $t_i^{\text{in}}$.

\paragraph{3. Scheduling.} The request waits in the queue until there is 
sufficient GPU capacity. During this waiting period, other requests get to participate
in the busy loop iterations.

\paragraph{4. Chunked prefill.} The uncached portion of this request is 
prefilled one chunk at a time. The latency of this stage equals the sum of the 
duration of the busy loop iterations in which this request prefills.

\paragraph{5. Decode.} The latency of this stage equals the sum of the duration
of the busy loop iterations in which this request is in its decode phase. 
The first output token is generated during the prefill phase, and subsequent 
tokens are generated in the decode iterations.

\paragraph{6. Post-process.} Once all output tokens are produced, 
the API server detokenizes them and
packages the final response. The latency of this stage depends linearly on the 
number of output tokens $t_i^{\text{out}}$.

\paragraph{7. Egress.}
The server emits response $i$ to the
client. This contributes a network latency that depends linearly 
on the number of output tokens $t_i^{\text{out}}$.

\subsection{End-to-End request latency model}
\label{subsec:end-to-end-latency-model}

Let $\lambda_j$ denote the duration of busy loop iteration $j$. We start
with a high-level model for the end-to-end latency as follows.

\begin{align}
e_i \approx \alpha_0
 + \alpha_1\, t_i^{\text{in}}
 + \alpha_2\, t_i^{\text{out}}
 + \sum_{\text{i waits at j}} \lambda_j
 + \sum_{\text{i prefills at j}} \lambda_j
 + \sum_{\text{i decodes at j}} \lambda_j
\label{eq:e2e-param}
\end{align}

\textbf{Note:} When we write $a \approx b$, it is shorthand for $a=b+\varepsilon$ 
where $\varepsilon$ is a residual noise term. Regression equations such 
as \eqref{eq:e2e-param} are understood to hold up to residual noise term.


\subsubsection{Busy-loop latency model}
\label{subsubsec:busy-loop-latency-model}
Let $\chi_{j,i}^{\text{hit}}$ and $\chi_{j,i}^{\text{miss}}$ be the numbers of 
cached and uncached (new) tokens of request $i$ during iteration $j$.
Define the following request sets.
\begin{align*}
  \mathcal{P}_j & = & \{i:\text{$i$ is in prefill at $j$}\} \nonumber \\
  \mathcal{P}^{(1)}_j & = & \{i:\text{$i$ is in its first prefill chunk at $j$}\} \nonumber \\
  \mathcal{D}_j & = & \{i:\text{$i$ is decoding at $j$}\} \nonumber \\
  \mathcal{F}_j & = & \{i:\text{$i$ finishes at $j$}\}
\end{align*}
Let $\ell_{j,i}$ denote the context length of request $i$ at the 
start of iteration $j$.

\paragraph{Quadratic prefill term.}
\begin{align}
\psi_j
  = \sum_{i \in \mathcal{P}_j}
    \Big(
      \chi_{j,i}^{\text{miss}} \cdot \chi_{j,i}^{\text{hit}}
      + \big(\chi_{j,i}^{\text{miss}}\big)^2
    \Big)
\label{eq:quad}
\end{align}

\paragraph{Latency model.}
Let $L^{\text{dec}}_j=\sum_{i\in\mathcal{D}_j}\ell_{j,i}$ be the total decode
context length in iteration $j$, and
$L^{\text{fin}}_j=\sum_{i\in\mathcal{F}_j}\ell_{j,i}$ the total context length of
requests that finish in $j$ (a proxy for KV cache tokens to free). 
We model the iteration time as
\begin{align}
\lambda_j 
  \approx \beta_0
  + \beta_1 \sum_{i \in \mathcal{P}^{(1)}_j} \chi_{j,i}^{\text{hit}}
  + \beta_2 \sum_{i \in \mathcal{P}_j}      \chi_{j,i}^{\text{miss}}
  + \beta_3 \psi_j 
  + \beta_4 |\mathcal{D}_j|
  + \beta_5 L^{\text{dec}}_j
  + \beta_6 |\mathcal{F}_j|
  + \beta_7 L^{\text{fin}}_j
\label{eq:busy-loop-expanded}
\end{align}

\paragraph{Justification.} Each busy-loop iteration mixes four kinds of work.

\begin{enumerate}
  \item \emph{Fixed work.} An iteration-invariant cost (\(\beta_0\)).
  \item \emph{Prefill work.} The first prefill iteration of a request involves hashing
its input tokens to determine the number of cache hits (linear in hits; \(\beta_1\)).
Every new token pays a linear per-token cost (embeddings, MLP, KV writes; \(\beta_2\)).
Attention mechanism during prefill is quadratic: every new token 
(cache miss) must attend to all tokens already in the context (cache hit) 
plus the other new tokens from the same chunk (\(\psi_j\) in~\eqref{eq:quad}; 
\(\beta_3\)).
  \item \emph{Decode work.} Decode generates one token per active request:
there is a per-request overhead (\(\beta_4\)) and compute that scales with
the request’s current context length (\(\beta_5\)).
  \item \emph{Completion work.} When a request completes, we free KV blocks, update
refcounts, and serialize output; this scales linearly in the number of 
finished requests 
(\(\beta_6\)) and the total length of finished requests (\(\beta_7\)).
\end{enumerate}

The coefficients \(\{\beta_k\}\) absorb model/hardware constants
(layers, heads, width, precision, kernel efficiency).


\section{Estimation}
\label{sec:estimation}

\subsection{Base Estimation with Linear Regression}
\label{subsec:base-estimation-linear-regression}

Equation (\ref{eq:busy-loop-expanded}) is a quadratic function of  
$\boldsymbol{\chi^{\text{miss}}}$ and $\boldsymbol{\chi^{\text{hit}}}$; however,  
it is a linear function of $\boldsymbol{\alpha}$ 
and $\boldsymbol{\beta}$. This enables the estimation of $\boldsymbol{\alpha}$ 
and $\boldsymbol{\beta}$ through linear regression as described
in Scenario 3.

\subsubsection{Scenario 3}
\label{subsubsec:scenario3}

Scenario~3 involves generating request quadruples (``quads'').  

\paragraph{Setup.}
Unless otherwise specified, all lengths are in tokens. 
Fix a chunk size $C \in \{256, 512, 1024, 2048, 4096\}$, 
a block size $b \in \{8, 16, 32\}$, and maximum sequence length 
$M = 8192$ blocks.

Each quad $i$ consists of four requests $(i,1)$, $(i,2)$, $(i,3)$, and $(i,4)$.
Requests within a quad may share prefixes, while requests across quads do not. 
Requests are issued strictly sequentially; a request 
is emitted by the client only after it receives the response to the previous request:
$\ldots \to (i-1, 4) \to (i,1) \to (i,2) \to (i,3) \to (i,4) \to (i + 1,1) \to \ldots$ 

The input length, output length, and the content of the requests are as follows.

\begin{enumerate}
  \item $(i,1)$ is a prefill-only request with input length 
        $4 + \Delta_{i,1}$. Its first four tokens are distinct from the 
        first four tokens of any request in quads $\{1, 2, \ldots, i-1 \}$. 
        This guarantees that request $(i, 1)$ does not share a prefix with any 
        previously generated requests. Its maximum output length is set to $1$.
  \item $(i,2)$ is a prefill-only request with two input segments, of total length 
        $5 + \Delta_{i,1} + \Delta_{i,2}$. 
        The first segment is identical to $(i,1)$; the second has length 
        $1 + \Delta_{i,2}$. Its maximum output length is set to $1$.
  \item The input sequence of $(i,3)$ is identical to that of $(i,1)$. 
        Its maximum output length is set to $1 + \Delta_{i,3}$.
  \item $(i,4)$ has two input segments. The first segment is identical to $(i,1)$.  
        The second segment has length $1 + \Delta_{i,2}$, but its first token  
        differs from the first token of the second segment in $(i,2)$. 
        This guarantees that request $(i, 4)$ shares a prefix of length exactly 
        $4 + \Delta_{i,1}$ with other requests in this quad. Its maximum output length 
        is set to $1 + \Delta_{i,3}$ blocks.
\end{enumerate}

The structure of the requests in the quad is summarized in the following table.

\begin{table}[h]
\centering
\caption{Structure of requests in quad in Scenario 3; ``='' indicates a segment 
that is identical to the $(i,1)$ input sequence.}
\label{tab:quad-structure}
\begin{tabular}{@{}c|c|c|c@{}}
\toprule
Request & Input Segments & Total Input & Output (max) \\ \midrule
$(i,1)$ & $[4+\Delta_{i,1}]$ 
        & $4+\Delta_{i,1}$ 
        & 1 (prefill only) \\
$(i,2)$ & $[= (i,1)], [1+\Delta_{i,2}]$ 
        & $5+\Delta_{i,1}+\Delta_{i,2}$ 
        & 1 (prefill only) \\
$(i,3)$ & $[= (i,1)\,]$ 
        & $4+\Delta_{i,1}$ 
        & $1+\Delta_{i,3}$ \\
$(i,4)$ & $[= (i,1)], [1+\Delta_{i,2}]$
        & $5+\Delta_{i,1}+\Delta_{i,2}$ 
        & $1+\Delta_{i,3}$ \\ \bottomrule
\end{tabular}
\end{table}


\paragraph{Sampling.}
Requests in a quad have a maximum length budget of $M$ that must be divided across input segments and output, 
with variability. We therefore sample the $\Delta$s using a Dirichlet–Multinomial: 
a Dirichlet distribution enforces the simplex constraint and provides a 
single concentration knob to control balanced vs. skewed splits, and the Multinomial yields integer lengths. 
This also induces realistic negative correlations between parts (e.g., longer Segment~1 leaves fewer tokens 
for Segment~2 or output). The sampler is given in Algorithm~\ref{algo:sc3sampler}.

\begin{algorithm}[th]
  \caption{Segment lengths sampler in Scenario 3 (Dirichlet–Multinomial)}
  \label{algo:sc3sampler}
  \begin{algorithmic}[1]
    \Require Budget $N = M-6$; concentration $\boldsymbol{\eta}=(\eta_1,\eta_2,\eta_3)$
    \State Sample proportions $\mathbf{p} \sim \mathrm{Dirichlet}(\boldsymbol{\eta})$
    \State Sample counts $(\Delta_{i,1},\Delta_{i,2},\Delta_{i,3}) \sim \mathrm{Multinomial}\left(N,\mathbf{p}\right)$
    \State \textbf{return} $\Delta_{i,1}, \Delta_{i,2}, \Delta_{i,3}$
  \end{algorithmic}
\end{algorithm}

\iffalse

\begin{lemma}
\label{lemma:scenario3}
In Scenario 3, the end-to-end latency \eqref{eq:e2e-param} for the four requests 
in quad $i$ reduces to:
\begin{align}
e_{i,1} &\approx 
\alpha_0 
+ \alpha_1(1+\Delta_{i,1}) b
+ \alpha_2 
+ \sum_{j \in \mathcal{P}_{i,1}} \lambda_j \label{eq:sc3mem1}\\
e_{i,2} &\approx 
\alpha_0 
+ \alpha_1 (2+\Delta_{i,1}+\Delta_{i,2}) b
+ \alpha_2 
+ \sum_{j \in \mathcal{P}_{i,2}} \lambda_j \label{eq:sc3mem2}\\
e_{i,3} &\approx 
\alpha_0 
+ \alpha_1 (1+\Delta_{i,1}) b
+ \alpha_2\, t^{\text{out}}_{i,3}
+ \sum_{j \in \mathcal{P}_{i,3}} \lambda_j
+ \sum_{j \in \mathcal{D}_{i,3}} \lambda_j
\label{eq:sc3mem3}\\
e_{i,4} &\approx 
\alpha_0 
+ \alpha_1 (2+\Delta_{i,1}+\Delta_{i,2}) b
+ \alpha_2\, t^{\text{out}}_{i,4}
+ \sum_{j \in \mathcal{P}_{i,4}} \lambda_j
+ \sum_{j \in \mathcal{D}_{i,4}} \lambda_j
\label{eq:sc3mem4}
\end{align}
\end{lemma}

\begin{proof}
~\newline
\noindent\textbf{1) No scheduling delay.} In Scenario 3, a request is emitted only after the previous request finishes. 
Hence, $\mathcal{S}_{i,k}=\varnothing$ and $\sum_{j\in\mathcal{S}_{i,k}} \lambda_j=0$ for 
$k\in\{1,2,3,4\}$.

\noindent\textbf{2) Input tokens.} Segment lengths are in blocks; converting to 
tokens with block size $b$:
\begin{align*}
t^{\text{in}}_{i,1} & = & (1+\Delta_{i,1}) b \\
t^{\text{in}}_{i,2} & = & (2+\Delta_{i,1}+\Delta_{i,2}) b \\
t^{\text{in}}_{i,3} & = & (1+\Delta_{i,1}) b \\
t^{\text{in}}_{i,4} & = & (2+\Delta_{i,1}+\Delta_{i,2})b
\end{align*}

\noindent\textbf{3) Prefill-only requests.} Requests $(i, 1)$ and $(i, 2)$ are prefill-only. 
Hence, $\mathcal{D}_{i,1}=\mathcal{D}_{i,2}=\varnothing$ and 
$t^{\text{out}}_{i,1}=t^{\text{out}}_{i,2}=1$.
~\newline
~\newline
\noindent Substituting these facts into \eqref{eq:e2e-param} gives 
\eqref{eq:sc3mem1}–\eqref{eq:sc3mem4}.
\end{proof}

\begin{lemma}[Latency of request $(i,1)$]
\label{lemma:req1-latency}
Let the size of request $(i, 1)$ in chunks be $m_{i,1} \doteq \frac{(1 + \Delta_{i,1})b}{C}$. 
The end-to-end latency of request $(i,1)$ satisfies
\begin{align*}
e_{i,1} & \approx \alpha_0 + \alpha_2 + \beta_1 + \beta_3 + \beta_9 + \beta_{11} \\
        & + \alpha_1 m_{i,1} C 
        + \beta_0 \lceil m_{i,1} \rceil 
        + \beta_2 m_{i,1} C 
        + \beta_5 \frac{m_{i,1}C}{b} 
        + \beta_6 \frac{C^2}{b^2}
        \left(
          \frac{\lfloor m_{i,1} \rfloor + m_{i,1}^2}{2}
          + \frac{\big(m_{i,1}-\lfloor m_{i,1}\rfloor\big)^2}{2}
        \right)
\end{align*}
\end{lemma}


\begin{proof}[Proof of Lemma~\ref{lemma:req1-latency}]
By Scenario~3, request $(i,1)$ is prefill-only, shares no prefix with any
previous request, and emits exactly one output token. Specializing the
end-to-end model \eqref{eq:e2e-param} (and using \eqref{eq:sc3mem1}) gives
\[
e_{i,1}\;\approx\; \alpha_0 \;+\; \alpha_1\,t^{\text{in}}_{i,1} \;+\; \alpha_2 \;+\; 
\sum_{j\in\mathcal{P}_{i,1}} \lambda_j,
\qquad
t^{\text{in}}_{i,1}=(1+\Delta_{i,1})b \doteq m_{i,1} C.
\]
Thus it suffices to evaluate $\sum_{j\in\mathcal{P}_{i,1}} \lambda_j$ using the
busy-loop model \eqref{eq:busy-loop-expanded}.

\paragraph{(1) Number of iterations and per-iteration intercept.}
Prefill is processed in chunks of $C$ tokens. With $m_{i,1} \doteq
\frac{(1+\Delta_{i,1})b}{C}$, there are $q\!\doteq\!\lfloor m_{i,1}\rfloor$
full chunks and an optional fractional chunk of size $rC$ tokens where
$r\doteq m_{i,1}-q\in[0,1)$. Hence the request participates in
$\lceil m_{i,1}\rceil$ iterations, contributing
\[
\sum_{j\in\mathcal{P}_{i,1}} \beta_0 \;=\; \beta_0\,\lceil m_{i,1}\rceil.
\]

\paragraph{(2) Scheduling features.}
By construction of the features, the admission indicator aggregates to one per
request, while the token-count feature aggregates to the total processed input
tokens:
\[
\sum_{j\in\mathcal{P}_{i,1}} \beta_1 \rho_j \;=\; \beta_1,
\qquad
\sum_{j\in\mathcal{P}_{i,1}} \beta_2 \tau_j \;=\; \beta_2\,t^{\text{in}}_{i,1}
\;=\; \beta_2\,m_{i,1}C.
\]

\paragraph{(3) Prefill features (non-quadratic).}
There are no ``new hits'' for $(i,1)$, so $\sum_j \chi_{j,i}^{\text{hit,new}}=0$.
Every block is a miss exactly once, so $\sum_j \chi_{j,i}^{\text{miss}}
=1+\Delta_{i,1}$. The batch-formation indicator contributes once:
\[
\sum_{j\in\mathcal{P}_{i,1}} \beta_3 \pi_j = \beta_3
\qquad
\sum_{j\in\mathcal{P}_{i,1}} \beta_5 \chi_{j,i}^{\text{miss}} 
= \beta_5 (1+\Delta_{i,1})
= \beta_5 \frac{m_{i,1}C}{b}
\]

\paragraph{(4) Prefill quadratic term.}
In iteration $k\in\{1,\ldots,q\}$ (the $k$-th full chunk), the \emph{missed}
blocks equal $C/b$ and the \emph{hit} blocks equal $(k-1)\,C/b$. Therefore
\[
\chi_{j,i}^{\text{miss}}\cdot \chi_{j,i}^{\text{hit}}
+ \big(\chi_{j,i}^{\text{miss}}\big)^2
\;=\; \Big(\frac{C}{b}\Big)^2 k.
\]
Summing over full chunks yields
\(
\big(\frac{C}{b}\big)^2 \sum_{k=1}^{q} k
= \big(\frac{C}{b}\big)^2 \frac{q(q+1)}{2}.
\)

If $r>0$, the final partial chunk has miss $rC/b$ and hit $qC/b$, contributing
$\big(\frac{C}{b}\big)^2(q r + r^2)$. Hence,
\[
\sum_{j\in\mathcal{P}_{i,1}} \beta_6 \psi_j
=
\beta_6 \frac{C^2}{b^2}\!\left(
\frac{\lfloor m_{i,1}\rfloor + m_{i,1}^2}{2}
+\frac{(m_{i,1}-\lfloor m_{i,1}\rfloor)^2}{2}
\right).
\]

\paragraph{(5) Decode and completion features.}
There is no decode for $(i,1)$, so the $\beta_7$ and $\beta_8$ terms vanish, as
does $\beta_{10}$. Prefill completes once and the request finishes once:
\[
\sum_{j\in\mathcal{P}_{i,1}} \beta_9 \rho_j^{\text{pre-fin}} \;=\; \beta_9,
\qquad
\sum_{j\in\mathcal{P}_{i,1}} \beta_{11} \tau_j^{\text{fin}} \;=\; \beta_{11}.
\]

Combining (1)–(5) with $t^{\text{in}}_{i,1}=m_{i,1}C$ yields the Lemma.
\end{proof}

\begin{lemma}[Latency of request $(i,2)$]
\label{lemma:req2-latency}
Let
\[
m_{i,1}\doteq \frac{(1+\Delta_{i,1})b}{C},
\qquad
m_{i,2}\doteq \frac{(2+\Delta_{i,1}+\Delta_{i,2})b}{C},
\qquad
d_i \doteq m_{i,2}-m_{i,1}=\frac{(1+\Delta_{i,2})b}{C}.
\]
Then the end-to-end latency of request $(i,2)$ satisfies
\begin{align*}
e_{i,2}
&\approx \alpha_0 + \alpha_2 + \beta_1 + \beta_3 + \beta_9 + \beta_{11} \\
&\quad + \alpha_1\, m_{i,2} C
       + \beta_0\, \lceil d_i \rceil
       + \beta_2\, d_i C
       + \beta_4\, \frac{m_{i,1}C}{b}
       + \beta_5\, \frac{d_i C}{b} \\
&\quad + \beta_6\, \frac{C^2}{b^2}\left(
            \frac{m_{i,2}^2 - m_{i,1}^2}{2}
          + \frac{\lfloor d_i \rfloor + (d_i-\lfloor d_i \rfloor)^2}{2}
        \right).
\end{align*}
\end{lemma}

\begin{proof}
By Scenario~3, $(i,2)$ is prefill-only, with two segments: the first equals
$(i,1)$ (prefix reuse), the second adds $(1+\Delta_{i,2})$ new blocks. From
\eqref{eq:e2e-param} and \eqref{eq:sc3mem2},
\[
e_{i,2}\approx \alpha_0 + \alpha_1 t^{\text{in}}_{i,2} + \alpha_2
               + \sum_{j\in\mathcal{P}_{i,2}} \lambda_j,
\quad
t^{\text{in}}_{i,2}=(2+\Delta_{i,1}+\Delta_{i,2})b = m_{i,2}C.
\]
Thus we evaluate $\sum_{j\in\mathcal{P}_{i,2}} \lambda_j$ using
\eqref{eq:busy-loop-expanded}.

\paragraph{(1) Iterations and per-iteration intercept.}
The engine materializes \emph{only the new tokens} from the second segment.
With $d_i=\frac{(1+\Delta_{i,2})b}{C}$, there are $q=\lfloor d_i\rfloor$
full miss-chunks and a final partial of size $rC$ tokens where
$r=d_i-q\in[0,1)$. Hence the prefill intercept contributes
$\sum_{j\in\mathcal{P}_{i,2}} \beta_0 = \beta_0 \lceil d_i\rceil$.

\paragraph{(2) Scheduling features.}
The admission indicator aggregates to one per request, while the token-count
feature aggregates to the \emph{new} tokens processed on GPU:
\[
\sum_{j} \beta_1 \rho_j = \beta_1,
\qquad
\sum_{j} \beta_2 \tau_j = \beta_2\, d_i C.
\]

\paragraph{(3) Prefill features (non-quadratic).}
Prefix reuse contributes ``new hits'' of size equal to the reused first segment,
and the second segment contributes misses:
\[
\sum_{j}\beta_4 \chi_{j,i}^{\text{hit,new}} = \beta_4\,\frac{m_{i,1}C}{b},
\qquad
\sum_{j}\beta_5 \chi_{j,i}^{\text{miss}}   = \beta_5\,\frac{d_i C}{b}.
\]
Batch-formation contributes once: $\sum_j \beta_3 \pi_j=\beta_3$.

\paragraph{(4) Prefill quadratic term.}
Let $s=\frac{C}{b}$ blocks per full chunk. In the $k$-th full miss-chunk
($k=1,\ldots,q$), the miss is $s$ and the hit equals the reused prefix plus
previous misses: $\frac{m_{i,1}C}{b} + (k-1)s$. The quadratic per-chunk term
is $s\!\cdot\!\big(\frac{m_{i,1}C}{b}+(k-1)s\big) + s^2
= s\!\cdot\!\frac{m_{i,1}C}{b} + s^2 k$.
Summing full chunks and adding the partial ($r s$ miss, $q s$ additional hit)
gives
\[
\frac{C^2}{b^2}\Big( m_{i,1} d_i + \frac{d_i^2 + q + r^2}{2} \Big)
= \frac{C^2}{b^2}\left(
\frac{m_{i,2}^2 - m_{i,1}^2}{2}
+ \frac{\lfloor d_i \rfloor + (d_i-\lfloor d_i \rfloor)^2}{2}
\right),
\]
using $m_{i,2}=m_{i,1}+d_i$ and $q=\lfloor d_i\rfloor$, $r=d_i-q$.

\paragraph{(5) Decode and completion features.}
There is no decode, so $\beta_7$ and $\beta_8$ vanish, as does $\beta_{10}$.
Prefill completes once and the request finishes once:
$\sum_j \beta_9 \rho_j^{\text{pre-fin}}=\beta_9$ and
$\sum_j \beta_{11} \tau_j^{\text{fin}}=\beta_{11}$.

Collecting all terms with $t^{\text{in}}_{i,2}=m_{i,2}C$ yields the claim.
\end{proof}

\begin{lemma}[Latency of request $(i,3)$]
\label{lemma:req3-latency}
Let
\[
m_{i,1}\doteq \frac{(1+\Delta_{i,1})b}{C},
\qquad
o_{i,3}\doteq t^{\text{out}}_{i,3}\;\;(\text{decode tokens}).
\]
Since $(i,3)$ reuses the entire input of $(i,1)$, its end-to-end latency satisfies
\begin{align*}
e_{i,3}
&\approx \alpha_0 + \alpha_1\, m_{i,1} C + \alpha_2\, o_{i,3} \\
&\quad + \beta_1 + \beta_3
      + \beta_4\,\frac{m_{i,1}C}{b}
      \\
&\quad + (\beta_0 + \beta_7 + \beta_8)\,o_{i,3}
      + \beta_{10} + \beta_{11}.
\end{align*}
\end{lemma}

\begin{proof}
From \eqref{eq:e2e-param} and \eqref{eq:sc3mem3},
$e_{i,3}\approx \alpha_0 + \alpha_1 t^{\text{in}}_{i,3} + \alpha_2 o_{i,3}
+ \sum_{j\in\mathcal{P}_{i,3}} \lambda_j + \sum_{j\in\mathcal{D}_{i,3}} \lambda_j$
with $t^{\text{in}}_{i,3}=m_{i,1}C$.
\textbf{Prefill:} APC reuses the entire prefix, so there are no prefill compute
iterations: $\sum_{j\in\mathcal{P}_{i,3}}\beta_0=0$, $\sum\beta_2\tau_j=0$,
$\sum\beta_5\chi^{\text{miss}}_{j,i}=0$, and no quadratic term. Attaching hits
contributes $\sum\beta_4\chi^{\text{hit,new}}_{j,i}=\beta_4\,\frac{m_{i,1}C}{b}$.
\textbf{Decode:} For a single active request, the number of decode steps equals
$o_{i,3}$, hence $\sum \beta_0=\beta_0 o_{i,3}$, $\sum \beta_7\delta_j=\beta_7
o_{i,3}$, and $\sum \beta_8 \tau_j^{\text{dec}}=\beta_8 o_{i,3}$. \textbf{Indicators:}
We keep one-per-request admission/formation as $\beta_1+\beta_3$. There is no
prefill completion, so $\beta_9$ vanishes; decode completion contributes
$\beta_{10}$ and finalization contributes $\beta_{11}$. Collect terms.
\end{proof}


\begin{lemma}[Latency of request $(i,4)$]
\label{lemma:req4-latency}
Let
\[
m_{i,1}\doteq \frac{(1+\Delta_{i,1})b}{C},\quad
m_{i,2}\doteq \frac{(2+\Delta_{i,1}+\Delta_{i,2})b}{C},\quad
d_i \doteq m_{i,2}-m_{i,1}=\frac{(1+\Delta_{i,2})b}{C},
\]
and let $o_{i,4}\doteq t^{\text{out}}_{i,4}$ be the decode token count.
Then the end-to-end latency of $(i,4)$ satisfies
\begin{align*}
e_{i,4}
&\approx \alpha_0 + \alpha_1\, m_{i,2} C + \alpha_2\, o_{i,4} \\
&\quad + \beta_1 + \beta_3
      + \beta_0 \big(\lceil d_i\rceil + o_{i,4}\big)
      + \beta_2\, d_i C
      + \beta_4\, \frac{m_{i,1}C}{b}
      + \beta_5\, \frac{d_i C}{b} \\
&\quad + \beta_6\, \frac{C^2}{b^2}\!\left(
          \frac{m_{i,2}^2 - m_{i,1}^2}{2}
        + \frac{\lfloor d_i \rfloor + (d_i-\lfloor d_i \rfloor)^2}{2}
      \right) \\
&\quad + (\beta_7 + \beta_8)\, o_{i,4}
      + \beta_{9} + \beta_{10} + \beta_{11}.
\end{align*}
\end{lemma}

\begin{proof}
From \eqref{eq:e2e-param} and \eqref{eq:sc3mem4},
$e_{i,4}\approx \alpha_0 + \alpha_1 t^{\text{in}}_{i,4} + \alpha_2 o_{i,4}
+ \sum_{j\in\mathcal{P}_{i,4}} \lambda_j + \sum_{j\in\mathcal{D}_{i,4}} \lambda_j$
with $t^{\text{in}}_{i,4}=m_{i,2}C$. \textbf{Prefill (segment 2 only):}
There are $q=\lfloor d_i\rfloor$ full miss-chunks and a partial of size
$r=d_i-q\in[0,1)$, so $\sum \beta_0=\beta_0\lceil d_i\rceil$,
$\sum \beta_2\tau_j=\beta_2 d_i C$, and misses sum to
$\beta_5 \frac{d_i C}{b}$. Reusing the first segment contributes hits
$\beta_4\frac{m_{i,1}C}{b}$. The quadratic sum over prefill equals
\[
\frac{C^2}{b^2}\!\left(m_{i,1}d_i+\frac{d_i^2+q+r^2}{2}\right)
=
\frac{C^2}{b^2}\!\left(
\frac{m_{i,2}^2 - m_{i,1}^2}{2}
+ \frac{\lfloor d_i \rfloor + (d_i-\lfloor d_i \rfloor)^2}{2}
\right).
\]
\textbf{Decode:} With a single active sequence, decode runs for $o_{i,4}$ steps,
giving $\sum \beta_0=\beta_0 o_{i,4}$, $\sum \beta_7\delta_j=\beta_7 o_{i,4}$,
$\sum \beta_8 \tau_j^{\text{dec}}=\beta_8 o_{i,4}$. \textbf{Indicators:}
Admission/formation contribute $\beta_1+\beta_3$. Prefill completes once
($\beta_9$), decode completes once ($\beta_{10}$), and finalization contributes
$\beta_{11}$. Collect terms.
\end{proof}

\fi


\subsection{Refinements using Blackbox Optimization}
\label{subsec:refinements-blackbox-optimization}

\section{Evaluation}
\label{sec:evaluation}

\section{Implementation}
\label{sec:implementation}

\section{Limitations of our Approach}
\label{sec:limitations}

Our current approach has several important limitations:

\begin{enumerate}
    \item \textbf{Hardware and model specificity.}
    BLIS is calibrated for a fixed combination of GPU type and model
    architecture. We do not attempt to generalize across different hardware or
    models. If either changes, BLIS must be retrained with new data.

    \item \textbf{No pre-emption.}
    We do not model pre-emption of running requests. In practice, triggering
    pre-emption during request handling is rare and avoided by the scheduler
    logic, but its absence in our model means we cannot accurately predict
    latency under pre-emptive scenarios.

    \item \textbf{No speculative decoding.}
    We currently omit speculative decoding mechanisms (such as those recently
    introduced in vLLM as an experimental feature).
\end{enumerate}

\section{Future Work}
\label{sec:future-work}

\section{Conclusion}
\label{sec:conclusion}

\bibliographystyle{plain}
\bibliography{blis}

\end{document}
